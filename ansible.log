2025-07-07 08:48:09,202 p=46657 u=gpadmin n=ansible | Starting galaxy collection install process
2025-07-07 08:48:09,378 p=46657 u=gpadmin n=ansible | Nothing to do. All requested collections are already installed. If you want to reinstall them, consider using `--force`.
2025-07-07 08:48:14,045 p=46713 u=gpadmin n=ansible | playbook: site.yml
2025-07-07 08:48:26,723 p=46816 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 08:48:26,733 p=46816 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 08:48:26,978 p=46816 u=gpadmin n=ansible | [WARNING]: Unhandled error in Python interpreter discovery for host MASTER: Failed to
connect to the host via ssh: ssh: connect to host 192.168.40.240 port 22: Connection
refused

2025-07-07 08:48:26,995 p=46816 u=gpadmin n=ansible | fatal: [MASTER]: UNREACHABLE! => {"changed": false, "msg": "Data could not be sent to remote host \"192.168.40.240\". Make sure this host can be reached over ssh: ssh: connect to host 192.168.40.240 port 22: Connection refused\r\n", "unreachable": true}
2025-07-07 08:48:27,139 p=46816 u=gpadmin n=ansible | [WARNING]: Unhandled error in Python interpreter discovery for host G-244: Failed to
connect to the host via ssh: Warning: Permanently added '192.168.40.244' (ED25519) to the
list of known hosts.  gpadmin@192.168.40.244: Permission denied (publickey,password).

2025-07-07 08:48:27,143 p=46816 u=gpadmin n=ansible | [WARNING]: Unhandled error in Python interpreter discovery for host G-243: Failed to
connect to the host via ssh: Warning: Permanently added '192.168.40.243' (ED25519) to the
list of known hosts.  gpadmin@192.168.40.243: Permission denied (publickey,password).

2025-07-07 08:48:27,216 p=46816 u=gpadmin n=ansible | [WARNING]: Unhandled error in Python interpreter discovery for host G-242: Failed to
connect to the host via ssh: Warning: Permanently added '192.168.40.242' (ED25519) to the
list of known hosts.  gpadmin@192.168.40.242: Permission denied (publickey,password).

2025-07-07 08:48:27,243 p=46816 u=gpadmin n=ansible | [WARNING]: Unhandled error in Python interpreter discovery for host G-241: Failed to
connect to the host via ssh: gpadmin@192.168.40.241: Permission denied
(publickey,password).

2025-07-07 08:48:27,337 p=46816 u=gpadmin n=ansible | fatal: [G-243]: UNREACHABLE! => {"changed": false, "msg": "Data could not be sent to remote host \"192.168.40.243\". Make sure this host can be reached over ssh: gpadmin@192.168.40.243: Permission denied (publickey,password).\r\n", "unreachable": true}
2025-07-07 08:48:27,343 p=46816 u=gpadmin n=ansible | fatal: [G-244]: UNREACHABLE! => {"changed": false, "msg": "Data could not be sent to remote host \"192.168.40.244\". Make sure this host can be reached over ssh: gpadmin@192.168.40.244: Permission denied (publickey,password).\r\n", "unreachable": true}
2025-07-07 08:48:27,431 p=46816 u=gpadmin n=ansible | fatal: [G-242]: UNREACHABLE! => {"changed": false, "msg": "Data could not be sent to remote host \"192.168.40.242\". Make sure this host can be reached over ssh: gpadmin@192.168.40.242: Permission denied (publickey,password).\r\n", "unreachable": true}
2025-07-07 08:48:27,501 p=46816 u=gpadmin n=ansible | fatal: [G-241]: UNREACHABLE! => {"changed": false, "msg": "Data could not be sent to remote host \"192.168.40.241\". Make sure this host can be reached over ssh: gpadmin@192.168.40.241: Permission denied (publickey,password).\r\n", "unreachable": true}
2025-07-07 08:48:27,502 p=46816 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 08:48:27,502 p=46816 u=gpadmin n=ansible | G-241                      : ok=0    changed=0    unreachable=1    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 08:48:27,502 p=46816 u=gpadmin n=ansible | G-242                      : ok=0    changed=0    unreachable=1    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 08:48:27,502 p=46816 u=gpadmin n=ansible | G-243                      : ok=0    changed=0    unreachable=1    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 08:48:27,502 p=46816 u=gpadmin n=ansible | G-244                      : ok=0    changed=0    unreachable=1    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 08:48:27,503 p=46816 u=gpadmin n=ansible | MASTER                     : ok=0    changed=0    unreachable=1    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 08:55:42,404 p=51216 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 08:55:42,412 p=51216 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 08:55:42,648 p=51216 u=gpadmin n=ansible | [WARNING]: Unhandled error in Python interpreter discovery for host MASTER: Failed to
connect to the host via ssh: ssh: connect to host 192.168.40.240 port 22: Connection
refused

2025-07-07 08:55:42,667 p=51216 u=gpadmin n=ansible | fatal: [MASTER]: UNREACHABLE! => {"changed": false, "msg": "Data could not be sent to remote host \"192.168.40.240\". Make sure this host can be reached over ssh: ssh: connect to host 192.168.40.240 port 22: Connection refused\r\n", "unreachable": true}
2025-07-07 08:55:43,110 p=51216 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"msg": "Missing sudo password"}
2025-07-07 08:55:43,120 p=51216 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"msg": "Missing sudo password"}
2025-07-07 08:55:43,135 p=51216 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"msg": "Missing sudo password"}
2025-07-07 08:55:43,156 p=51216 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"msg": "Missing sudo password"}
2025-07-07 08:55:43,157 p=51216 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 08:55:43,157 p=51216 u=gpadmin n=ansible | G-241                      : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-07-07 08:55:43,157 p=51216 u=gpadmin n=ansible | G-242                      : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-07-07 08:55:43,157 p=51216 u=gpadmin n=ansible | G-243                      : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-07-07 08:55:43,157 p=51216 u=gpadmin n=ansible | G-244                      : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-07-07 08:55:43,158 p=51216 u=gpadmin n=ansible | MASTER                     : ok=0    changed=0    unreachable=1    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 08:56:32,275 p=51714 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 08:56:32,284 p=51714 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 08:56:33,365 p=51714 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 08:56:33,377 p=51714 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 08:56:33,387 p=51714 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 08:56:33,458 p=51714 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 08:56:33,753 p=51714 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:56:33,829 p=51714 u=gpadmin n=ansible | TASK [common : Update apt cache] **********************************************************
2025-07-07 08:56:34,314 p=51714 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 08:56:34,681 p=51714 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:56:34,963 p=51714 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 08:56:36,696 p=51714 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 08:56:37,293 p=51714 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 08:56:37,305 p=51714 u=gpadmin n=ansible | TASK [common : Install common packages] ***************************************************
2025-07-07 08:56:39,356 p=51714 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:56:57,131 p=51714 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 08:56:58,058 p=51714 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 08:57:01,280 p=51714 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 08:57:01,939 p=51714 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 08:57:01,952 p=51714 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] ********************************************
2025-07-07 08:57:02,241 p=51714 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 08:57:02,251 p=51714 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 08:57:02,251 p=51714 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 08:57:02,256 p=51714 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 08:57:02,363 p=51714 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 08:57:02,377 p=51714 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] *********************************************
2025-07-07 08:57:02,670 p=51714 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 08:57:02,676 p=51714 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 08:57:02,685 p=51714 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 08:57:02,687 p=51714 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 08:57:02,773 p=51714 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:57:02,826 p=51714 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed] **********************************************
2025-07-07 08:57:03,273 p=51714 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:57:03,484 p=51714 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 08:57:03,486 p=51714 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 08:57:03,540 p=51714 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 08:57:03,548 p=51714 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 08:57:03,561 p=51714 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] ********************************************************
2025-07-07 08:57:03,589 p=51714 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 08:57:03,607 p=51714 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 08:57:03,621 p=51714 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 08:57:03,636 p=51714 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 08:57:03,648 p=51714 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 08:57:03,657 p=51714 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] *************************************************
2025-07-07 08:57:03,676 p=51714 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 08:57:03,704 p=51714 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 08:57:03,718 p=51714 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 08:57:03,719 p=51714 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 08:57:03,732 p=51714 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 08:57:03,740 p=51714 u=gpadmin n=ansible | TASK [docker : Install Docker Engine] *****************************************************
2025-07-07 08:57:03,763 p=51714 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 08:57:03,793 p=51714 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 08:57:03,807 p=51714 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 08:57:03,808 p=51714 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 08:57:03,816 p=51714 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 08:57:03,825 p=51714 u=gpadmin n=ansible | TASK [docker : Ensure Docker service is running and enabled] ******************************
2025-07-07 08:57:04,305 p=51714 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Could not find the requested service docker: host"}
2025-07-07 08:57:04,306 p=51714 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Could not find the requested service docker: host"}
2025-07-07 08:57:04,309 p=51714 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Could not find the requested service docker: host"}
2025-07-07 08:57:04,317 p=51714 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Could not find the requested service docker: host"}
2025-07-07 08:57:04,501 p=51714 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:57:04,508 p=51714 u=gpadmin n=ansible | TASK [docker : Add user to docker group] **************************************************
2025-07-07 08:57:04,526 p=51714 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 08:57:04,534 p=51714 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] *****************************************************
2025-07-07 08:57:43,513 p=51714 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 08:57:43,566 p=51714 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] ************************************************************
2025-07-07 08:57:43,580 p=51714 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 08:57:45,768 p=51714 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:57:45,804 p=51714 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] **********************
2025-07-07 08:57:46,153 p=51714 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:57:46,170 p=51714 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ***************
2025-07-07 08:57:47,028 p=51714 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:57:47,041 p=51714 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] **************************
2025-07-07 08:57:47,363 p=51714 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:57:47,378 p=51714 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] *********************************
2025-07-07 08:57:48,251 p=51714 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 08:57:48,268 p=51714 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] ************************************************
2025-07-07 08:57:50,548 p=51714 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 08:57:50,563 p=51714 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ***************************************
2025-07-07 08:57:50,896 p=51714 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:57:50,910 p=51714 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] *****************************************
2025-07-07 08:57:50,930 p=51714 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED         STATUS                  PORTS     NAMES\n6ce6c8435670   rayproject/ray:2.9.0   \"ray start --head --…\"   2 seconds ago   Up Less than a second             ray_head"
}
2025-07-07 08:57:50,944 p=51714 u=gpadmin n=ansible | TASK [ray_head : Wait for Ray head to be ready] *******************************************
2025-07-07 08:58:51,407 p=51714 u=gpadmin n=ansible | fatal: [MASTER -> localhost]: FAILED! => {"changed": false, "elapsed": 60, "msg": "Timeout when waiting for 192.168.40.240:6379"}
2025-07-07 08:58:51,409 p=51714 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 08:58:51,409 p=51714 u=gpadmin n=ansible | G-241                      : ok=6    changed=2    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-07-07 08:58:51,409 p=51714 u=gpadmin n=ansible | G-242                      : ok=6    changed=3    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-07-07 08:58:51,409 p=51714 u=gpadmin n=ansible | G-243                      : ok=6    changed=3    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-07-07 08:58:51,409 p=51714 u=gpadmin n=ansible | G-244                      : ok=6    changed=3    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-07-07 08:58:51,409 p=51714 u=gpadmin n=ansible | MASTER                     : ok=16   changed=4    unreachable=0    failed=1    skipped=4    rescued=0    ignored=0   
2025-07-07 08:59:37,330 p=56844 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 08:59:37,339 p=56844 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 08:59:38,878 p=56844 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:59:38,911 p=56844 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 08:59:39,014 p=56844 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 08:59:39,023 p=56844 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 08:59:39,053 p=56844 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 08:59:39,125 p=56844 u=gpadmin n=ansible | TASK [common : Update apt cache] **********************************************************
2025-07-07 08:59:39,608 p=56844 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 08:59:39,621 p=56844 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 08:59:39,631 p=56844 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 08:59:39,640 p=56844 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 08:59:40,014 p=56844 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:59:40,026 p=56844 u=gpadmin n=ansible | TASK [common : Install common packages] ***************************************************
2025-07-07 08:59:40,487 p=56844 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 08:59:40,507 p=56844 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 08:59:40,515 p=56844 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 08:59:40,515 p=56844 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 08:59:41,088 p=56844 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:59:41,100 p=56844 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] ********************************************
2025-07-07 08:59:41,378 p=56844 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 08:59:41,380 p=56844 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 08:59:41,386 p=56844 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 08:59:41,390 p=56844 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 08:59:41,502 p=56844 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:59:41,514 p=56844 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] *********************************************
2025-07-07 08:59:41,786 p=56844 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 08:59:41,788 p=56844 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 08:59:41,796 p=56844 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 08:59:41,800 p=56844 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 08:59:41,899 p=56844 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:59:41,952 p=56844 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed] **********************************************
2025-07-07 08:59:42,230 p=56844 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 08:59:42,231 p=56844 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 08:59:42,238 p=56844 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 08:59:42,247 p=56844 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 08:59:42,331 p=56844 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:59:42,344 p=56844 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] ********************************************************
2025-07-07 08:59:42,371 p=56844 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 08:59:42,391 p=56844 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 08:59:42,420 p=56844 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 08:59:42,422 p=56844 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 08:59:42,432 p=56844 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 08:59:42,441 p=56844 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] *************************************************
2025-07-07 08:59:42,459 p=56844 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 08:59:42,487 p=56844 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 08:59:42,502 p=56844 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 08:59:42,502 p=56844 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 08:59:42,513 p=56844 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 08:59:42,521 p=56844 u=gpadmin n=ansible | TASK [docker : Install Docker Engine] *****************************************************
2025-07-07 08:59:42,553 p=56844 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 08:59:42,567 p=56844 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 08:59:42,587 p=56844 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 08:59:42,587 p=56844 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 08:59:42,595 p=56844 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 08:59:42,604 p=56844 u=gpadmin n=ansible | TASK [docker : Ensure Docker service is running and enabled] ******************************
2025-07-07 08:59:43,108 p=56844 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Could not find the requested service docker: host"}
2025-07-07 08:59:43,110 p=56844 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Could not find the requested service docker: host"}
2025-07-07 08:59:43,114 p=56844 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Could not find the requested service docker: host"}
2025-07-07 08:59:43,115 p=56844 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Could not find the requested service docker: host"}
2025-07-07 08:59:43,301 p=56844 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:59:43,315 p=56844 u=gpadmin n=ansible | TASK [docker : Add user to docker group] **************************************************
2025-07-07 08:59:43,336 p=56844 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 08:59:43,351 p=56844 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] *****************************************************
2025-07-07 08:59:44,491 p=56844 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:59:44,548 p=56844 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] ************************************************************
2025-07-07 08:59:44,558 p=56844 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 08:59:45,794 p=56844 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:59:45,827 p=56844 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] **********************
2025-07-07 08:59:46,127 p=56844 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:59:46,141 p=56844 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ***************
2025-07-07 08:59:46,899 p=56844 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 08:59:46,912 p=56844 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] **************************
2025-07-07 08:59:47,205 p=56844 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 08:59:47,214 p=56844 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] *********************************
2025-07-07 08:59:47,922 p=56844 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 08:59:47,939 p=56844 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] ************************************************
2025-07-07 08:59:48,462 p=56844 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 08:59:48,477 p=56844 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ***************************************
2025-07-07 08:59:48,794 p=56844 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 08:59:48,809 p=56844 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] *****************************************
2025-07-07 08:59:48,829 p=56844 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n33ddd73d4c21   rayproject/ray:2.9.0   \"ray start --head --…\"   Less than a second ago   Up Less than a second             ray_head"
}
2025-07-07 08:59:48,843 p=56844 u=gpadmin n=ansible | TASK [ray_head : Wait for Ray head to be ready] *******************************************
2025-07-07 09:00:49,279 p=56844 u=gpadmin n=ansible | fatal: [MASTER -> localhost]: FAILED! => {"changed": false, "elapsed": 60, "msg": "Timeout when waiting for 127.0.0.1:6379"}
2025-07-07 09:00:49,280 p=56844 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 09:00:49,280 p=56844 u=gpadmin n=ansible | G-241                      : ok=6    changed=0    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-07-07 09:00:49,280 p=56844 u=gpadmin n=ansible | G-242                      : ok=6    changed=0    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-07-07 09:00:49,281 p=56844 u=gpadmin n=ansible | G-243                      : ok=6    changed=0    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-07-07 09:00:49,281 p=56844 u=gpadmin n=ansible | G-244                      : ok=6    changed=0    unreachable=0    failed=1    skipped=3    rescued=0    ignored=0   
2025-07-07 09:00:49,281 p=56844 u=gpadmin n=ansible | MASTER                     : ok=16   changed=4    unreachable=0    failed=1    skipped=4    rescued=0    ignored=0   
2025-07-07 09:05:06,272 p=61439 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 09:05:06,281 p=61439 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:05:07,795 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:05:07,949 p=61439 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:05:07,978 p=61439 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:05:08,009 p=61439 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:05:08,059 p=61439 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:05:08,132 p=61439 u=gpadmin n=ansible | TASK [common : Update apt cache] **********************************************************
2025-07-07 09:05:08,608 p=61439 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:05:08,611 p=61439 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:05:08,612 p=61439 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:05:08,620 p=61439 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:05:09,000 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:05:09,013 p=61439 u=gpadmin n=ansible | TASK [common : Install common packages] ***************************************************
2025-07-07 09:05:09,468 p=61439 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:05:09,493 p=61439 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:05:09,504 p=61439 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:05:09,509 p=61439 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:05:10,066 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:05:10,079 p=61439 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] ********************************************
2025-07-07 09:05:10,365 p=61439 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:05:10,370 p=61439 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:05:10,374 p=61439 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:05:10,381 p=61439 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:05:10,477 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:05:10,489 p=61439 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] *********************************************
2025-07-07 09:05:10,761 p=61439 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:05:10,769 p=61439 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:05:10,771 p=61439 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:05:10,776 p=61439 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:05:10,879 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:05:10,934 p=61439 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed (apt or snap)] ********************************
2025-07-07 09:05:11,216 p=61439 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:05:11,217 p=61439 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:05:11,217 p=61439 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:05:11,225 p=61439 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:05:11,311 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:05:11,324 p=61439 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] ********************************************************
2025-07-07 09:05:11,353 p=61439 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:05:11,374 p=61439 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:05:11,389 p=61439 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:05:11,405 p=61439 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:05:11,417 p=61439 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:05:11,425 p=61439 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] *************************************************
2025-07-07 09:05:11,457 p=61439 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:05:11,472 p=61439 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:05:11,486 p=61439 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:05:11,488 p=61439 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:05:11,499 p=61439 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:05:11,508 p=61439 u=gpadmin n=ansible | TASK [docker : Install Docker Engine] *****************************************************
2025-07-07 09:05:11,542 p=61439 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:05:11,556 p=61439 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:05:11,573 p=61439 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:05:11,574 p=61439 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:05:11,587 p=61439 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:05:11,595 p=61439 u=gpadmin n=ansible | TASK [docker : Check Docker service status] ***********************************************
2025-07-07 09:05:11,772 p=61439 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003157", "end": "2025-07-07 13:05:11.758015", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:05:11.754858", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:05:11,772 p=61439 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:05:11,783 p=61439 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003422", "end": "2025-07-07 13:05:11.771204", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:05:11.767782", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:05:11,784 p=61439 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:05:11,794 p=61439 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003259", "end": "2025-07-07 13:05:11.776628", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:05:11.773369", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:05:11,794 p=61439 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:05:11,805 p=61439 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003257", "end": "2025-07-07 13:05:11.792004", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:05:11.788747", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:05:11,806 p=61439 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:05:11,892 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:05:11,905 p=61439 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] ********************************************
2025-07-07 09:05:11,946 p=61439 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:05:11,976 p=61439 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:05:11,976 p=61439 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:05:11,984 p=61439 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:05:12,618 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:05:12,629 p=61439 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ***********************************************
2025-07-07 09:05:12,650 p=61439 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:05:12,956 p=61439 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:05:46,018 p=61439 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:05:46,022 p=61439 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:05:46,025 p=61439 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:05:46,039 p=61439 u=gpadmin n=ansible | TASK [docker : Add user to docker group] **************************************************
2025-07-07 09:05:46,067 p=61439 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:05:46,085 p=61439 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:05:46,113 p=61439 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:05:46,115 p=61439 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:05:46,126 p=61439 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:05:46,135 p=61439 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] *****************************************************
2025-07-07 09:05:48,098 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:06:14,047 p=61439 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:06:17,859 p=61439 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:06:22,577 p=61439 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:06:23,605 p=61439 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:06:23,782 p=61439 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] ************************************************************
2025-07-07 09:06:23,792 p=61439 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:06:24,988 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:06:25,025 p=61439 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] **********************
2025-07-07 09:06:25,336 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:06:25,349 p=61439 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ***************
2025-07-07 09:06:26,059 p=61439 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:06:26,074 p=61439 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] **************************
2025-07-07 09:06:26,378 p=61439 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:06:26,393 p=61439 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] *********************************
2025-07-07 09:06:27,044 p=61439 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:06:27,059 p=61439 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] ************************************************
2025-07-07 09:06:27,620 p=61439 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:06:27,635 p=61439 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ***************************************
2025-07-07 09:06:27,955 p=61439 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:06:27,970 p=61439 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] *****************************************
2025-07-07 09:06:27,995 p=61439 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n0ee491cff313   rayproject/ray:2.9.0   \"ray start --head --…\"   Less than a second ago   Up Less than a second             ray_head"
}
2025-07-07 09:06:28,009 p=61439 u=gpadmin n=ansible | TASK [ray_head : Wait for Ray head to be ready] *******************************************
2025-07-07 09:07:28,446 p=61439 u=gpadmin n=ansible | fatal: [MASTER -> localhost]: FAILED! => {"changed": false, "elapsed": 60, "msg": "Timeout when waiting for 127.0.0.1:6379"}
2025-07-07 09:07:28,448 p=61439 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 09:07:28,448 p=61439 u=gpadmin n=ansible | G-241                      : ok=9    changed=3    unreachable=0    failed=0    skipped=5    rescued=0    ignored=1   
2025-07-07 09:07:28,448 p=61439 u=gpadmin n=ansible | G-242                      : ok=9    changed=3    unreachable=0    failed=0    skipped=5    rescued=0    ignored=1   
2025-07-07 09:07:28,448 p=61439 u=gpadmin n=ansible | G-243                      : ok=9    changed=3    unreachable=0    failed=0    skipped=5    rescued=0    ignored=1   
2025-07-07 09:07:28,448 p=61439 u=gpadmin n=ansible | G-244                      : ok=9    changed=3    unreachable=0    failed=0    skipped=5    rescued=0    ignored=1   
2025-07-07 09:07:28,448 p=61439 u=gpadmin n=ansible | MASTER                     : ok=17   changed=4    unreachable=0    failed=1    skipped=5    rescued=0    ignored=0   
2025-07-07 09:09:28,126 p=65214 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 09:09:28,136 p=65214 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:09:29,611 p=65214 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:29,733 p=65214 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:09:29,822 p=65214 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:29,856 p=65214 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:29,872 p=65214 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:29,951 p=65214 u=gpadmin n=ansible | TASK [common : Update apt cache] **********************************************************
2025-07-07 09:09:30,429 p=65214 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:30,435 p=65214 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:30,445 p=65214 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:30,815 p=65214 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:31,114 p=65214 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:09:31,127 p=65214 u=gpadmin n=ansible | TASK [common : Install common packages] ***************************************************
2025-07-07 09:09:31,594 p=65214 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:09:31,613 p=65214 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:31,617 p=65214 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:31,631 p=65214 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:32,176 p=65214 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:32,189 p=65214 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] ********************************************
2025-07-07 09:09:32,488 p=65214 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:32,489 p=65214 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:32,491 p=65214 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:09:32,505 p=65214 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:32,607 p=65214 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:32,620 p=65214 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] *********************************************
2025-07-07 09:09:32,889 p=65214 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:09:32,897 p=65214 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:32,901 p=65214 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:32,907 p=65214 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:33,001 p=65214 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:33,056 p=65214 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed (apt or snap)] ********************************
2025-07-07 09:09:33,329 p=65214 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:33,335 p=65214 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:33,336 p=65214 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:09:33,336 p=65214 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:33,435 p=65214 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:33,448 p=65214 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] ********************************************************
2025-07-07 09:09:33,475 p=65214 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:09:33,494 p=65214 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:09:33,510 p=65214 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:09:33,526 p=65214 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:09:33,537 p=65214 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:09:33,547 p=65214 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] *************************************************
2025-07-07 09:09:33,580 p=65214 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:09:33,596 p=65214 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:09:33,610 p=65214 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:09:33,612 p=65214 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:09:33,623 p=65214 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:09:33,632 p=65214 u=gpadmin n=ansible | TASK [docker : Install Docker Engine] *****************************************************
2025-07-07 09:09:33,652 p=65214 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:09:33,666 p=65214 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:09:33,698 p=65214 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:09:33,699 p=65214 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:09:33,707 p=65214 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:09:33,716 p=65214 u=gpadmin n=ansible | TASK [docker : Check Docker service status] ***********************************************
2025-07-07 09:09:33,898 p=65214 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003172", "end": "2025-07-07 13:09:33.882852", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:09:33.879680", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:09:33,898 p=65214 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:09:33,901 p=65214 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003379", "end": "2025-07-07 13:09:33.890313", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:09:33.886934", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:09:33,901 p=65214 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:09:33,918 p=65214 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003199", "end": "2025-07-07 13:09:33.906143", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:09:33.902944", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:09:33,918 p=65214 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:09:33,925 p=65214 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003170", "end": "2025-07-07 13:09:33.912312", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:09:33.909142", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:09:33,925 p=65214 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:09:34,006 p=65214 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:34,018 p=65214 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] ********************************************
2025-07-07 09:09:34,061 p=65214 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:09:34,089 p=65214 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:09:34,089 p=65214 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:09:34,098 p=65214 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:09:34,717 p=65214 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:34,728 p=65214 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ***********************************************
2025-07-07 09:09:34,762 p=65214 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:09:35,062 p=65214 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:09:41,907 p=65214 u=gpadmin n=ansible |  [ERROR]: User interrupted execution

2025-07-07 09:09:54,756 p=66114 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] ***************************************
2025-07-07 09:09:54,765 p=66114 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************
2025-07-07 09:09:55,867 p=66114 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:55,878 p=66114 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:09:55,907 p=66114 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:55,947 p=66114 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:56,192 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:56,266 p=66114 u=gpadmin n=ansible | TASK [common : Update apt cache] ***********************************************
2025-07-07 09:09:56,740 p=66114 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:09:56,745 p=66114 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:56,747 p=66114 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:56,759 p=66114 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:57,107 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:57,119 p=66114 u=gpadmin n=ansible | TASK [common : Install common packages] ****************************************
2025-07-07 09:09:57,586 p=66114 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:09:57,605 p=66114 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:57,614 p=66114 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:57,623 p=66114 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:58,144 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:58,156 p=66114 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] *********************************
2025-07-07 09:09:58,437 p=66114 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:09:58,439 p=66114 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:58,442 p=66114 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:58,443 p=66114 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:58,549 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:58,558 p=66114 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] **********************************
2025-07-07 09:09:58,833 p=66114 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:58,836 p=66114 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:09:58,848 p=66114 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:58,864 p=66114 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:58,948 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:59,002 p=66114 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed (apt or snap)] *********************
2025-07-07 09:09:59,267 p=66114 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:09:59,289 p=66114 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:09:59,293 p=66114 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:09:59,305 p=66114 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:09:59,374 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:59,387 p=66114 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] *********************************************
2025-07-07 09:09:59,413 p=66114 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:09:59,435 p=66114 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:09:59,465 p=66114 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:09:59,467 p=66114 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:09:59,473 p=66114 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:09:59,482 p=66114 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] **************************************
2025-07-07 09:09:59,502 p=66114 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:09:59,532 p=66114 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:09:59,547 p=66114 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:09:59,547 p=66114 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:09:59,560 p=66114 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:09:59,569 p=66114 u=gpadmin n=ansible | TASK [docker : Install Docker Engine] ******************************************
2025-07-07 09:09:59,603 p=66114 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:09:59,618 p=66114 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:09:59,635 p=66114 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:09:59,636 p=66114 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:09:59,644 p=66114 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:09:59,653 p=66114 u=gpadmin n=ansible | TASK [docker : Check Docker service status] ************************************
2025-07-07 09:09:59,829 p=66114 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003132", "end": "2025-07-07 13:09:59.814100", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:09:59.810968", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:09:59,829 p=66114 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:09:59,839 p=66114 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003154", "end": "2025-07-07 13:09:59.826830", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:09:59.823676", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:09:59,839 p=66114 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:09:59,841 p=66114 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003319", "end": "2025-07-07 13:09:59.830186", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:09:59.826867", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:09:59,841 p=66114 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:09:59,856 p=66114 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003114", "end": "2025-07-07 13:09:59.843885", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:09:59.840771", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:09:59,856 p=66114 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:09:59,963 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:09:59,976 p=66114 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] *********************************
2025-07-07 09:10:00,032 p=66114 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:10:00,046 p=66114 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:10:00,047 p=66114 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:10:00,059 p=66114 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:10:00,707 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:10:00,723 p=66114 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ************************************
2025-07-07 09:10:00,751 p=66114 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:10:00,941 p=66114 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": true, "cmd": ["snap", "start", "docker"], "delta": "0:00:00.014617", "end": "2025-07-07 13:10:00.928435", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:10:00.913818", "stderr": "error: snap \"docker\" has \"service-control\" change in progress", "stderr_lines": ["error: snap \"docker\" has \"service-control\" change in progress"], "stdout": "", "stdout_lines": []}
2025-07-07 09:10:00,941 p=66114 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:10:00,947 p=66114 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": true, "cmd": ["snap", "start", "docker"], "delta": "0:00:00.014064", "end": "2025-07-07 13:10:00.935432", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:10:00.921368", "stderr": "error: snap \"docker\" has \"service-control\" change in progress", "stderr_lines": ["error: snap \"docker\" has \"service-control\" change in progress"], "stdout": "", "stdout_lines": []}
2025-07-07 09:10:00,947 p=66114 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:10:00,958 p=66114 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": true, "cmd": ["snap", "start", "docker"], "delta": "0:00:00.014316", "end": "2025-07-07 13:10:00.944552", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:10:00.930236", "stderr": "error: snap \"docker\" has \"service-control\" change in progress", "stderr_lines": ["error: snap \"docker\" has \"service-control\" change in progress"], "stdout": "", "stdout_lines": []}
2025-07-07 09:10:00,958 p=66114 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:10:01,052 p=66114 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:10:01,066 p=66114 u=gpadmin n=ansible | TASK [docker : Add user to docker group] ***************************************
2025-07-07 09:10:01,093 p=66114 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:10:01,110 p=66114 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:10:01,138 p=66114 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:10:01,140 p=66114 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:10:01,148 p=66114 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:10:01,157 p=66114 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] ******************************************
2025-07-07 09:10:02,185 p=66114 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:10:02,200 p=66114 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:10:02,222 p=66114 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:10:02,230 p=66114 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:10:02,321 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:10:02,493 p=66114 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] *************************************************
2025-07-07 09:10:02,503 p=66114 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************
2025-07-07 09:10:03,689 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:10:03,727 p=66114 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] ***********
2025-07-07 09:10:04,023 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:10:04,036 p=66114 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ****
2025-07-07 09:10:04,756 p=66114 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:10:04,769 p=66114 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] ***************
2025-07-07 09:10:05,058 p=66114 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:10:05,072 p=66114 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] **********************
2025-07-07 09:10:05,706 p=66114 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:10:05,723 p=66114 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] *************************************
2025-07-07 09:10:06,237 p=66114 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:10:06,252 p=66114 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ****************************
2025-07-07 09:10:06,570 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:10:06,585 p=66114 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] ******************************
2025-07-07 09:10:06,604 p=66114 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\nff80b8e8e272   rayproject/ray:2.9.0   \"ray start --head --…\"   Less than a second ago   Up Less than a second             ray_head"
}
2025-07-07 09:10:06,619 p=66114 u=gpadmin n=ansible | TASK [ray_head : Wait for a moment to let Ray head node initialize] ************
2025-07-07 09:10:06,633 p=66114 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 09:10:06,634 p=66114 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 09:10:16,638 p=66114 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:10:16,653 p=66114 u=gpadmin n=ansible | TASK [ray_head : Check Ray head node status] ***********************************
2025-07-07 09:10:16,977 p=66114 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_head", "ray", "status"], "delta": "0:00:00.032873", "end": "2025-07-07 09:10:16.934555", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 09:10:16.901682", "stderr": "Error response from daemon: container ff80b8e8e272fc6a976b3cdf9e7a9a29a72059465f98367ad3fa7a39420df3a7 is not running", "stderr_lines": ["Error response from daemon: container ff80b8e8e272fc6a976b3cdf9e7a9a29a72059465f98367ad3fa7a39420df3a7 is not running"], "stdout": "", "stdout_lines": []}
2025-07-07 09:10:16,977 p=66114 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:10:16,987 p=66114 u=gpadmin n=ansible | TASK [ray_head : Print Ray head node status] ***********************************
2025-07-07 09:10:17,024 p=66114 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": []
}
2025-07-07 09:10:17,080 p=66114 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] **********************************************
2025-07-07 09:10:17,088 p=66114 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************
2025-07-07 09:10:17,799 p=66114 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:10:17,946 p=66114 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:10:17,977 p=66114 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:10:17,982 p=66114 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:10:18,045 p=66114 u=gpadmin n=ansible | TASK [ray_worker : Ensure Ray temporary directory exists on worker node] *******
2025-07-07 09:10:18,219 p=66114 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:10:18,236 p=66114 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:10:18,252 p=66114 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:10:18,259 p=66114 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:10:18,271 p=66114 u=gpadmin n=ansible | TASK [ray_worker : Stop and remove existing Ray worker container (idempotency)] ***
2025-07-07 09:10:18,556 p=66114 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:10:18,562 p=66114 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:10:18,579 p=66114 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:10:18,596 p=66114 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:10:18,608 p=66114 u=gpadmin n=ansible | TASK [ray_worker : Remove old Ray worker start script (idempotency)] ***********
2025-07-07 09:10:18,788 p=66114 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:10:18,808 p=66114 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:10:18,808 p=66114 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:10:18,825 p=66114 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:10:18,836 p=66114 u=gpadmin n=ansible | TASK [ray_worker : Copy Ray worker start script to worker node] ****************
2025-07-07 09:10:18,940 p=66114 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined
2025-07-07 09:10:18,941 p=66114 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined"}
2025-07-07 09:10:18,967 p=66114 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined
2025-07-07 09:10:18,967 p=66114 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined"}
2025-07-07 09:10:18,990 p=66114 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined
2025-07-07 09:10:18,990 p=66114 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined"}
2025-07-07 09:10:19,004 p=66114 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined
2025-07-07 09:10:19,005 p=66114 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined"}
2025-07-07 09:10:19,006 p=66114 u=gpadmin n=ansible | PLAY RECAP *********************************************************************
2025-07-07 09:10:19,006 p=66114 u=gpadmin n=ansible | G-241                      : ok=13   changed=1    unreachable=0    failed=1    skipped=5    rescued=0    ignored=1   
2025-07-07 09:10:19,006 p=66114 u=gpadmin n=ansible | G-242                      : ok=13   changed=1    unreachable=0    failed=1    skipped=5    rescued=0    ignored=2   
2025-07-07 09:10:19,006 p=66114 u=gpadmin n=ansible | G-243                      : ok=13   changed=1    unreachable=0    failed=1    skipped=5    rescued=0    ignored=2   
2025-07-07 09:10:19,006 p=66114 u=gpadmin n=ansible | G-244                      : ok=13   changed=1    unreachable=0    failed=1    skipped=5    rescued=0    ignored=2   
2025-07-07 09:10:19,006 p=66114 u=gpadmin n=ansible | MASTER                     : ok=20   changed=4    unreachable=0    failed=0    skipped=5    rescued=0    ignored=1   
2025-07-07 09:11:50,222 p=68406 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] ***************************************
2025-07-07 09:11:50,231 p=68406 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************
2025-07-07 09:11:51,720 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:11:51,922 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:11:51,938 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:11:52,003 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:11:52,018 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:11:52,090 p=68406 u=gpadmin n=ansible | TASK [common : Update apt cache] ***********************************************
2025-07-07 09:11:52,572 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:11:52,573 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:11:52,575 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:11:52,577 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:11:52,945 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:11:52,958 p=68406 u=gpadmin n=ansible | TASK [common : Install common packages] ****************************************
2025-07-07 09:11:53,418 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:11:53,443 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:11:53,450 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:11:53,452 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:11:53,990 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:11:54,003 p=68406 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] *********************************
2025-07-07 09:11:54,287 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:11:54,292 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:11:54,299 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:11:54,308 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:11:54,396 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:11:54,410 p=68406 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] **********************************
2025-07-07 09:11:54,694 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:11:54,709 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:11:54,709 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:11:54,710 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:11:54,790 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:11:54,841 p=68406 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed (apt or snap)] *********************
2025-07-07 09:11:55,124 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:11:55,124 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:11:55,125 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:11:55,135 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:11:55,213 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:11:55,227 p=68406 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] *********************************************
2025-07-07 09:11:55,259 p=68406 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:11:55,276 p=68406 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:11:55,305 p=68406 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:11:55,306 p=68406 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:11:55,314 p=68406 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:11:55,323 p=68406 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] **************************************
2025-07-07 09:11:55,355 p=68406 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:11:55,370 p=68406 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:11:55,384 p=68406 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:11:55,385 p=68406 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:11:55,393 p=68406 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:11:55,401 p=68406 u=gpadmin n=ansible | TASK [docker : Install Docker Engine] ******************************************
2025-07-07 09:11:55,433 p=68406 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:11:55,452 p=68406 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:11:55,466 p=68406 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:11:55,468 p=68406 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:11:55,476 p=68406 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:11:55,484 p=68406 u=gpadmin n=ansible | TASK [docker : Check Docker service status] ************************************
2025-07-07 09:11:55,649 p=68406 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003107", "end": "2025-07-07 13:11:55.634834", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:11:55.631727", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:11:55,649 p=68406 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:11:55,665 p=68406 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003457", "end": "2025-07-07 13:11:55.654285", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:11:55.650828", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:11:55,666 p=68406 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:11:55,692 p=68406 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003135", "end": "2025-07-07 13:11:55.675633", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:11:55.672498", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:11:55,692 p=68406 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:11:55,697 p=68406 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003216", "end": "2025-07-07 13:11:55.683102", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:11:55.679886", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:11:55,697 p=68406 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:11:55,788 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:11:55,801 p=68406 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] *********************************
2025-07-07 09:11:55,844 p=68406 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:11:55,872 p=68406 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:11:55,874 p=68406 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:11:55,881 p=68406 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:11:56,495 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:11:56,512 p=68406 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] ***********
2025-07-07 09:11:56,540 p=68406 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:11:56,724 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:11:56,729 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:11:56,741 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:11:56,757 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:11:56,770 p=68406 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ************************************
2025-07-07 09:11:56,797 p=68406 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:11:57,135 p=68406 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:12:30,160 p=68406 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:12:30,186 p=68406 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:12:30,190 p=68406 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:12:30,203 p=68406 u=gpadmin n=ansible | TASK [docker : Add user to docker group] ***************************************
2025-07-07 09:12:30,235 p=68406 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:12:30,253 p=68406 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:12:30,282 p=68406 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:12:30,283 p=68406 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:12:30,291 p=68406 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:12:30,300 p=68406 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] ******************************************
2025-07-07 09:12:31,256 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:12:31,269 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:12:31,286 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:12:31,296 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:12:31,442 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:12:31,615 p=68406 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] *************************************************
2025-07-07 09:12:31,625 p=68406 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************
2025-07-07 09:12:32,852 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:12:32,885 p=68406 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] ***********
2025-07-07 09:12:33,188 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:12:33,202 p=68406 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ****
2025-07-07 09:12:33,902 p=68406 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:12:33,916 p=68406 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] ***************
2025-07-07 09:12:34,205 p=68406 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:12:34,219 p=68406 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] **********************
2025-07-07 09:12:34,866 p=68406 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:12:34,882 p=68406 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] *************************************
2025-07-07 09:12:35,430 p=68406 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:12:35,444 p=68406 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ****************************
2025-07-07 09:12:35,765 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:12:35,780 p=68406 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] ******************************
2025-07-07 09:12:35,805 p=68406 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n06fe8891c68a   rayproject/ray:2.9.0   \"ray start --head --…\"   Less than a second ago   Up Less than a second             ray_head"
}
2025-07-07 09:12:35,819 p=68406 u=gpadmin n=ansible | TASK [ray_head : Wait for a moment to let Ray head node initialize] ************
2025-07-07 09:12:35,838 p=68406 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 09:12:35,839 p=68406 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 09:12:45,843 p=68406 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:12:45,857 p=68406 u=gpadmin n=ansible | TASK [ray_head : Check Ray head node status] ***********************************
2025-07-07 09:12:46,187 p=68406 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_head", "ray", "status"], "delta": "0:00:00.031145", "end": "2025-07-07 09:12:46.148218", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 09:12:46.117073", "stderr": "Error response from daemon: container 06fe8891c68a4b5175d92237309f2e2c86d23e332863704a5c165b3a1090a4b7 is not running", "stderr_lines": ["Error response from daemon: container 06fe8891c68a4b5175d92237309f2e2c86d23e332863704a5c165b3a1090a4b7 is not running"], "stdout": "", "stdout_lines": []}
2025-07-07 09:12:46,188 p=68406 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:12:46,201 p=68406 u=gpadmin n=ansible | TASK [ray_head : Print Ray head node status] ***********************************
2025-07-07 09:12:46,238 p=68406 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": []
}
2025-07-07 09:12:46,294 p=68406 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] **********************************************
2025-07-07 09:12:46,302 p=68406 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************
2025-07-07 09:12:47,116 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:12:47,126 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:12:47,129 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:12:47,227 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:12:47,290 p=68406 u=gpadmin n=ansible | TASK [ray_worker : Ensure Ray temporary directory exists on worker node] *******
2025-07-07 09:12:47,462 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:12:47,471 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:12:47,486 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:12:47,510 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:12:47,521 p=68406 u=gpadmin n=ansible | TASK [ray_worker : Stop and remove existing Ray worker container (idempotency)] ***
2025-07-07 09:12:47,798 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:12:47,819 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:12:47,838 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:12:47,840 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:12:47,851 p=68406 u=gpadmin n=ansible | TASK [ray_worker : Remove old Ray worker start script (idempotency)] ***********
2025-07-07 09:12:48,016 p=68406 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:12:48,047 p=68406 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:12:48,048 p=68406 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:12:48,071 p=68406 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:12:48,084 p=68406 u=gpadmin n=ansible | TASK [ray_worker : Copy Ray worker start script to worker node] ****************
2025-07-07 09:12:48,201 p=68406 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined
2025-07-07 09:12:48,201 p=68406 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined"}
2025-07-07 09:12:48,213 p=68406 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined
2025-07-07 09:12:48,213 p=68406 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined"}
2025-07-07 09:12:48,230 p=68406 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined
2025-07-07 09:12:48,230 p=68406 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined"}
2025-07-07 09:12:48,245 p=68406 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined
2025-07-07 09:12:48,245 p=68406 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "AnsibleUndefinedVariable: 'ray_head_ip' is undefined. 'ray_head_ip' is undefined"}
2025-07-07 09:12:48,246 p=68406 u=gpadmin n=ansible | PLAY RECAP *********************************************************************
2025-07-07 09:12:48,246 p=68406 u=gpadmin n=ansible | G-241                      : ok=14   changed=1    unreachable=0    failed=1    skipped=5    rescued=0    ignored=1   
2025-07-07 09:12:48,247 p=68406 u=gpadmin n=ansible | G-242                      : ok=14   changed=1    unreachable=0    failed=1    skipped=5    rescued=0    ignored=1   
2025-07-07 09:12:48,247 p=68406 u=gpadmin n=ansible | G-243                      : ok=14   changed=1    unreachable=0    failed=1    skipped=5    rescued=0    ignored=1   
2025-07-07 09:12:48,247 p=68406 u=gpadmin n=ansible | G-244                      : ok=14   changed=1    unreachable=0    failed=1    skipped=5    rescued=0    ignored=1   
2025-07-07 09:12:48,247 p=68406 u=gpadmin n=ansible | MASTER                     : ok=20   changed=4    unreachable=0    failed=0    skipped=6    rescued=0    ignored=1   
2025-07-07 09:14:51,045 p=71281 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 09:14:51,055 p=71281 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:14:52,519 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:14:52,712 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:14:52,755 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:14:52,772 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:14:52,808 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:14:52,887 p=71281 u=gpadmin n=ansible | TASK [common : Update apt cache] **********************************************************
2025-07-07 09:14:53,374 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:14:53,374 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:14:53,391 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:14:53,393 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:14:53,786 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:14:53,799 p=71281 u=gpadmin n=ansible | TASK [common : Install common packages] ***************************************************
2025-07-07 09:14:54,254 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:14:54,283 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:14:54,288 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:14:54,297 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:14:54,839 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:14:54,851 p=71281 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] ********************************************
2025-07-07 09:14:55,128 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:14:55,132 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:14:55,148 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:14:55,153 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:14:55,243 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:14:55,256 p=71281 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] *********************************************
2025-07-07 09:14:55,534 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:14:55,538 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:14:55,544 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:14:55,557 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:14:55,644 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:14:55,699 p=71281 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed (apt or snap)] ********************************
2025-07-07 09:14:55,967 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:14:55,975 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:14:55,979 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:14:55,982 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:14:56,084 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:14:56,101 p=71281 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] ********************************************************
2025-07-07 09:14:56,129 p=71281 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:14:56,145 p=71281 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:14:56,160 p=71281 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:14:56,174 p=71281 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:14:56,183 p=71281 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:14:56,191 p=71281 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] *************************************************
2025-07-07 09:14:56,225 p=71281 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:14:56,241 p=71281 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:14:56,254 p=71281 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:14:56,256 p=71281 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:14:56,263 p=71281 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:14:56,272 p=71281 u=gpadmin n=ansible | TASK [docker : Install Docker Engine] *****************************************************
2025-07-07 09:14:56,308 p=71281 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:14:56,324 p=71281 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:14:56,339 p=71281 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:14:56,339 p=71281 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:14:56,350 p=71281 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:14:56,360 p=71281 u=gpadmin n=ansible | TASK [docker : Check Docker service status] ***********************************************
2025-07-07 09:14:56,536 p=71281 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003172", "end": "2025-07-07 13:14:56.517265", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:14:56.514093", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:14:56,537 p=71281 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:14:56,563 p=71281 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003256", "end": "2025-07-07 13:14:56.547619", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:14:56.544363", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:14:56,563 p=71281 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:14:56,567 p=71281 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003293", "end": "2025-07-07 13:14:56.550626", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:14:56.547333", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:14:56,567 p=71281 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:14:56,578 p=71281 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003237", "end": "2025-07-07 13:14:56.560669", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:14:56.557432", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:14:56,578 p=71281 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:14:56,661 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:14:56,675 p=71281 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] ********************************************
2025-07-07 09:14:56,732 p=71281 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:14:56,752 p=71281 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:14:56,752 p=71281 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:14:56,765 p=71281 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:14:57,375 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:14:57,388 p=71281 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] **********************
2025-07-07 09:14:57,416 p=71281 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:14:57,586 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:14:57,604 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:14:57,620 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:14:57,627 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:14:57,641 p=71281 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ***********************************************
2025-07-07 09:14:57,666 p=71281 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:14:57,993 p=71281 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:15:31,052 p=71281 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:15:31,059 p=71281 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:15:31,073 p=71281 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:15:31,090 p=71281 u=gpadmin n=ansible | TASK [docker : Add user to docker group] **************************************************
2025-07-07 09:15:31,120 p=71281 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:15:31,138 p=71281 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:15:31,153 p=71281 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:15:31,168 p=71281 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:15:31,177 p=71281 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:15:31,186 p=71281 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] *****************************************************
2025-07-07 09:15:32,139 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:15:32,141 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:15:32,165 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:15:32,252 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:15:32,316 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:15:32,481 p=71281 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] ************************************************************
2025-07-07 09:15:32,491 p=71281 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:15:33,696 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:15:33,731 p=71281 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] **********************
2025-07-07 09:15:34,038 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:15:34,056 p=71281 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ***************
2025-07-07 09:15:34,784 p=71281 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:15:34,798 p=71281 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] **************************
2025-07-07 09:15:35,090 p=71281 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:15:35,108 p=71281 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] *********************************
2025-07-07 09:15:35,771 p=71281 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:15:35,786 p=71281 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] ************************************************
2025-07-07 09:15:36,316 p=71281 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:15:36,332 p=71281 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ***************************************
2025-07-07 09:15:36,641 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:15:36,656 p=71281 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] *****************************************
2025-07-07 09:15:36,676 p=71281 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n0c24e7a16b22   rayproject/ray:2.9.0   \"ray start --head --…\"   Less than a second ago   Up Less than a second             ray_head"
}
2025-07-07 09:15:36,691 p=71281 u=gpadmin n=ansible | TASK [ray_head : Wait for a moment to let Ray head node initialize] ***********************
2025-07-07 09:15:36,712 p=71281 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 09:15:36,713 p=71281 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 09:15:46,717 p=71281 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:15:46,731 p=71281 u=gpadmin n=ansible | TASK [ray_head : Check Ray head node status] **********************************************
2025-07-07 09:15:47,044 p=71281 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_head", "ray", "status"], "delta": "0:00:00.027208", "end": "2025-07-07 09:15:47.002038", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 09:15:46.974830", "stderr": "Error response from daemon: Container 0c24e7a16b227a4af7face02a67d1045352cf57d73e047da562abdc2c867f0f5 is restarting, wait until the container is running", "stderr_lines": ["Error response from daemon: Container 0c24e7a16b227a4af7face02a67d1045352cf57d73e047da562abdc2c867f0f5 is restarting, wait until the container is running"], "stdout": "", "stdout_lines": []}
2025-07-07 09:15:47,045 p=71281 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:15:47,058 p=71281 u=gpadmin n=ansible | TASK [ray_head : Print Ray head node status] **********************************************
2025-07-07 09:15:47,101 p=71281 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": []
}
2025-07-07 09:15:47,156 p=71281 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] *********************************************************
2025-07-07 09:15:47,164 p=71281 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:15:47,983 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:15:48,022 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:15:48,031 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:15:48,034 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:15:48,104 p=71281 u=gpadmin n=ansible | TASK [ray_worker : Ensure Ray temporary directory exists on worker node] ******************
2025-07-07 09:15:48,284 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:15:48,294 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:15:48,302 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:15:48,342 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:15:48,354 p=71281 u=gpadmin n=ansible | TASK [ray_worker : Stop and remove existing Ray worker container (idempotency)] ***********
2025-07-07 09:15:48,640 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:15:48,654 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:15:48,669 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:15:48,692 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:15:48,706 p=71281 u=gpadmin n=ansible | TASK [ray_worker : Remove old Ray worker start script (idempotency)] **********************
2025-07-07 09:15:48,865 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:15:48,895 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:15:48,908 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:15:48,937 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:15:48,950 p=71281 u=gpadmin n=ansible | TASK [ray_worker : Copy Ray worker start script to worker node] ***************************
2025-07-07 09:15:49,424 p=71281 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:15:49,451 p=71281 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:15:49,460 p=71281 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:15:49,496 p=71281 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:15:49,509 p=71281 u=gpadmin n=ansible | TASK [ray_worker : Start Ray worker container] ********************************************
2025-07-07 09:15:50,303 p=71281 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:15:50,314 p=71281 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:15:50,352 p=71281 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:15:50,359 p=71281 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:15:50,372 p=71281 u=gpadmin n=ansible | TASK [ray_worker : Display Ray worker container status] ***********************************
2025-07-07 09:15:50,556 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:15:50,582 p=71281 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:15:50,584 p=71281 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:15:50,604 p=71281 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:15:50,620 p=71281 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker container status] *************************************
2025-07-07 09:15:50,670 p=71281 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\nbbb255787f69   rayproject/ray:2.9.0   \"ray start --address…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:15:50,685 p=71281 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\n8bf31be322c8   rayproject/ray:2.9.0   \"ray start --address…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:15:50,687 p=71281 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\na70c0b2f3a47   rayproject/ray:2.9.0   \"ray start --address…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:15:50,703 p=71281 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\ncc12a5974174   rayproject/ray:2.9.0   \"ray start --address…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:15:50,711 p=71281 u=gpadmin n=ansible | TASK [ray_worker : Wait for a moment to let Ray worker node initialize] *******************
2025-07-07 09:15:50,726 p=71281 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 09:15:50,727 p=71281 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 09:16:00,731 p=71281 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:16:00,744 p=71281 u=gpadmin n=ansible | TASK [ray_worker : Check Ray worker node status] ******************************************
2025-07-07 09:16:00,952 p=71281 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.040181", "end": "2025-07-07 13:16:00.936726", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:16:00.896545", "stderr": "Error response from daemon: Container bbb255787f69ba53c66eb315c721cb0c4fd52c904381d597622f94d9a9ca5760 is restarting, wait until the container is running", "stderr_lines": ["Error response from daemon: Container bbb255787f69ba53c66eb315c721cb0c4fd52c904381d597622f94d9a9ca5760 is restarting, wait until the container is running"], "stdout": "", "stdout_lines": []}
2025-07-07 09:16:00,953 p=71281 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:16:00,960 p=71281 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.038623", "end": "2025-07-07 13:16:00.947397", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:16:00.908774", "stderr": "Error response from daemon: Container 8bf31be322c82a0ccc0e651878c1834e8007624499622fcf81b35eb327995584 is restarting, wait until the container is running", "stderr_lines": ["Error response from daemon: Container 8bf31be322c82a0ccc0e651878c1834e8007624499622fcf81b35eb327995584 is restarting, wait until the container is running"], "stdout": "", "stdout_lines": []}
2025-07-07 09:16:00,960 p=71281 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:16:00,986 p=71281 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.030878", "end": "2025-07-07 13:16:00.973172", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:16:00.942294", "stderr": "Error response from daemon: Container cc12a5974174be3941007e7849578c32ec5f375c1480483168966f3d4f4d1f51 is restarting, wait until the container is running", "stderr_lines": ["Error response from daemon: Container cc12a5974174be3941007e7849578c32ec5f375c1480483168966f3d4f4d1f51 is restarting, wait until the container is running"], "stdout": "", "stdout_lines": []}
2025-07-07 09:16:00,987 p=71281 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:16:00,988 p=71281 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.040949", "end": "2025-07-07 13:16:00.975052", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:16:00.934103", "stderr": "Error response from daemon: Container a70c0b2f3a47d37428a9ae96cb13261c03637d5ccfd46895c7cf75b89251aaad is restarting, wait until the container is running", "stderr_lines": ["Error response from daemon: Container a70c0b2f3a47d37428a9ae96cb13261c03637d5ccfd46895c7cf75b89251aaad is restarting, wait until the container is running"], "stdout": "", "stdout_lines": []}
2025-07-07 09:16:00,989 p=71281 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:16:01,004 p=71281 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker node status] ******************************************
2025-07-07 09:16:01,076 p=71281 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": []
}
2025-07-07 09:16:01,079 p=71281 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": []
}
2025-07-07 09:16:01,093 p=71281 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": []
}
2025-07-07 09:16:01,106 p=71281 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": []
}
2025-07-07 09:16:01,214 p=71281 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] ******************************************************
2025-07-07 09:16:01,234 p=71281 u=gpadmin n=ansible | TASK [Check Ray head node status] *********************************************************
2025-07-07 09:16:01,573 p=71281 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_head", "ray", "status"], "delta": "0:00:00.031375", "end": "2025-07-07 09:16:01.534976", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 09:16:01.503601", "stderr": "Error response from daemon: Container 0c24e7a16b227a4af7face02a67d1045352cf57d73e047da562abdc2c867f0f5 is restarting, wait until the container is running", "stderr_lines": ["Error response from daemon: Container 0c24e7a16b227a4af7face02a67d1045352cf57d73e047da562abdc2c867f0f5 is restarting, wait until the container is running"], "stdout": "", "stdout_lines": []}
2025-07-07 09:16:01,573 p=71281 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:16:01,589 p=71281 u=gpadmin n=ansible | TASK [Display Ray cluster status] *********************************************************
2025-07-07 09:16:01,610 p=71281 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:16:01,662 p=71281 u=gpadmin n=ansible | TASK [Display Ray cluster error] **********************************************************
2025-07-07 09:16:01,687 p=71281 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Ray cluster status check failed: Error response from daemon: Container 0c24e7a16b227a4af7face02a67d1045352cf57d73e047da562abdc2c867f0f5 is restarting, wait until the container is running"
}
2025-07-07 09:16:01,714 p=71281 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 09:16:01,714 p=71281 u=gpadmin n=ansible | G-241                      : ok=21   changed=3    unreachable=0    failed=0    skipped=5    rescued=0    ignored=2   
2025-07-07 09:16:01,714 p=71281 u=gpadmin n=ansible | G-242                      : ok=20   changed=3    unreachable=0    failed=0    skipped=5    rescued=0    ignored=2   
2025-07-07 09:16:01,714 p=71281 u=gpadmin n=ansible | G-243                      : ok=20   changed=3    unreachable=0    failed=0    skipped=5    rescued=0    ignored=2   
2025-07-07 09:16:01,714 p=71281 u=gpadmin n=ansible | G-244                      : ok=20   changed=3    unreachable=0    failed=0    skipped=5    rescued=0    ignored=2   
2025-07-07 09:16:01,715 p=71281 u=gpadmin n=ansible | MASTER                     : ok=22   changed=4    unreachable=0    failed=0    skipped=7    rescued=0    ignored=2   
2025-07-07 09:20:53,565 p=77484 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 09:20:53,574 p=77484 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:20:55,092 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:20:55,254 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:20:55,285 p=77484 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:20:55,302 p=77484 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:20:55,307 p=77484 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:20:55,379 p=77484 u=gpadmin n=ansible | TASK [common : Update apt cache] **********************************************************
2025-07-07 09:20:55,853 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:20:55,854 p=77484 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:20:55,854 p=77484 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:20:55,859 p=77484 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:20:56,244 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:20:56,257 p=77484 u=gpadmin n=ansible | TASK [common : Install common packages] ***************************************************
2025-07-07 09:20:56,710 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:20:56,739 p=77484 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:20:56,749 p=77484 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:20:56,774 p=77484 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:20:57,306 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:20:57,319 p=77484 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] ********************************************
2025-07-07 09:20:57,597 p=77484 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:20:57,604 p=77484 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:20:57,612 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:20:57,615 p=77484 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:20:57,704 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:20:57,717 p=77484 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] *********************************************
2025-07-07 09:20:58,005 p=77484 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:20:58,006 p=77484 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:20:58,008 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:20:58,016 p=77484 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:20:58,102 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:20:58,156 p=77484 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed (apt or snap)] ********************************
2025-07-07 09:20:58,418 p=77484 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:20:58,419 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:20:58,432 p=77484 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:20:58,452 p=77484 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:20:58,538 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:20:58,548 p=77484 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] ********************************************************
2025-07-07 09:20:58,584 p=77484 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:20:58,599 p=77484 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:20:58,614 p=77484 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:20:58,615 p=77484 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:20:58,623 p=77484 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:20:58,631 p=77484 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] *************************************************
2025-07-07 09:20:58,663 p=77484 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:20:58,678 p=77484 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:20:58,691 p=77484 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:20:58,693 p=77484 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:20:58,704 p=77484 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:20:58,712 p=77484 u=gpadmin n=ansible | TASK [docker : Install Docker Engine] *****************************************************
2025-07-07 09:20:58,747 p=77484 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:20:58,761 p=77484 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:20:58,775 p=77484 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:20:58,777 p=77484 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:20:58,787 p=77484 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:20:58,796 p=77484 u=gpadmin n=ansible | TASK [docker : Check Docker service status] ***********************************************
2025-07-07 09:20:58,965 p=77484 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003077", "end": "2025-07-07 13:20:58.950336", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:20:58.947259", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:20:58,966 p=77484 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:20:58,978 p=77484 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003439", "end": "2025-07-07 13:20:58.966517", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:20:58.963078", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:20:58,978 p=77484 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:20:58,984 p=77484 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003195", "end": "2025-07-07 13:20:58.972358", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:20:58.969163", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:20:58,984 p=77484 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:20:58,996 p=77484 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003210", "end": "2025-07-07 13:20:58.983028", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:20:58.979818", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:20:58,996 p=77484 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:20:59,092 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:20:59,105 p=77484 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] ********************************************
2025-07-07 09:20:59,149 p=77484 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:20:59,164 p=77484 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:20:59,183 p=77484 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:20:59,192 p=77484 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:20:59,782 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:20:59,795 p=77484 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] **********************
2025-07-07 09:20:59,823 p=77484 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:21:00,001 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:21:00,008 p=77484 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:21:00,019 p=77484 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:21:00,032 p=77484 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:21:00,046 p=77484 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ***********************************************
2025-07-07 09:21:00,074 p=77484 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:21:00,419 p=77484 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:21:33,446 p=77484 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:21:33,470 p=77484 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:21:33,587 p=77484 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:21:33,604 p=77484 u=gpadmin n=ansible | TASK [docker : Add user to docker group] **************************************************
2025-07-07 09:21:33,633 p=77484 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:21:33,651 p=77484 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:21:33,666 p=77484 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:21:33,681 p=77484 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:21:33,688 p=77484 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:21:33,697 p=77484 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] *****************************************************
2025-07-07 09:21:34,698 p=77484 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:21:34,709 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:21:34,723 p=77484 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:21:34,730 p=77484 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:21:34,855 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:21:35,020 p=77484 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] ************************************************************
2025-07-07 09:21:35,031 p=77484 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:21:36,238 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:21:36,273 p=77484 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] **********************
2025-07-07 09:21:36,579 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:21:36,593 p=77484 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ***************
2025-07-07 09:21:37,331 p=77484 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:21:37,345 p=77484 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] **************************
2025-07-07 09:21:37,631 p=77484 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:21:37,647 p=77484 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] *********************************
2025-07-07 09:21:38,323 p=77484 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:21:38,337 p=77484 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] ************************************************
2025-07-07 09:21:38,873 p=77484 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:21:38,888 p=77484 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ***************************************
2025-07-07 09:21:39,220 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:21:39,235 p=77484 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] *****************************************
2025-07-07 09:21:39,260 p=77484 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\nc5fce4a320ba   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_head"
}
2025-07-07 09:21:39,274 p=77484 u=gpadmin n=ansible | TASK [ray_head : Wait for a moment to let Ray head node initialize] ***********************
2025-07-07 09:21:39,295 p=77484 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 09:21:39,295 p=77484 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 09:21:49,300 p=77484 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:21:49,313 p=77484 u=gpadmin n=ansible | TASK [ray_head : Check Ray head node status] **********************************************
2025-07-07 09:21:51,341 p=77484 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_head", "ray", "status"], "delta": "0:00:01.742450", "end": "2025-07-07 09:21:51.301048", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 09:21:49.558598", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:21:51,341 p=77484 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:21:51,356 p=77484 u=gpadmin n=ansible | TASK [ray_head : Print Ray head node status] **********************************************
2025-07-07 09:21:51,398 p=77484 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": []
}
2025-07-07 09:21:51,453 p=77484 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] *********************************************************
2025-07-07 09:21:51,461 p=77484 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:21:52,270 p=77484 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:21:52,273 p=77484 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:21:52,297 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:21:52,342 p=77484 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:21:52,403 p=77484 u=gpadmin n=ansible | TASK [ray_worker : Ensure Ray temporary directory exists on worker node] ******************
2025-07-07 09:21:52,580 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:21:52,585 p=77484 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:21:52,608 p=77484 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:21:52,631 p=77484 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:21:52,644 p=77484 u=gpadmin n=ansible | TASK [ray_worker : Stop and remove existing Ray worker container (idempotency)] ***********
2025-07-07 09:21:52,998 p=77484 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:21:53,025 p=77484 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:21:53,033 p=77484 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:21:53,049 p=77484 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:21:53,062 p=77484 u=gpadmin n=ansible | TASK [ray_worker : Remove old Ray worker start script (idempotency)] **********************
2025-07-07 09:21:53,235 p=77484 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:21:53,252 p=77484 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:21:53,266 p=77484 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:21:53,310 p=77484 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:21:53,326 p=77484 u=gpadmin n=ansible | TASK [ray_worker : Copy Ray worker start script to worker node] ***************************
2025-07-07 09:21:53,793 p=77484 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:21:53,798 p=77484 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:21:53,816 p=77484 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:21:53,836 p=77484 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:21:53,847 p=77484 u=gpadmin n=ansible | TASK [ray_worker : Start Ray worker container] ********************************************
2025-07-07 09:21:54,378 p=77484 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:21:54,398 p=77484 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:21:54,407 p=77484 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:21:54,410 p=77484 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:21:54,422 p=77484 u=gpadmin n=ansible | TASK [ray_worker : Display Ray worker container status] ***********************************
2025-07-07 09:21:54,607 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:21:54,629 p=77484 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:21:54,630 p=77484 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:21:54,657 p=77484 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:21:54,673 p=77484 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker container status] *************************************
2025-07-07 09:21:54,703 p=77484 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n5cf4ece49ed0   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:21:54,735 p=77484 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n0ca8007acb7c   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:21:54,738 p=77484 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n8e955edacf6d   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:21:54,753 p=77484 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\ne7ae4bbe8e02   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:21:54,761 p=77484 u=gpadmin n=ansible | TASK [ray_worker : Wait for a moment to let Ray worker node initialize] *******************
2025-07-07 09:21:54,782 p=77484 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 09:21:54,782 p=77484 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 09:22:04,787 p=77484 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:22:04,799 p=77484 u=gpadmin n=ansible | TASK [ray_worker : Check Ray worker node status] ******************************************
2025-07-07 09:22:05,867 p=77484 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.894081", "end": "2025-07-07 13:22:05.850604", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:22:04.956523", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:22:05,867 p=77484 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:22:05,884 p=77484 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.891351", "end": "2025-07-07 13:22:05.869934", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:22:04.978583", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:22:05,884 p=77484 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:22:05,906 p=77484 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.924160", "end": "2025-07-07 13:22:05.893251", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:22:04.969091", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:22:05,906 p=77484 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:22:05,914 p=77484 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.896672", "end": "2025-07-07 13:22:05.900375", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:22:05.003703", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:22:05,914 p=77484 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:22:05,927 p=77484 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker node status] ******************************************
2025-07-07 09:22:05,975 p=77484 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": []
}
2025-07-07 09:22:05,999 p=77484 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": []
}
2025-07-07 09:22:06,007 p=77484 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": []
}
2025-07-07 09:22:06,034 p=77484 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": []
}
2025-07-07 09:22:06,153 p=77484 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] ******************************************************
2025-07-07 09:22:06,173 p=77484 u=gpadmin n=ansible | TASK [Check Ray head node status] *********************************************************
2025-07-07 09:22:08,210 p=77484 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_head", "ray", "status"], "delta": "0:00:01.742913", "end": "2025-07-07 09:22:08.168884", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 09:22:06.425971", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:22:08,211 p=77484 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:22:08,226 p=77484 u=gpadmin n=ansible | TASK [Display Ray cluster status] *********************************************************
2025-07-07 09:22:08,248 p=77484 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:22:08,300 p=77484 u=gpadmin n=ansible | TASK [Display Ray cluster error] **********************************************************
2025-07-07 09:22:08,327 p=77484 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Ray cluster status check failed: Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."
}
2025-07-07 09:22:08,354 p=77484 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 09:22:08,354 p=77484 u=gpadmin n=ansible | G-241                      : ok=21   changed=5    unreachable=0    failed=0    skipped=5    rescued=0    ignored=2   
2025-07-07 09:22:08,354 p=77484 u=gpadmin n=ansible | G-242                      : ok=20   changed=5    unreachable=0    failed=0    skipped=5    rescued=0    ignored=2   
2025-07-07 09:22:08,354 p=77484 u=gpadmin n=ansible | G-243                      : ok=20   changed=5    unreachable=0    failed=0    skipped=5    rescued=0    ignored=2   
2025-07-07 09:22:08,355 p=77484 u=gpadmin n=ansible | G-244                      : ok=20   changed=5    unreachable=0    failed=0    skipped=5    rescued=0    ignored=2   
2025-07-07 09:22:08,355 p=77484 u=gpadmin n=ansible | MASTER                     : ok=22   changed=4    unreachable=0    failed=0    skipped=7    rescued=0    ignored=2   
2025-07-07 09:30:35,488 p=83788 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 09:30:35,497 p=83788 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:30:37,007 p=83788 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:30:37,125 p=83788 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:30:37,151 p=83788 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:30:37,262 p=83788 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:30:37,291 p=83788 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:30:37,541 p=83788 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] ************************************************************
2025-07-07 09:30:37,551 p=83788 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:30:38,763 p=83788 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:30:38,832 p=83788 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] *********************************************************
2025-07-07 09:30:38,841 p=83788 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:30:39,694 p=83788 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:30:39,703 p=83788 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:30:39,706 p=83788 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:30:39,711 p=83788 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:30:39,904 p=83788 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] ******************************************************
2025-07-07 09:30:39,944 p=83788 u=gpadmin n=ansible | PLAY [Deploy Monitoring Stack] ************************************************************
2025-07-07 09:30:39,954 p=83788 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:30:40,776 p=83788 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:30:40,818 p=83788 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:30:40,832 p=83788 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:30:40,862 p=83788 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:30:41,151 p=83788 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:30:41,354 p=83788 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 09:30:41,354 p=83788 u=gpadmin n=ansible | G-241                      : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 09:30:41,354 p=83788 u=gpadmin n=ansible | G-242                      : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 09:30:41,354 p=83788 u=gpadmin n=ansible | G-243                      : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 09:30:41,354 p=83788 u=gpadmin n=ansible | G-244                      : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 09:30:41,354 p=83788 u=gpadmin n=ansible | MASTER                     : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 09:30:51,240 p=84582 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] *********************************************************
2025-07-07 09:30:51,250 p=84582 u=gpadmin n=ansible | TASK [Gathering Facts] ***************************************************************************
2025-07-07 09:30:52,354 p=84582 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:30:52,369 p=84582 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:30:52,372 p=84582 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:30:52,419 p=84582 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:30:52,747 p=84582 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:30:52,827 p=84582 u=gpadmin n=ansible | TASK [common : Update apt cache] *****************************************************************
2025-07-07 09:30:53,313 p=84582 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:30:53,320 p=84582 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:30:53,322 p=84582 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:30:53,322 p=84582 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:30:53,728 p=84582 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:30:53,741 p=84582 u=gpadmin n=ansible | TASK [common : Install common packages] **********************************************************
2025-07-07 09:30:54,189 p=84582 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:30:54,214 p=84582 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:30:54,233 p=84582 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:30:54,236 p=84582 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:30:54,784 p=84582 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:30:54,797 p=84582 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] ***************************************************
2025-07-07 09:30:55,107 p=84582 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:30:55,108 p=84582 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:30:55,108 p=84582 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:30:55,114 p=84582 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:30:55,217 p=84582 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:30:55,230 p=84582 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] ****************************************************
2025-07-07 09:30:55,505 p=84582 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:30:55,513 p=84582 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:30:55,517 p=84582 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:30:55,517 p=84582 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:30:55,621 p=84582 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:30:55,677 p=84582 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed (apt or snap)] ***************************************
2025-07-07 09:30:55,963 p=84582 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:30:55,964 p=84582 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:30:55,966 p=84582 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:30:55,967 p=84582 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:30:56,062 p=84582 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:30:56,075 p=84582 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] ***************************************************************
2025-07-07 09:30:56,104 p=84582 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:30:56,122 p=84582 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:30:56,151 p=84582 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:30:56,154 p=84582 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:30:56,161 p=84582 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:30:56,170 p=84582 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] ********************************************************
2025-07-07 09:30:56,209 p=84582 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:30:56,223 p=84582 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:30:56,239 p=84582 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:30:56,239 p=84582 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:30:56,247 p=84582 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:30:56,256 p=84582 u=gpadmin n=ansible | TASK [docker : Install Docker Engine] ************************************************************
2025-07-07 09:30:56,275 p=84582 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:30:56,303 p=84582 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:30:56,319 p=84582 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:30:56,321 p=84582 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:30:56,332 p=84582 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:30:56,341 p=84582 u=gpadmin n=ansible | TASK [docker : Check Docker service status] ******************************************************
2025-07-07 09:30:56,522 p=84582 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003171", "end": "2025-07-07 13:30:56.507222", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:30:56.504051", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:30:56,522 p=84582 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:30:56,532 p=84582 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003326", "end": "2025-07-07 13:30:56.520210", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:30:56.516884", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:30:56,532 p=84582 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:30:56,537 p=84582 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003092", "end": "2025-07-07 13:30:56.527361", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:30:56.524269", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:30:56,537 p=84582 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:30:56,558 p=84582 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003139", "end": "2025-07-07 13:30:56.545576", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:30:56.542437", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:30:56,558 p=84582 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:30:56,644 p=84582 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:30:56,658 p=84582 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] ***************************************************
2025-07-07 09:30:56,702 p=84582 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:30:56,717 p=84582 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:30:56,733 p=84582 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:30:56,744 p=84582 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:30:57,368 p=84582 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:30:57,382 p=84582 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] *****************************
2025-07-07 09:30:57,410 p=84582 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:30:57,578 p=84582 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:30:57,605 p=84582 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:30:57,608 p=84582 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:30:57,627 p=84582 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:30:57,644 p=84582 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ******************************************************
2025-07-07 09:30:57,679 p=84582 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:30:58,007 p=84582 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:31:31,070 p=84582 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:31:31,078 p=84582 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:31:31,083 p=84582 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:31:31,097 p=84582 u=gpadmin n=ansible | TASK [docker : Add user to docker group] *********************************************************
2025-07-07 09:31:31,124 p=84582 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:31:31,141 p=84582 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:31:31,170 p=84582 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:31:31,172 p=84582 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:31:31,183 p=84582 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:31:31,192 p=84582 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] ************************************************************
2025-07-07 09:31:32,179 p=84582 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:31:32,186 p=84582 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:31:32,202 p=84582 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:31:32,250 p=84582 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:31:32,315 p=84582 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:31:32,403 p=84582 u=gpadmin n=ansible |  [ERROR]: User interrupted execution

2025-07-07 09:33:40,769 p=86688 u=gpadmin n=ansible | Using /home/gpadmin/cursor-projects/02-Ray-Deploy/ansible.cfg as config file
2025-07-07 09:33:41,061 p=86688 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 09:33:41,070 p=86688 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:33:42,600 p=86688 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:33:42,621 p=86688 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:33:42,733 p=86688 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:33:42,786 p=86688 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:33:43,790 p=86688 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:33:43,868 p=86688 u=gpadmin n=ansible | TASK [common : Update apt cache] **********************************************************
2025-07-07 09:33:44,350 p=86688 u=gpadmin n=ansible | ok: [G-241] => {"cache_update_time": 1751893770, "cache_updated": false, "changed": false}
2025-07-07 09:33:44,355 p=86688 u=gpadmin n=ansible | ok: [G-244] => {"cache_update_time": 1751892994, "cache_updated": false, "changed": false}
2025-07-07 09:33:44,359 p=86688 u=gpadmin n=ansible | ok: [G-243] => {"cache_update_time": 1751892994, "cache_updated": false, "changed": false}
2025-07-07 09:33:44,363 p=86688 u=gpadmin n=ansible | ok: [G-242] => {"cache_update_time": 1751892995, "cache_updated": false, "changed": false}
2025-07-07 09:33:44,748 p=86688 u=gpadmin n=ansible | ok: [MASTER] => {"cache_update_time": 1751892911, "cache_updated": false, "changed": false}
2025-07-07 09:33:44,761 p=86688 u=gpadmin n=ansible | TASK [common : Install common packages] ***************************************************
2025-07-07 09:33:45,230 p=86688 u=gpadmin n=ansible | ok: [G-241] => {"cache_update_time": 1751893770, "cache_updated": false, "changed": false}
2025-07-07 09:33:45,243 p=86688 u=gpadmin n=ansible | ok: [G-243] => {"cache_update_time": 1751892994, "cache_updated": false, "changed": false}
2025-07-07 09:33:45,255 p=86688 u=gpadmin n=ansible | ok: [G-242] => {"cache_update_time": 1751892995, "cache_updated": false, "changed": false}
2025-07-07 09:33:45,263 p=86688 u=gpadmin n=ansible | ok: [G-244] => {"cache_update_time": 1751892994, "cache_updated": false, "changed": false}
2025-07-07 09:33:45,819 p=86688 u=gpadmin n=ansible | ok: [MASTER] => {"cache_update_time": 1751892911, "cache_updated": false, "changed": false}
2025-07-07 09:33:45,832 p=86688 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] ********************************************
2025-07-07 09:33:46,131 p=86688 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:33:46,133 p=86688 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:33:46,133 p=86688 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:33:46,150 p=86688 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:33:46,247 p=86688 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:33:46,260 p=86688 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] *********************************************
2025-07-07 09:33:46,550 p=86688 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "stat": {"atime": 1751893510.3693118, "attr_flags": "e", "attributes": ["extents"], "block_size": 4096, "blocks": 8, "charset": "binary", "ctime": 1751894514.102447, "dev": 64512, "device_type": 0, "executable": true, "exists": true, "gid": 1000, "gr_name": "gpadmin", "inode": 16384017, "isblk": false, "ischr": false, "isdir": true, "isfifo": false, "isgid": false, "islnk": false, "isreg": false, "issock": false, "isuid": false, "mimetype": "inode/directory", "mode": "0777", "mtime": 1751894514.102447, "nlink": 2, "path": "/home/gpadmin/ray_temp", "pw_name": "gpadmin", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 4096, "uid": 1000, "version": "1435149338", "wgrp": true, "woth": true, "writeable": true, "wusr": true, "xgrp": true, "xoth": true, "xusr": true}}
2025-07-07 09:33:46,555 p=86688 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "stat": {"atime": 1751893510.3611715, "attr_flags": "e", "attributes": ["extents"], "block_size": 4096, "blocks": 8, "charset": "binary", "ctime": 1751894514.1356137, "dev": 64512, "device_type": 0, "executable": true, "exists": true, "gid": 1000, "gr_name": "gpadmin", "inode": 39059468, "isblk": false, "ischr": false, "isdir": true, "isfifo": false, "isgid": false, "islnk": false, "isreg": false, "issock": false, "isuid": false, "mimetype": "inode/directory", "mode": "0777", "mtime": 1751894514.1356137, "nlink": 2, "path": "/home/gpadmin/ray_temp", "pw_name": "gpadmin", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 4096, "uid": 1000, "version": "1105510836", "wgrp": true, "woth": true, "writeable": true, "wusr": true, "xgrp": true, "xoth": true, "xusr": true}}
2025-07-07 09:33:46,565 p=86688 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "stat": {"atime": 1751893510.357805, "attr_flags": "e", "attributes": ["extents"], "block_size": 4096, "blocks": 8, "charset": "binary", "ctime": 1751894514.1358685, "dev": 64512, "device_type": 0, "executable": true, "exists": true, "gid": 1000, "gr_name": "gpadmin", "inode": 9699340, "isblk": false, "ischr": false, "isdir": true, "isfifo": false, "isgid": false, "islnk": false, "isreg": false, "issock": false, "isuid": false, "mimetype": "inode/directory", "mode": "0777", "mtime": 1751894514.1358685, "nlink": 2, "path": "/home/gpadmin/ray_temp", "pw_name": "gpadmin", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 4096, "uid": 1000, "version": "3381391995", "wgrp": true, "woth": true, "writeable": true, "wusr": true, "xgrp": true, "xoth": true, "xusr": true}}
2025-07-07 09:33:46,569 p=86688 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "stat": {"atime": 1751893510.3523293, "attr_flags": "e", "attributes": ["extents"], "block_size": 4096, "blocks": 8, "charset": "binary", "ctime": 1751894514.1077826, "dev": 64512, "device_type": 0, "executable": true, "exists": true, "gid": 1000, "gr_name": "gpadmin", "inode": 118226955, "isblk": false, "ischr": false, "isdir": true, "isfifo": false, "isgid": false, "islnk": false, "isreg": false, "issock": false, "isuid": false, "mimetype": "inode/directory", "mode": "0777", "mtime": 1751894514.1077826, "nlink": 2, "path": "/home/gpadmin/ray_temp", "pw_name": "gpadmin", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 4096, "uid": 1000, "version": "2833573626", "wgrp": true, "woth": true, "writeable": true, "wusr": true, "xgrp": true, "xoth": true, "xusr": true}}
2025-07-07 09:33:46,672 p=86688 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "stat": {"atime": 1751894238.2056296, "attr_flags": "e", "attributes": ["extents"], "block_size": 4096, "blocks": 16, "charset": "binary", "ctime": 1751894498.643538, "dev": 64512, "device_type": 0, "executable": true, "exists": true, "gid": 1000, "gr_name": "gpadmin", "inode": 37618228, "isblk": false, "ischr": false, "isdir": true, "isfifo": false, "isgid": false, "islnk": false, "isreg": false, "issock": false, "isuid": false, "mimetype": "inode/directory", "mode": "0777", "mtime": 1751894498.643538, "nlink": 6, "path": "/home/gpadmin/ray_temp", "pw_name": "gpadmin", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 4096, "uid": 1000, "version": "3803985524", "wgrp": true, "woth": true, "writeable": true, "wusr": true, "xgrp": true, "xoth": true, "xusr": true}}
2025-07-07 09:33:46,727 p=86688 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed (apt or snap)] ********************************
2025-07-07 09:33:47,003 p=86688 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "cmd": "which docker || echo \"not_installed\"", "delta": "0:00:00.001786", "end": "2025-07-07 13:33:46.987913", "msg": "", "rc": 0, "start": "2025-07-07 13:33:46.986127", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:33:47,004 p=86688 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "cmd": "which docker || echo \"not_installed\"", "delta": "0:00:00.001856", "end": "2025-07-07 13:33:46.991923", "msg": "", "rc": 0, "start": "2025-07-07 13:33:46.990067", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:33:47,011 p=86688 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "cmd": "which docker || echo \"not_installed\"", "delta": "0:00:00.001872", "end": "2025-07-07 13:33:46.999001", "msg": "", "rc": 0, "start": "2025-07-07 13:33:46.997129", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:33:47,018 p=86688 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "cmd": "which docker || echo \"not_installed\"", "delta": "0:00:00.001757", "end": "2025-07-07 13:33:47.008366", "msg": "", "rc": 0, "start": "2025-07-07 13:33:47.006609", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:33:47,120 p=86688 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "cmd": "which docker || echo \"not_installed\"", "delta": "0:00:00.006987", "end": "2025-07-07 09:33:47.077831", "msg": "", "rc": 0, "start": "2025-07-07 09:33:47.070844", "stderr": "", "stderr_lines": [], "stdout": "/usr/local/bin/docker", "stdout_lines": ["/usr/local/bin/docker"]}
2025-07-07 09:33:47,133 p=86688 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] ********************************************************
2025-07-07 09:33:47,160 p=86688 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,177 p=86688 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,206 p=86688 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,208 p=86688 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,218 p=86688 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,227 p=86688 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] *************************************************
2025-07-07 09:33:47,249 p=86688 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,279 p=86688 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,294 p=86688 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,296 p=86688 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,306 p=86688 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,315 p=86688 u=gpadmin n=ansible | TASK [docker : Install Docker Engine] *****************************************************
2025-07-07 09:33:47,348 p=86688 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,363 p=86688 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,377 p=86688 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,379 p=86688 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,390 p=86688 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,399 p=86688 u=gpadmin n=ansible | TASK [docker : Check Docker service status] ***********************************************
2025-07-07 09:33:47,579 p=86688 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003126", "end": "2025-07-07 13:33:47.563558", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:33:47.560432", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:33:47,579 p=86688 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:33:47,584 p=86688 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003403", "end": "2025-07-07 13:33:47.572818", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:33:47.569415", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:33:47,584 p=86688 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:33:47,596 p=86688 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003271", "end": "2025-07-07 13:33:47.585402", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:33:47.582131", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:33:47,596 p=86688 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:33:47,614 p=86688 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003289", "end": "2025-07-07 13:33:47.601134", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:33:47.597845", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:33:47,615 p=86688 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:33:47,733 p=86688 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.012575", "end": "2025-07-07 09:33:47.689663", "msg": "", "rc": 0, "start": "2025-07-07 09:33:47.677088", "stderr": "", "stderr_lines": [], "stdout": "active", "stdout_lines": ["active"]}
2025-07-07 09:33:47,746 p=86688 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] ********************************************
2025-07-07 09:33:47,790 p=86688 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "docker_service_status.rc == 0", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,821 p=86688 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "docker_service_status.rc == 0", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,822 p=86688 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "docker_service_status.rc == 0", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:47,832 p=86688 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "docker_service_status.rc == 0", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:48,492 p=86688 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "enabled": true, "name": "docker", "state": "started", "status": {"ActiveEnterTimestamp": "Sat 2025-07-05 12:10:24 EDT", "ActiveEnterTimestampMonotonic": "14500501", "ActiveExitTimestampMonotonic": "0", "ActiveState": "active", "After": "time-set.target network-online.target system.slice basic.target nss-lookup.target docker.socket firewalld.service systemd-journald.socket containerd.service sysinit.target", "AllowIsolate": "no", "AssertResult": "yes", "AssertTimestamp": "Sat 2025-07-05 12:10:23 EDT", "AssertTimestampMonotonic": "13516583", "Before": "shutdown.target multi-user.target", "BlockIOAccounting": "no", "BlockIOWeight": "[not set]", "CPUAccounting": "yes", "CPUAffinityFromNUMA": "no", "CPUQuotaPerSecUSec": "infinity", "CPUQuotaPeriodUSec": "infinity", "CPUSchedulingPolicy": "0", "CPUSchedulingPriority": "0", "CPUSchedulingResetOnFork": "no", "CPUShares": "[not set]", "CPUUsageNSec": "92402717000", "CPUWeight": "[not set]", "CacheDirectoryMode": "0755", "CanFreeze": "yes", "CanIsolate": "no", "CanReload": "yes", "CanStart": "yes", "CanStop": "yes", "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore", "CleanResult": "success", "CollectMode": "inactive", "ConditionResult": "yes", "ConditionTimestamp": "Sat 2025-07-05 12:10:23 EDT", "ConditionTimestampMonotonic": "13516581", "ConfigurationDirectoryMode": "0755", "Conflicts": "shutdown.target", "ControlGroup": "/system.slice/docker.service", "ControlGroupId": "7651", "ControlPID": "0", "CoredumpFilter": "0x33", "CoredumpReceive": "no", "DefaultDependencies": "yes", "DefaultMemoryLow": "0", "DefaultMemoryMin": "0", "DefaultStartupMemoryLow": "0", "Delegate": "yes", "DelegateControllers": "cpu cpuset io memory pids", "Description": "Docker Application Container Engine", "DevicePolicy": "auto", "Documentation": "https://docs.docker.com", "DynamicUser": "no", "EffectiveCPUs": "0-31", "EffectiveMemoryNodes": "0", "ExecMainCode": "0", "ExecMainExitTimestampMonotonic": "0", "ExecMainPID": "2018", "ExecMainStartTimestamp": "Sat 2025-07-05 12:10:23 EDT", "ExecMainStartTimestampMonotonic": "13518391", "ExecMainStatus": "0", "ExecReload": "{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecReloadEx": "{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStart": "{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStartEx": "{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExitType": "main", "ExtensionImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "FailureAction": "none", "FileDescriptorStoreMax": "0", "FileDescriptorStorePreserve": "restart", "FinalKillSignal": "9", "FragmentPath": "/usr/lib/systemd/system/docker.service", "FreezerState": "running", "GID": "[not set]", "GuessMainPID": "yes", "IOAccounting": "no", "IOReadBytes": "[not set]", "IOReadOperations": "[not set]", "IOSchedulingClass": "2", "IOSchedulingPriority": "4", "IOWeight": "[not set]", "IOWriteBytes": "[not set]", "IOWriteOperations": "[not set]", "IPAccounting": "no", "IPEgressBytes": "[no data]", "IPEgressPackets": "[no data]", "IPIngressBytes": "[no data]", "IPIngressPackets": "[no data]", "Id": "docker.service", "IgnoreOnIsolate": "no", "IgnoreSIGPIPE": "yes", "InactiveEnterTimestampMonotonic": "0", "InactiveExitTimestamp": "Sat 2025-07-05 12:10:23 EDT", "InactiveExitTimestampMonotonic": "13518558", "InvocationID": "efe8e95911d1408482a24e5be07cc2d7", "JobRunningTimeoutUSec": "infinity", "JobTimeoutAction": "none", "JobTimeoutUSec": "infinity", "KeyringMode": "private", "KillMode": "process", "KillSignal": "15", "LimitAS": "infinity", "LimitASSoft": "infinity", "LimitCORE": "infinity", "LimitCORESoft": "infinity", "LimitCPU": "infinity", "LimitCPUSoft": "infinity", "LimitDATA": "infinity", "LimitDATASoft": "infinity", "LimitFSIZE": "infinity", "LimitFSIZESoft": "infinity", "LimitLOCKS": "infinity", "LimitLOCKSSoft": "infinity", "LimitMEMLOCK": "8388608", "LimitMEMLOCKSoft": "8388608", "LimitMSGQUEUE": "819200", "LimitMSGQUEUESoft": "819200", "LimitNICE": "0", "LimitNICESoft": "0", "LimitNOFILE": "524288", "LimitNOFILESoft": "1024", "LimitNPROC": "infinity", "LimitNPROCSoft": "infinity", "LimitRSS": "infinity", "LimitRSSSoft": "infinity", "LimitRTPRIO": "0", "LimitRTPRIOSoft": "0", "LimitRTTIME": "infinity", "LimitRTTIMESoft": "infinity", "LimitSIGPENDING": "514099", "LimitSIGPENDINGSoft": "514099", "LimitSTACK": "infinity", "LimitSTACKSoft": "8388608", "LoadState": "loaded", "LockPersonality": "no", "LogLevelMax": "-1", "LogRateLimitBurst": "0", "LogRateLimitIntervalUSec": "0", "LogsDirectoryMode": "0755", "MainPID": "2018", "ManagedOOMMemoryPressure": "auto", "ManagedOOMMemoryPressureLimit": "0", "ManagedOOMPreference": "none", "ManagedOOMSwap": "auto", "MemoryAccounting": "yes", "MemoryAvailable": "126046736384", "MemoryCurrent": "2735316992", "MemoryDenyWriteExecute": "no", "MemoryHigh": "infinity", "MemoryKSM": "no", "MemoryLimit": "infinity", "MemoryLow": "0", "MemoryMax": "infinity", "MemoryMin": "0", "MemoryPeak": "3042648064", "MemoryPressureThresholdUSec": "200ms", "MemoryPressureWatch": "auto", "MemorySwapCurrent": "0", "MemorySwapMax": "infinity", "MemorySwapPeak": "0", "MemoryZSwapCurrent": "0", "MemoryZSwapMax": "infinity", "MountAPIVFS": "no", "MountImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "NFileDescriptorStore": "0", "NRestarts": "0", "NUMAPolicy": "n/a", "Names": "docker.service", "NeedDaemonReload": "yes", "Nice": "0", "NoNewPrivileges": "no", "NonBlocking": "no", "NotifyAccess": "main", "OOMPolicy": "continue", "OOMScoreAdjust": "-500", "OnFailureJobMode": "replace", "OnSuccessJobMode": "fail", "Perpetual": "no", "PrivateDevices": "no", "PrivateIPC": "no", "PrivateMounts": "no", "PrivateNetwork": "no", "PrivateTmp": "no", "PrivateUsers": "no", "ProcSubset": "all", "ProtectClock": "no", "ProtectControlGroups": "no", "ProtectHome": "no", "ProtectHostname": "no", "ProtectKernelLogs": "no", "ProtectKernelModules": "no", "ProtectKernelTunables": "no", "ProtectProc": "default", "ProtectSystem": "no", "RefuseManualStart": "no", "RefuseManualStop": "no", "ReloadResult": "success", "ReloadSignal": "1", "RemainAfterExit": "no", "RemoveIPC": "no", "Requires": "sysinit.target docker.socket system.slice", "Restart": "always", "RestartKillSignal": "15", "RestartMaxDelayUSec": "infinity", "RestartMode": "normal", "RestartSteps": "0", "RestartUSec": "2s", "RestartUSecNext": "2s", "RestrictNamespaces": "no", "RestrictRealtime": "no", "RestrictSUIDSGID": "no", "Result": "success", "RootDirectoryStartOnly": "no", "RootEphemeral": "no", "RootImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "RuntimeDirectoryMode": "0755", "RuntimeDirectoryPreserve": "no", "RuntimeMaxUSec": "infinity", "RuntimeRandomizedExtraUSec": "0", "SameProcessGroup": "no", "SecureBits": "0", "SendSIGHUP": "no", "SendSIGKILL": "yes", "SetLoginEnvironment": "no", "Slice": "system.slice", "StandardError": "inherit", "StandardInput": "null", "StandardOutput": "journal", "StartLimitAction": "none", "StartLimitBurst": "3", "StartLimitIntervalUSec": "1min", "StartupBlockIOWeight": "[not set]", "StartupCPUShares": "[not set]", "StartupCPUWeight": "[not set]", "StartupIOWeight": "[not set]", "StartupMemoryHigh": "infinity", "StartupMemoryLow": "0", "StartupMemoryMax": "infinity", "StartupMemorySwapMax": "infinity", "StartupMemoryZSwapMax": "infinity", "StateChangeTimestamp": "Sat 2025-07-05 12:10:24 EDT", "StateChangeTimestampMonotonic": "14500501", "StateDirectoryMode": "0755", "StatusErrno": "0", "StopWhenUnneeded": "no", "SubState": "running", "SuccessAction": "none", "SurviveFinalKillSignal": "no", "SyslogFacility": "3", "SyslogLevel": "6", "SyslogLevelPrefix": "yes", "SyslogPriority": "30", "SystemCallErrorNumber": "2147483646", "TTYReset": "no", "TTYVHangup": "no", "TTYVTDisallocate": "no", "TasksAccounting": "yes", "TasksCurrent": "34", "TasksMax": "infinity", "TimeoutAbortUSec": "1min 30s", "TimeoutCleanUSec": "infinity", "TimeoutStartFailureMode": "terminate", "TimeoutStartUSec": "infinity", "TimeoutStopFailureMode": "terminate", "TimeoutStopUSec": "1min 30s", "TimerSlackNSec": "50000", "Transient": "no", "TriggeredBy": "docker.socket", "Type": "notify", "UID": "[not set]", "UMask": "0022", "UnitFilePreset": "enabled", "UnitFileState": "enabled", "UtmpMode": "init", "WantedBy": "multi-user.target", "Wants": "containerd.service network-online.target", "WatchdogSignal": "6", "WatchdogTimestampMonotonic": "0", "WatchdogUSec": "0"}}
2025-07-07 09:33:48,506 p=86688 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] **********************
2025-07-07 09:33:48,533 p=86688 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_service_status.rc != 0 and docker_check.stdout != \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:48,698 p=86688 u=gpadmin n=ansible | ok: [G-241] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.013680", "end": "2025-07-07 13:33:48.680625", "msg": "", "rc": 0, "start": "2025-07-07 13:33:48.666945", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:33:48,710 p=86688 u=gpadmin n=ansible | ok: [G-242] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.014289", "end": "2025-07-07 13:33:48.695857", "msg": "", "rc": 0, "start": "2025-07-07 13:33:48.681568", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:33:48,726 p=86688 u=gpadmin n=ansible | ok: [G-243] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.013947", "end": "2025-07-07 13:33:48.709936", "msg": "", "rc": 0, "start": "2025-07-07 13:33:48.695989", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:33:48,745 p=86688 u=gpadmin n=ansible | ok: [G-244] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.014116", "end": "2025-07-07 13:33:48.730609", "msg": "", "rc": 0, "start": "2025-07-07 13:33:48.716493", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:33:48,760 p=86688 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ***********************************************
2025-07-07 09:33:48,789 p=86688 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_service_status.rc != 0 and docker_check.stdout != \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:33:49,114 p=86688 u=gpadmin n=ansible | changed: [G-241] => {"attempts": 1, "changed": true, "cmd": ["snap", "start", "docker"], "delta": "0:00:00.152368", "end": "2025-07-07 13:33:49.091400", "msg": "", "rc": 0, "start": "2025-07-07 13:33:48.939032", "stderr": "", "stderr_lines": [], "stdout": "Started.", "stdout_lines": ["Started."]}
2025-07-07 09:33:59,030 p=86688 u=gpadmin n=ansible |  [ERROR]: User interrupted execution

2025-07-07 09:34:16,651 p=87684 u=gpadmin n=ansible | Using /home/gpadmin/cursor-projects/02-Ray-Deploy/ansible.cfg as config file
2025-07-07 09:34:16,938 p=87684 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] ****************************************************
2025-07-07 09:34:16,946 p=87684 u=gpadmin n=ansible | TASK [Gathering Facts] **********************************************************************
2025-07-07 09:34:17,964 p=87684 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:34:18,021 p=87684 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:34:18,053 p=87684 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:34:18,092 p=87684 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:34:18,365 p=87684 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:34:18,448 p=87684 u=gpadmin n=ansible | TASK [common : Update apt cache] ************************************************************
2025-07-07 09:34:18,923 p=87684 u=gpadmin n=ansible | ok: [G-241] => {"cache_update_time": 1751893770, "cache_updated": false, "changed": false}
2025-07-07 09:34:18,925 p=87684 u=gpadmin n=ansible | ok: [G-243] => {"cache_update_time": 1751892994, "cache_updated": false, "changed": false}
2025-07-07 09:34:18,938 p=87684 u=gpadmin n=ansible | ok: [G-244] => {"cache_update_time": 1751892994, "cache_updated": false, "changed": false}
2025-07-07 09:34:18,945 p=87684 u=gpadmin n=ansible | ok: [G-242] => {"cache_update_time": 1751892995, "cache_updated": false, "changed": false}
2025-07-07 09:34:19,316 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {"cache_update_time": 1751892911, "cache_updated": false, "changed": false}
2025-07-07 09:34:19,329 p=87684 u=gpadmin n=ansible | TASK [common : Install common packages] *****************************************************
2025-07-07 09:34:19,786 p=87684 u=gpadmin n=ansible | ok: [G-241] => {"cache_update_time": 1751893770, "cache_updated": false, "changed": false}
2025-07-07 09:34:19,816 p=87684 u=gpadmin n=ansible | ok: [G-243] => {"cache_update_time": 1751892994, "cache_updated": false, "changed": false}
2025-07-07 09:34:19,835 p=87684 u=gpadmin n=ansible | ok: [G-244] => {"cache_update_time": 1751892994, "cache_updated": false, "changed": false}
2025-07-07 09:34:19,844 p=87684 u=gpadmin n=ansible | ok: [G-242] => {"cache_update_time": 1751892995, "cache_updated": false, "changed": false}
2025-07-07 09:34:20,374 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {"cache_update_time": 1751892911, "cache_updated": false, "changed": false}
2025-07-07 09:34:20,387 p=87684 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] **********************************************
2025-07-07 09:34:20,666 p=87684 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:34:20,674 p=87684 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:34:20,675 p=87684 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:34:20,675 p=87684 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:34:20,780 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:34:20,793 p=87684 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] ***********************************************
2025-07-07 09:34:21,074 p=87684 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "stat": {"atime": 1751893510.3693118, "attr_flags": "e", "attributes": ["extents"], "block_size": 4096, "blocks": 8, "charset": "binary", "ctime": 1751894514.102447, "dev": 64512, "device_type": 0, "executable": true, "exists": true, "gid": 1000, "gr_name": "gpadmin", "inode": 16384017, "isblk": false, "ischr": false, "isdir": true, "isfifo": false, "isgid": false, "islnk": false, "isreg": false, "issock": false, "isuid": false, "mimetype": "inode/directory", "mode": "0777", "mtime": 1751894514.102447, "nlink": 2, "path": "/home/gpadmin/ray_temp", "pw_name": "gpadmin", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 4096, "uid": 1000, "version": "1435149338", "wgrp": true, "woth": true, "writeable": true, "wusr": true, "xgrp": true, "xoth": true, "xusr": true}}
2025-07-07 09:34:21,076 p=87684 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "stat": {"atime": 1751893510.3611715, "attr_flags": "e", "attributes": ["extents"], "block_size": 4096, "blocks": 8, "charset": "binary", "ctime": 1751894514.1356137, "dev": 64512, "device_type": 0, "executable": true, "exists": true, "gid": 1000, "gr_name": "gpadmin", "inode": 39059468, "isblk": false, "ischr": false, "isdir": true, "isfifo": false, "isgid": false, "islnk": false, "isreg": false, "issock": false, "isuid": false, "mimetype": "inode/directory", "mode": "0777", "mtime": 1751894514.1356137, "nlink": 2, "path": "/home/gpadmin/ray_temp", "pw_name": "gpadmin", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 4096, "uid": 1000, "version": "1105510836", "wgrp": true, "woth": true, "writeable": true, "wusr": true, "xgrp": true, "xoth": true, "xusr": true}}
2025-07-07 09:34:21,080 p=87684 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "stat": {"atime": 1751893510.357805, "attr_flags": "e", "attributes": ["extents"], "block_size": 4096, "blocks": 8, "charset": "binary", "ctime": 1751894514.1358685, "dev": 64512, "device_type": 0, "executable": true, "exists": true, "gid": 1000, "gr_name": "gpadmin", "inode": 9699340, "isblk": false, "ischr": false, "isdir": true, "isfifo": false, "isgid": false, "islnk": false, "isreg": false, "issock": false, "isuid": false, "mimetype": "inode/directory", "mode": "0777", "mtime": 1751894514.1358685, "nlink": 2, "path": "/home/gpadmin/ray_temp", "pw_name": "gpadmin", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 4096, "uid": 1000, "version": "3381391995", "wgrp": true, "woth": true, "writeable": true, "wusr": true, "xgrp": true, "xoth": true, "xusr": true}}
2025-07-07 09:34:21,088 p=87684 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "stat": {"atime": 1751893510.3523293, "attr_flags": "e", "attributes": ["extents"], "block_size": 4096, "blocks": 8, "charset": "binary", "ctime": 1751894514.1077826, "dev": 64512, "device_type": 0, "executable": true, "exists": true, "gid": 1000, "gr_name": "gpadmin", "inode": 118226955, "isblk": false, "ischr": false, "isdir": true, "isfifo": false, "isgid": false, "islnk": false, "isreg": false, "issock": false, "isuid": false, "mimetype": "inode/directory", "mode": "0777", "mtime": 1751894514.1077826, "nlink": 2, "path": "/home/gpadmin/ray_temp", "pw_name": "gpadmin", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 4096, "uid": 1000, "version": "2833573626", "wgrp": true, "woth": true, "writeable": true, "wusr": true, "xgrp": true, "xoth": true, "xusr": true}}
2025-07-07 09:34:21,192 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "stat": {"atime": 1751894238.2056296, "attr_flags": "e", "attributes": ["extents"], "block_size": 4096, "blocks": 16, "charset": "binary", "ctime": 1751894498.643538, "dev": 64512, "device_type": 0, "executable": true, "exists": true, "gid": 1000, "gr_name": "gpadmin", "inode": 37618228, "isblk": false, "ischr": false, "isdir": true, "isfifo": false, "isgid": false, "islnk": false, "isreg": false, "issock": false, "isuid": false, "mimetype": "inode/directory", "mode": "0777", "mtime": 1751894498.643538, "nlink": 6, "path": "/home/gpadmin/ray_temp", "pw_name": "gpadmin", "readable": true, "rgrp": true, "roth": true, "rusr": true, "size": 4096, "uid": 1000, "version": "3803985524", "wgrp": true, "woth": true, "writeable": true, "wusr": true, "xgrp": true, "xoth": true, "xusr": true}}
2025-07-07 09:34:21,249 p=87684 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed (apt or snap)] **********************************
2025-07-07 09:34:21,519 p=87684 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "cmd": "which docker || echo \"not_installed\"", "delta": "0:00:00.001807", "end": "2025-07-07 13:34:21.508027", "msg": "", "rc": 0, "start": "2025-07-07 13:34:21.506220", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:34:21,521 p=87684 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "cmd": "which docker || echo \"not_installed\"", "delta": "0:00:00.001828", "end": "2025-07-07 13:34:21.508977", "msg": "", "rc": 0, "start": "2025-07-07 13:34:21.507149", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:34:21,528 p=87684 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "cmd": "which docker || echo \"not_installed\"", "delta": "0:00:00.001850", "end": "2025-07-07 13:34:21.517188", "msg": "", "rc": 0, "start": "2025-07-07 13:34:21.515338", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:34:21,529 p=87684 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "cmd": "which docker || echo \"not_installed\"", "delta": "0:00:00.001826", "end": "2025-07-07 13:34:21.513725", "msg": "", "rc": 0, "start": "2025-07-07 13:34:21.511899", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:34:21,632 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "cmd": "which docker || echo \"not_installed\"", "delta": "0:00:00.007764", "end": "2025-07-07 09:34:21.590292", "msg": "", "rc": 0, "start": "2025-07-07 09:34:21.582528", "stderr": "", "stderr_lines": [], "stdout": "/usr/local/bin/docker", "stdout_lines": ["/usr/local/bin/docker"]}
2025-07-07 09:34:21,646 p=87684 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] **********************************************************
2025-07-07 09:34:21,674 p=87684 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,692 p=87684 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,721 p=87684 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,722 p=87684 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,729 p=87684 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,738 p=87684 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] ***************************************************
2025-07-07 09:34:21,762 p=87684 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,777 p=87684 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,807 p=87684 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,809 p=87684 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,815 p=87684 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,824 p=87684 u=gpadmin n=ansible | TASK [docker : Install Docker Engine] *******************************************************
2025-07-07 09:34:21,843 p=87684 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,872 p=87684 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,887 p=87684 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,888 p=87684 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,901 p=87684 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:21,910 p=87684 u=gpadmin n=ansible | TASK [docker : Check Docker service status] *************************************************
2025-07-07 09:34:22,091 p=87684 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003038", "end": "2025-07-07 13:34:22.076001", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:34:22.072963", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:34:22,091 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:34:22,105 p=87684 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003236", "end": "2025-07-07 13:34:22.095728", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:34:22.092492", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:34:22,105 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:34:22,110 p=87684 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003973", "end": "2025-07-07 13:34:22.098368", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:34:22.094395", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:34:22,110 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:34:22,135 p=87684 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003086", "end": "2025-07-07 13:34:22.123910", "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:34:22.120824", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:34:22,136 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:34:22,202 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.011478", "end": "2025-07-07 09:34:22.159183", "msg": "", "rc": 0, "start": "2025-07-07 09:34:22.147705", "stderr": "", "stderr_lines": [], "stdout": "active", "stdout_lines": ["active"]}
2025-07-07 09:34:22,216 p=87684 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] **********************************************
2025-07-07 09:34:22,261 p=87684 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "docker_service_status.rc == 0", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:22,290 p=87684 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "docker_service_status.rc == 0", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:22,292 p=87684 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "docker_service_status.rc == 0", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:22,303 p=87684 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "docker_service_status.rc == 0", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:22,917 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "enabled": true, "name": "docker", "state": "started", "status": {"ActiveEnterTimestamp": "Sat 2025-07-05 12:10:24 EDT", "ActiveEnterTimestampMonotonic": "14500501", "ActiveExitTimestampMonotonic": "0", "ActiveState": "active", "After": "time-set.target network-online.target system.slice basic.target nss-lookup.target docker.socket firewalld.service systemd-journald.socket containerd.service sysinit.target", "AllowIsolate": "no", "AssertResult": "yes", "AssertTimestamp": "Sat 2025-07-05 12:10:23 EDT", "AssertTimestampMonotonic": "13516583", "Before": "shutdown.target multi-user.target", "BlockIOAccounting": "no", "BlockIOWeight": "[not set]", "CPUAccounting": "yes", "CPUAffinityFromNUMA": "no", "CPUQuotaPerSecUSec": "infinity", "CPUQuotaPeriodUSec": "infinity", "CPUSchedulingPolicy": "0", "CPUSchedulingPriority": "0", "CPUSchedulingResetOnFork": "no", "CPUShares": "[not set]", "CPUUsageNSec": "92404653000", "CPUWeight": "[not set]", "CacheDirectoryMode": "0755", "CanFreeze": "yes", "CanIsolate": "no", "CanReload": "yes", "CanStart": "yes", "CanStop": "yes", "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore", "CleanResult": "success", "CollectMode": "inactive", "ConditionResult": "yes", "ConditionTimestamp": "Sat 2025-07-05 12:10:23 EDT", "ConditionTimestampMonotonic": "13516581", "ConfigurationDirectoryMode": "0755", "Conflicts": "shutdown.target", "ControlGroup": "/system.slice/docker.service", "ControlGroupId": "7651", "ControlPID": "0", "CoredumpFilter": "0x33", "CoredumpReceive": "no", "DefaultDependencies": "yes", "DefaultMemoryLow": "0", "DefaultMemoryMin": "0", "DefaultStartupMemoryLow": "0", "Delegate": "yes", "DelegateControllers": "cpu cpuset io memory pids", "Description": "Docker Application Container Engine", "DevicePolicy": "auto", "Documentation": "https://docs.docker.com", "DynamicUser": "no", "EffectiveCPUs": "0-31", "EffectiveMemoryNodes": "0", "ExecMainCode": "0", "ExecMainExitTimestampMonotonic": "0", "ExecMainPID": "2018", "ExecMainStartTimestamp": "Sat 2025-07-05 12:10:23 EDT", "ExecMainStartTimestampMonotonic": "13518391", "ExecMainStatus": "0", "ExecReload": "{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecReloadEx": "{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStart": "{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStartEx": "{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExitType": "main", "ExtensionImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "FailureAction": "none", "FileDescriptorStoreMax": "0", "FileDescriptorStorePreserve": "restart", "FinalKillSignal": "9", "FragmentPath": "/usr/lib/systemd/system/docker.service", "FreezerState": "running", "GID": "[not set]", "GuessMainPID": "yes", "IOAccounting": "no", "IOReadBytes": "[not set]", "IOReadOperations": "[not set]", "IOSchedulingClass": "2", "IOSchedulingPriority": "4", "IOWeight": "[not set]", "IOWriteBytes": "[not set]", "IOWriteOperations": "[not set]", "IPAccounting": "no", "IPEgressBytes": "[no data]", "IPEgressPackets": "[no data]", "IPIngressBytes": "[no data]", "IPIngressPackets": "[no data]", "Id": "docker.service", "IgnoreOnIsolate": "no", "IgnoreSIGPIPE": "yes", "InactiveEnterTimestampMonotonic": "0", "InactiveExitTimestamp": "Sat 2025-07-05 12:10:23 EDT", "InactiveExitTimestampMonotonic": "13518558", "InvocationID": "efe8e95911d1408482a24e5be07cc2d7", "JobRunningTimeoutUSec": "infinity", "JobTimeoutAction": "none", "JobTimeoutUSec": "infinity", "KeyringMode": "private", "KillMode": "process", "KillSignal": "15", "LimitAS": "infinity", "LimitASSoft": "infinity", "LimitCORE": "infinity", "LimitCORESoft": "infinity", "LimitCPU": "infinity", "LimitCPUSoft": "infinity", "LimitDATA": "infinity", "LimitDATASoft": "infinity", "LimitFSIZE": "infinity", "LimitFSIZESoft": "infinity", "LimitLOCKS": "infinity", "LimitLOCKSSoft": "infinity", "LimitMEMLOCK": "8388608", "LimitMEMLOCKSoft": "8388608", "LimitMSGQUEUE": "819200", "LimitMSGQUEUESoft": "819200", "LimitNICE": "0", "LimitNICESoft": "0", "LimitNOFILE": "524288", "LimitNOFILESoft": "1024", "LimitNPROC": "infinity", "LimitNPROCSoft": "infinity", "LimitRSS": "infinity", "LimitRSSSoft": "infinity", "LimitRTPRIO": "0", "LimitRTPRIOSoft": "0", "LimitRTTIME": "infinity", "LimitRTTIMESoft": "infinity", "LimitSIGPENDING": "514099", "LimitSIGPENDINGSoft": "514099", "LimitSTACK": "infinity", "LimitSTACKSoft": "8388608", "LoadState": "loaded", "LockPersonality": "no", "LogLevelMax": "-1", "LogRateLimitBurst": "0", "LogRateLimitIntervalUSec": "0", "LogsDirectoryMode": "0755", "MainPID": "2018", "ManagedOOMMemoryPressure": "auto", "ManagedOOMMemoryPressureLimit": "0", "ManagedOOMPreference": "none", "ManagedOOMSwap": "auto", "MemoryAccounting": "yes", "MemoryAvailable": "125988040704", "MemoryCurrent": "2734592000", "MemoryDenyWriteExecute": "no", "MemoryHigh": "infinity", "MemoryKSM": "no", "MemoryLimit": "infinity", "MemoryLow": "0", "MemoryMax": "infinity", "MemoryMin": "0", "MemoryPeak": "3042648064", "MemoryPressureThresholdUSec": "200ms", "MemoryPressureWatch": "auto", "MemorySwapCurrent": "0", "MemorySwapMax": "infinity", "MemorySwapPeak": "0", "MemoryZSwapCurrent": "0", "MemoryZSwapMax": "infinity", "MountAPIVFS": "no", "MountImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "NFileDescriptorStore": "0", "NRestarts": "0", "NUMAPolicy": "n/a", "Names": "docker.service", "NeedDaemonReload": "yes", "Nice": "0", "NoNewPrivileges": "no", "NonBlocking": "no", "NotifyAccess": "main", "OOMPolicy": "continue", "OOMScoreAdjust": "-500", "OnFailureJobMode": "replace", "OnSuccessJobMode": "fail", "Perpetual": "no", "PrivateDevices": "no", "PrivateIPC": "no", "PrivateMounts": "no", "PrivateNetwork": "no", "PrivateTmp": "no", "PrivateUsers": "no", "ProcSubset": "all", "ProtectClock": "no", "ProtectControlGroups": "no", "ProtectHome": "no", "ProtectHostname": "no", "ProtectKernelLogs": "no", "ProtectKernelModules": "no", "ProtectKernelTunables": "no", "ProtectProc": "default", "ProtectSystem": "no", "RefuseManualStart": "no", "RefuseManualStop": "no", "ReloadResult": "success", "ReloadSignal": "1", "RemainAfterExit": "no", "RemoveIPC": "no", "Requires": "sysinit.target docker.socket system.slice", "Restart": "always", "RestartKillSignal": "15", "RestartMaxDelayUSec": "infinity", "RestartMode": "normal", "RestartSteps": "0", "RestartUSec": "2s", "RestartUSecNext": "2s", "RestrictNamespaces": "no", "RestrictRealtime": "no", "RestrictSUIDSGID": "no", "Result": "success", "RootDirectoryStartOnly": "no", "RootEphemeral": "no", "RootImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "RuntimeDirectoryMode": "0755", "RuntimeDirectoryPreserve": "no", "RuntimeMaxUSec": "infinity", "RuntimeRandomizedExtraUSec": "0", "SameProcessGroup": "no", "SecureBits": "0", "SendSIGHUP": "no", "SendSIGKILL": "yes", "SetLoginEnvironment": "no", "Slice": "system.slice", "StandardError": "inherit", "StandardInput": "null", "StandardOutput": "journal", "StartLimitAction": "none", "StartLimitBurst": "3", "StartLimitIntervalUSec": "1min", "StartupBlockIOWeight": "[not set]", "StartupCPUShares": "[not set]", "StartupCPUWeight": "[not set]", "StartupIOWeight": "[not set]", "StartupMemoryHigh": "infinity", "StartupMemoryLow": "0", "StartupMemoryMax": "infinity", "StartupMemorySwapMax": "infinity", "StartupMemoryZSwapMax": "infinity", "StateChangeTimestamp": "Sat 2025-07-05 12:10:24 EDT", "StateChangeTimestampMonotonic": "14500501", "StateDirectoryMode": "0755", "StatusErrno": "0", "StopWhenUnneeded": "no", "SubState": "running", "SuccessAction": "none", "SurviveFinalKillSignal": "no", "SyslogFacility": "3", "SyslogLevel": "6", "SyslogLevelPrefix": "yes", "SyslogPriority": "30", "SystemCallErrorNumber": "2147483646", "TTYReset": "no", "TTYVHangup": "no", "TTYVTDisallocate": "no", "TasksAccounting": "yes", "TasksCurrent": "34", "TasksMax": "infinity", "TimeoutAbortUSec": "1min 30s", "TimeoutCleanUSec": "infinity", "TimeoutStartFailureMode": "terminate", "TimeoutStartUSec": "infinity", "TimeoutStopFailureMode": "terminate", "TimeoutStopUSec": "1min 30s", "TimerSlackNSec": "50000", "Transient": "no", "TriggeredBy": "docker.socket", "Type": "notify", "UID": "[not set]", "UMask": "0022", "UnitFilePreset": "enabled", "UnitFileState": "enabled", "UtmpMode": "init", "WantedBy": "multi-user.target", "Wants": "containerd.service network-online.target", "WatchdogSignal": "6", "WatchdogTimestampMonotonic": "0", "WatchdogUSec": "0"}}
2025-07-07 09:34:22,930 p=87684 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] ************************
2025-07-07 09:34:22,957 p=87684 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_service_status.rc != 0 and docker_check.stdout != \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:23,131 p=87684 u=gpadmin n=ansible | ok: [G-241] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.013995", "end": "2025-07-07 13:34:23.111257", "msg": "", "rc": 0, "start": "2025-07-07 13:34:23.097262", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:34:23,160 p=87684 u=gpadmin n=ansible | ok: [G-242] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.014292", "end": "2025-07-07 13:34:23.145445", "msg": "", "rc": 0, "start": "2025-07-07 13:34:23.131153", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:34:23,173 p=87684 u=gpadmin n=ansible | ok: [G-243] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.013857", "end": "2025-07-07 13:34:23.156179", "msg": "", "rc": 0, "start": "2025-07-07 13:34:23.142322", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:34:23,191 p=87684 u=gpadmin n=ansible | ok: [G-244] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.014142", "end": "2025-07-07 13:34:23.175992", "msg": "", "rc": 0, "start": "2025-07-07 13:34:23.161850", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:34:23,206 p=87684 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] *************************************************
2025-07-07 09:34:23,235 p=87684 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_service_status.rc != 0 and docker_check.stdout != \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:23,583 p=87684 u=gpadmin n=ansible | changed: [G-241] => {"attempts": 1, "changed": true, "cmd": ["snap", "start", "docker"], "delta": "0:00:00.155799", "end": "2025-07-07 13:34:23.556089", "msg": "", "rc": 0, "start": "2025-07-07 13:34:23.400290", "stderr": "", "stderr_lines": [], "stdout": "Started.", "stdout_lines": ["Started."]}
2025-07-07 09:34:56,615 p=87684 u=gpadmin n=ansible | changed: [G-243] => {"attempts": 1, "changed": true, "cmd": ["snap", "start", "docker"], "delta": "0:00:33.174522", "end": "2025-07-07 13:34:56.595777", "msg": "", "rc": 0, "start": "2025-07-07 13:34:23.421255", "stderr": "", "stderr_lines": [], "stdout": "Started.", "stdout_lines": ["Started."]}
2025-07-07 09:34:56,619 p=87684 u=gpadmin n=ansible | changed: [G-242] => {"attempts": 1, "changed": true, "cmd": ["snap", "start", "docker"], "delta": "0:00:33.190890", "end": "2025-07-07 13:34:56.581436", "msg": "", "rc": 0, "start": "2025-07-07 13:34:23.390546", "stderr": "", "stderr_lines": [], "stdout": "Started.", "stdout_lines": ["Started."]}
2025-07-07 09:34:56,635 p=87684 u=gpadmin n=ansible | changed: [G-244] => {"attempts": 1, "changed": true, "cmd": ["snap", "start", "docker"], "delta": "0:00:33.184395", "end": "2025-07-07 13:34:56.615170", "msg": "", "rc": 0, "start": "2025-07-07 13:34:23.430775", "stderr": "", "stderr_lines": [], "stdout": "Started.", "stdout_lines": ["Started."]}
2025-07-07 09:34:56,649 p=87684 u=gpadmin n=ansible | TASK [docker : Add user to docker group] ****************************************************
2025-07-07 09:34:56,678 p=87684 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:56,695 p=87684 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:56,710 p=87684 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:56,725 p=87684 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:56,734 p=87684 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "docker_check.stdout == \"not_installed\"", "skip_reason": "Conditional result was False"}
2025-07-07 09:34:56,744 p=87684 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] *******************************************************
2025-07-07 09:34:57,707 p=87684 u=gpadmin n=ansible | ok: [G-241] => {"actions": ["Pulled image rayproject/ray:2.9.0"], "attempts": 1, "changed": false, "image": {"Architecture": "amd64", "Author": "", "Comment": "buildkit.dockerfile.v0", "Config": {"AttachStderr": false, "AttachStdin": false, "AttachStdout": false, "Cmd": ["/bin/bash"], "Domainname": "", "Entrypoint": null, "Env": ["PATH=/home/ray/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin", "TZ=America/Los_Angeles", "LC_ALL=C.UTF-8", "LANG=C.UTF-8", "HOME=/home/ray"], "Hostname": "", "Image": "", "Labels": {"org.opencontainers.image.ref.name": "ubuntu", "org.opencontainers.image.version": "20.04"}, "OnBuild": null, "OpenStdin": false, "Shell": ["/bin/bash", "-c"], "StdinOnce": false, "Tty": false, "User": "1000", "Volumes": null, "WorkingDir": "/home/ray"}, "Created": "2023-12-18T23:18:11.720022433Z", "DockerVersion": "", "GraphDriver": {"Data": {"LowerDir": "/var/snap/docker/common/var-lib-docker/overlay2/a157404022c301bf1042d1e62800e03f8f4fe2b7be93c7c862c4e6eaae6df90d/diff:/var/snap/docker/common/var-lib-docker/overlay2/7c8ec995834d750508107df22fcd821c183169d30dc35bf7f25768aafcec562f/diff:/var/snap/docker/common/var-lib-docker/overlay2/cf10812da96d74f053a2e4968cfc6941e3d96eceba026c544287965b76b11e57/diff:/var/snap/docker/common/var-lib-docker/overlay2/f4dcfcf3dd1b2b040b7cb7b267f240f3686975fd8d6fac33709856daabb41b42/diff:/var/snap/docker/common/var-lib-docker/overlay2/7cf6f6e71e3a19a458ebc8cf88afa29c77dc9892a871de388cbfaf5964d96cbf/diff:/var/snap/docker/common/var-lib-docker/overlay2/1d577dbe5e25f3d389369f6e2e824ffcbb2a0107e0627d268ab88ca6390b5f86/diff:/var/snap/docker/common/var-lib-docker/overlay2/9d67b08f2f1954424e8abfb97648daa554b1b21bb6417ef6376bf124e3a12ad6/diff:/var/snap/docker/common/var-lib-docker/overlay2/0450e96ef997832d69ec77b754d4a5b215a634b6e6b5e35778800f2c825680ed/diff", "MergedDir": "/var/snap/docker/common/var-lib-docker/overlay2/e4664178e075ff8052b24dbedd1573538a1750df5588b99a0ea53180d91ddc58/merged", "UpperDir": "/var/snap/docker/common/var-lib-docker/overlay2/e4664178e075ff8052b24dbedd1573538a1750df5588b99a0ea53180d91ddc58/diff", "WorkDir": "/var/snap/docker/common/var-lib-docker/overlay2/e4664178e075ff8052b24dbedd1573538a1750df5588b99a0ea53180d91ddc58/work"}, "Name": "overlay2"}, "Id": "sha256:ee1c865e02572031f5c68be4184f21d929d5cd233289c83b2a10b9ce455eb75f", "Metadata": {"LastTagTime": "0001-01-01T00:00:00Z"}, "Os": "linux", "Parent": "", "RepoDigests": ["rayproject/ray@sha256:e64546fb5c3233bb0f33608e186e285c52cdd7440cae1af18f7fcde1c04e49f2"], "RepoTags": ["rayproject/ray:2.9.0"], "RootFS": {"Layers": ["sha256:3a03f09d212915b240e9d216069aba5652ed4765c7e4b098c65e71860d47b8e1", "sha256:db6680cb129d39fad6405b4fc83b612de4a228c95b437893af46d8b781ddd0f6", "sha256:81cb020e881ca3c7a87233ac403feea44f8964fbc18340a87a7f559591d90e94", "sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef", "sha256:180b33c2b848ae705b9f079c279f3b62207be4e6501dc80b2be118c7e140589c", "sha256:9e011b19e2eba55cf9b938fe9206e8c5af911843bc393ebf86f279514657df0e", "sha256:0575f54a9aec6019d39c0e95722f3a2062f95a28c3b1501918e0053c0696e7e8", "sha256:a7f9e2973cd370320020a1e5642e5665c2dff4da3384c3d0b5357ff48a25f84c", "sha256:4b4d53180b9fe6e2ae8a264f4a35d2b589415cf9cdff556f84e3b313ed4ec543"], "Type": "layers"}, "Size": 2195262629}}
2025-07-07 09:34:57,708 p=87684 u=gpadmin n=ansible | ok: [G-242] => {"actions": ["Pulled image rayproject/ray:2.9.0"], "attempts": 1, "changed": false, "image": {"Architecture": "amd64", "Author": "", "Comment": "buildkit.dockerfile.v0", "Config": {"AttachStderr": false, "AttachStdin": false, "AttachStdout": false, "Cmd": ["/bin/bash"], "Domainname": "", "Entrypoint": null, "Env": ["PATH=/home/ray/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin", "TZ=America/Los_Angeles", "LC_ALL=C.UTF-8", "LANG=C.UTF-8", "HOME=/home/ray"], "Hostname": "", "Image": "", "Labels": {"org.opencontainers.image.ref.name": "ubuntu", "org.opencontainers.image.version": "20.04"}, "OnBuild": null, "OpenStdin": false, "Shell": ["/bin/bash", "-c"], "StdinOnce": false, "Tty": false, "User": "1000", "Volumes": null, "WorkingDir": "/home/ray"}, "Created": "2023-12-18T23:18:11.720022433Z", "DockerVersion": "", "GraphDriver": {"Data": {"LowerDir": "/var/snap/docker/common/var-lib-docker/overlay2/83ab8d49727201f55d28753df43040f5b5f80afbd50448e0f730a7cc9e6cef6b/diff:/var/snap/docker/common/var-lib-docker/overlay2/86a1ad8b41ca5c50cff5227ed2b93f763e32259dacc7c8c43abd9e8733955fa9/diff:/var/snap/docker/common/var-lib-docker/overlay2/18794a87c087bde4f81f0be07fae9a3e2bc1112dbc45fd5c59966a31ad6da92a/diff:/var/snap/docker/common/var-lib-docker/overlay2/179f19b0d34b4d66c6f06ee3fbd18cd5414c8422c55b0d21d858fd51bca0c69f/diff:/var/snap/docker/common/var-lib-docker/overlay2/95bb925deb61bfee37b6af42db4274c24f0ba7eed1221c9e448e04a657d1f8bd/diff:/var/snap/docker/common/var-lib-docker/overlay2/8123ea3e8528655b66c9223be3bb829d6e6628e06bfa7a89e1857af270a9cf91/diff:/var/snap/docker/common/var-lib-docker/overlay2/f04a8078e2b2fb1c54ae6dc183b41a650c38e9aa8a7b3dbb4b7938d4f887d9a6/diff:/var/snap/docker/common/var-lib-docker/overlay2/137872994a15cf7c56954519ad0c88a5e80bf50a84d08860ac49c96d08c6c246/diff", "MergedDir": "/var/snap/docker/common/var-lib-docker/overlay2/b7900a8360f36ee96300c7bcac26f9c13169a6dc4c3da86026bc4c6d9495037f/merged", "UpperDir": "/var/snap/docker/common/var-lib-docker/overlay2/b7900a8360f36ee96300c7bcac26f9c13169a6dc4c3da86026bc4c6d9495037f/diff", "WorkDir": "/var/snap/docker/common/var-lib-docker/overlay2/b7900a8360f36ee96300c7bcac26f9c13169a6dc4c3da86026bc4c6d9495037f/work"}, "Name": "overlay2"}, "Id": "sha256:ee1c865e02572031f5c68be4184f21d929d5cd233289c83b2a10b9ce455eb75f", "Metadata": {"LastTagTime": "0001-01-01T00:00:00Z"}, "Os": "linux", "Parent": "", "RepoDigests": ["rayproject/ray@sha256:e64546fb5c3233bb0f33608e186e285c52cdd7440cae1af18f7fcde1c04e49f2"], "RepoTags": ["rayproject/ray:2.9.0"], "RootFS": {"Layers": ["sha256:3a03f09d212915b240e9d216069aba5652ed4765c7e4b098c65e71860d47b8e1", "sha256:db6680cb129d39fad6405b4fc83b612de4a228c95b437893af46d8b781ddd0f6", "sha256:81cb020e881ca3c7a87233ac403feea44f8964fbc18340a87a7f559591d90e94", "sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef", "sha256:180b33c2b848ae705b9f079c279f3b62207be4e6501dc80b2be118c7e140589c", "sha256:9e011b19e2eba55cf9b938fe9206e8c5af911843bc393ebf86f279514657df0e", "sha256:0575f54a9aec6019d39c0e95722f3a2062f95a28c3b1501918e0053c0696e7e8", "sha256:a7f9e2973cd370320020a1e5642e5665c2dff4da3384c3d0b5357ff48a25f84c", "sha256:4b4d53180b9fe6e2ae8a264f4a35d2b589415cf9cdff556f84e3b313ed4ec543"], "Type": "layers"}, "Size": 2195262629}}
2025-07-07 09:34:57,723 p=87684 u=gpadmin n=ansible | ok: [G-244] => {"actions": ["Pulled image rayproject/ray:2.9.0"], "attempts": 1, "changed": false, "image": {"Architecture": "amd64", "Author": "", "Comment": "buildkit.dockerfile.v0", "Config": {"AttachStderr": false, "AttachStdin": false, "AttachStdout": false, "Cmd": ["/bin/bash"], "Domainname": "", "Entrypoint": null, "Env": ["PATH=/home/ray/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin", "TZ=America/Los_Angeles", "LC_ALL=C.UTF-8", "LANG=C.UTF-8", "HOME=/home/ray"], "Hostname": "", "Image": "", "Labels": {"org.opencontainers.image.ref.name": "ubuntu", "org.opencontainers.image.version": "20.04"}, "OnBuild": null, "OpenStdin": false, "Shell": ["/bin/bash", "-c"], "StdinOnce": false, "Tty": false, "User": "1000", "Volumes": null, "WorkingDir": "/home/ray"}, "Created": "2023-12-18T23:18:11.720022433Z", "DockerVersion": "", "GraphDriver": {"Data": {"LowerDir": "/var/snap/docker/common/var-lib-docker/overlay2/51c6945fa243fe4afb8781ed4cf8a974ab741edff2463266ccde49d493ac59b2/diff:/var/snap/docker/common/var-lib-docker/overlay2/220ce155cfbbd587ddbbef284424840ad51450d90e447d267e0650cd1ebc3efd/diff:/var/snap/docker/common/var-lib-docker/overlay2/a23d5b0cc83f963124cccafedaf8ad7db7aab7f691000e575a82f5a4d97d3f95/diff:/var/snap/docker/common/var-lib-docker/overlay2/5e7b90cbce2921bb714b7e8c93d30100a7e48ba706ab2d9184e18bc5b66ed545/diff:/var/snap/docker/common/var-lib-docker/overlay2/0857cd0675c79aaabea4fea2de757915ab17fd3739aa7804b799cf009fa73e36/diff:/var/snap/docker/common/var-lib-docker/overlay2/0880726b515e9ca1dc75c25f956416ff11b885b5cbde9c93e30497f660d522f1/diff:/var/snap/docker/common/var-lib-docker/overlay2/70718f46cfee6a7dd2eb66971e726bdcc450be29f9c28cbe87c8c06da656087f/diff:/var/snap/docker/common/var-lib-docker/overlay2/f835350146ddd5cb9fbcd21726d0a862f10d1bc87ec9e9b21812e96b9e98bbf4/diff", "MergedDir": "/var/snap/docker/common/var-lib-docker/overlay2/ceabb4d43431ed05a1692b89bbe7360636907f35bd95a68d808c180698040d4d/merged", "UpperDir": "/var/snap/docker/common/var-lib-docker/overlay2/ceabb4d43431ed05a1692b89bbe7360636907f35bd95a68d808c180698040d4d/diff", "WorkDir": "/var/snap/docker/common/var-lib-docker/overlay2/ceabb4d43431ed05a1692b89bbe7360636907f35bd95a68d808c180698040d4d/work"}, "Name": "overlay2"}, "Id": "sha256:ee1c865e02572031f5c68be4184f21d929d5cd233289c83b2a10b9ce455eb75f", "Metadata": {"LastTagTime": "0001-01-01T00:00:00Z"}, "Os": "linux", "Parent": "", "RepoDigests": ["rayproject/ray@sha256:e64546fb5c3233bb0f33608e186e285c52cdd7440cae1af18f7fcde1c04e49f2"], "RepoTags": ["rayproject/ray:2.9.0"], "RootFS": {"Layers": ["sha256:3a03f09d212915b240e9d216069aba5652ed4765c7e4b098c65e71860d47b8e1", "sha256:db6680cb129d39fad6405b4fc83b612de4a228c95b437893af46d8b781ddd0f6", "sha256:81cb020e881ca3c7a87233ac403feea44f8964fbc18340a87a7f559591d90e94", "sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef", "sha256:180b33c2b848ae705b9f079c279f3b62207be4e6501dc80b2be118c7e140589c", "sha256:9e011b19e2eba55cf9b938fe9206e8c5af911843bc393ebf86f279514657df0e", "sha256:0575f54a9aec6019d39c0e95722f3a2062f95a28c3b1501918e0053c0696e7e8", "sha256:a7f9e2973cd370320020a1e5642e5665c2dff4da3384c3d0b5357ff48a25f84c", "sha256:4b4d53180b9fe6e2ae8a264f4a35d2b589415cf9cdff556f84e3b313ed4ec543"], "Type": "layers"}, "Size": 2195262629}}
2025-07-07 09:34:57,726 p=87684 u=gpadmin n=ansible | ok: [G-243] => {"actions": ["Pulled image rayproject/ray:2.9.0"], "attempts": 1, "changed": false, "image": {"Architecture": "amd64", "Author": "", "Comment": "buildkit.dockerfile.v0", "Config": {"AttachStderr": false, "AttachStdin": false, "AttachStdout": false, "Cmd": ["/bin/bash"], "Domainname": "", "Entrypoint": null, "Env": ["PATH=/home/ray/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin", "TZ=America/Los_Angeles", "LC_ALL=C.UTF-8", "LANG=C.UTF-8", "HOME=/home/ray"], "Hostname": "", "Image": "", "Labels": {"org.opencontainers.image.ref.name": "ubuntu", "org.opencontainers.image.version": "20.04"}, "OnBuild": null, "OpenStdin": false, "Shell": ["/bin/bash", "-c"], "StdinOnce": false, "Tty": false, "User": "1000", "Volumes": null, "WorkingDir": "/home/ray"}, "Created": "2023-12-18T23:18:11.720022433Z", "DockerVersion": "", "GraphDriver": {"Data": {"LowerDir": "/var/snap/docker/common/var-lib-docker/overlay2/699fda5df898f39cfe340f8d3c343ad747654042e3c2c5f4a09379b739d1bbc9/diff:/var/snap/docker/common/var-lib-docker/overlay2/c9be3c70783e31b8c59f7bb2aeae4f16445682c22222015a35cda1dfe927ac55/diff:/var/snap/docker/common/var-lib-docker/overlay2/e417d88b93425d325d88e0dd601dd7aec1f28b1c41516cdbedea17d8a1a0d6f4/diff:/var/snap/docker/common/var-lib-docker/overlay2/ae90c1cfde15a414a7954d1c9af848d114bdf4a13c6b41c1a722dd7a42f111fd/diff:/var/snap/docker/common/var-lib-docker/overlay2/2ea9f814f6d17b6a78ecaf08ea7a835ae577fcf7a3d8b5939164608506c04162/diff:/var/snap/docker/common/var-lib-docker/overlay2/7d732d9d8fa84509cc83b26a4d58af7f69116250f77165dabd2ae448b0c8f389/diff:/var/snap/docker/common/var-lib-docker/overlay2/d3e6e93c55f1da69c78d66e8ad77c435231f9a135b05898f49a8df7ef41745ef/diff:/var/snap/docker/common/var-lib-docker/overlay2/468e81afa16128b04dcbd884af0a97776e3bf443f0371abc4ac46883dbf6e582/diff", "MergedDir": "/var/snap/docker/common/var-lib-docker/overlay2/c75df0f8f9d4156800e9120320a1dffb62f702dfcbca236400792a85903922d1/merged", "UpperDir": "/var/snap/docker/common/var-lib-docker/overlay2/c75df0f8f9d4156800e9120320a1dffb62f702dfcbca236400792a85903922d1/diff", "WorkDir": "/var/snap/docker/common/var-lib-docker/overlay2/c75df0f8f9d4156800e9120320a1dffb62f702dfcbca236400792a85903922d1/work"}, "Name": "overlay2"}, "Id": "sha256:ee1c865e02572031f5c68be4184f21d929d5cd233289c83b2a10b9ce455eb75f", "Metadata": {"LastTagTime": "0001-01-01T00:00:00Z"}, "Os": "linux", "Parent": "", "RepoDigests": ["rayproject/ray@sha256:e64546fb5c3233bb0f33608e186e285c52cdd7440cae1af18f7fcde1c04e49f2"], "RepoTags": ["rayproject/ray:2.9.0"], "RootFS": {"Layers": ["sha256:3a03f09d212915b240e9d216069aba5652ed4765c7e4b098c65e71860d47b8e1", "sha256:db6680cb129d39fad6405b4fc83b612de4a228c95b437893af46d8b781ddd0f6", "sha256:81cb020e881ca3c7a87233ac403feea44f8964fbc18340a87a7f559591d90e94", "sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef", "sha256:180b33c2b848ae705b9f079c279f3b62207be4e6501dc80b2be118c7e140589c", "sha256:9e011b19e2eba55cf9b938fe9206e8c5af911843bc393ebf86f279514657df0e", "sha256:0575f54a9aec6019d39c0e95722f3a2062f95a28c3b1501918e0053c0696e7e8", "sha256:a7f9e2973cd370320020a1e5642e5665c2dff4da3384c3d0b5357ff48a25f84c", "sha256:4b4d53180b9fe6e2ae8a264f4a35d2b589415cf9cdff556f84e3b313ed4ec543"], "Type": "layers"}, "Size": 2195262629}}
2025-07-07 09:34:57,887 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {"actions": ["Pulled image rayproject/ray:2.9.0"], "attempts": 1, "changed": false, "image": {"Architecture": "amd64", "Author": "", "Comment": "buildkit.dockerfile.v0", "Config": {"Cmd": ["/bin/bash"], "Entrypoint": null, "Env": ["PATH=/home/ray/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin", "TZ=America/Los_Angeles", "LC_ALL=C.UTF-8", "LANG=C.UTF-8", "HOME=/home/ray"], "Labels": {"org.opencontainers.image.ref.name": "ubuntu", "org.opencontainers.image.version": "20.04"}, "OnBuild": null, "Shell": ["/bin/bash", "-c"], "User": "1000", "Volumes": null, "WorkingDir": "/home/ray"}, "Created": "2023-12-18T23:18:11.720022433Z", "DockerVersion": "", "GraphDriver": {"Data": {"LowerDir": "/var/lib/docker/overlay2/a7becb9ad73d236f3565797490417714dc391e6a543d1aeb6b4ca590de5aef83/diff:/var/lib/docker/overlay2/d6a40d2a2c0db8d0c92d82b8fda785bc5aed81d5dc73b3f2d7f3523d75f739b0/diff:/var/lib/docker/overlay2/3a90ebe7fb3b368e5be5144f83c481b230cdc4d5800793b334c1c1ecfc573638/diff:/var/lib/docker/overlay2/db17ccc5d8837ae58713b0bd273ab553abb70282291546b3bb5be42407dba64d/diff:/var/lib/docker/overlay2/917181e62e89e11f30e3acf992ab3c198b3656d2c9611f3437cc06ca35465e6b/diff:/var/lib/docker/overlay2/29016ef182caedb3568d015438765d46019a237c9f9c0c2e6c5500269d21709b/diff:/var/lib/docker/overlay2/6857c9a1337136341dfcd03fcc575d961b3616f689a9d455e3e4c8bd9b16ce80/diff:/var/lib/docker/overlay2/e3f017fd1cc3ec9e215f54f3ee32a27733eaee1bf47db1976a38cdf028ce7b80/diff", "MergedDir": "/var/lib/docker/overlay2/607d795adcbbebc6c403d28809e091210e7df04a3116d08e08ffd4311eb32026/merged", "UpperDir": "/var/lib/docker/overlay2/607d795adcbbebc6c403d28809e091210e7df04a3116d08e08ffd4311eb32026/diff", "WorkDir": "/var/lib/docker/overlay2/607d795adcbbebc6c403d28809e091210e7df04a3116d08e08ffd4311eb32026/work"}, "Name": "overlay2"}, "Id": "sha256:ee1c865e02572031f5c68be4184f21d929d5cd233289c83b2a10b9ce455eb75f", "Metadata": {"LastTagTime": "0001-01-01T00:00:00Z"}, "Os": "linux", "Parent": "", "RepoDigests": ["rayproject/ray@sha256:e64546fb5c3233bb0f33608e186e285c52cdd7440cae1af18f7fcde1c04e49f2"], "RepoTags": ["rayproject/ray:2.9.0"], "RootFS": {"Layers": ["sha256:3a03f09d212915b240e9d216069aba5652ed4765c7e4b098c65e71860d47b8e1", "sha256:db6680cb129d39fad6405b4fc83b612de4a228c95b437893af46d8b781ddd0f6", "sha256:81cb020e881ca3c7a87233ac403feea44f8964fbc18340a87a7f559591d90e94", "sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef", "sha256:180b33c2b848ae705b9f079c279f3b62207be4e6501dc80b2be118c7e140589c", "sha256:9e011b19e2eba55cf9b938fe9206e8c5af911843bc393ebf86f279514657df0e", "sha256:0575f54a9aec6019d39c0e95722f3a2062f95a28c3b1501918e0053c0696e7e8", "sha256:a7f9e2973cd370320020a1e5642e5665c2dff4da3384c3d0b5357ff48a25f84c", "sha256:4b4d53180b9fe6e2ae8a264f4a35d2b589415cf9cdff556f84e3b313ed4ec543"], "Type": "layers"}, "Size": 2195262629}}
2025-07-07 09:34:58,035 p=87684 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] **************************************************************
2025-07-07 09:34:58,045 p=87684 u=gpadmin n=ansible | TASK [Gathering Facts] **********************************************************************
2025-07-07 09:34:59,239 p=87684 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:34:59,274 p=87684 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] ************************
2025-07-07 09:34:59,574 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:34:59,588 p=87684 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] *****************
2025-07-07 09:35:00,357 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true}
2025-07-07 09:35:00,371 p=87684 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] ****************************
2025-07-07 09:35:00,666 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true, "path": "/usr/local/bin/start_ray_head.sh", "state": "absent"}
2025-07-07 09:35:00,680 p=87684 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] ***********************************
2025-07-07 09:35:01,328 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true, "checksum": "1b4c6c0e6819c87d86b4155afaa9bf1da31b80a3", "dest": "/usr/local/bin/start_ray_head.sh", "gid": 1000, "group": "gpadmin", "md5sum": "7e29ed567f4095bd1b9c4f0419e3a117", "mode": "0755", "owner": "gpadmin", "size": 1235, "src": "/home/gpadmin/.ansible/tmp/ansible-tmp-1751895300.714191-89025-10361752789290/source", "state": "file", "uid": 1000}
2025-07-07 09:35:01,343 p=87684 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] **************************************************
2025-07-07 09:35:01,884 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true, "cmd": ["/usr/local/bin/start_ray_head.sh"], "delta": "0:00:00.253993", "end": "2025-07-07 09:35:01.839324", "msg": "", "rc": 0, "start": "2025-07-07 09:35:01.585331", "stderr": "", "stderr_lines": [], "stdout": "a30361101b647a4844df2d1a43a87a38488c27b91102898bd4ed0d3a3a4ce25c\nRay head node started on G-K3S-Master with container name ray_head", "stdout_lines": ["a30361101b647a4844df2d1a43a87a38488c27b91102898bd4ed0d3a3a4ce25c", "Ray head node started on G-K3S-Master with container name ray_head"]}
2025-07-07 09:35:01,899 p=87684 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] *****************************************
2025-07-07 09:35:02,233 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "cmd": ["docker", "ps", "-f", "name=ray_head"], "delta": "0:00:00.030974", "end": "2025-07-07 09:35:02.186672", "msg": "", "rc": 0, "start": "2025-07-07 09:35:02.155698", "stderr": "", "stderr_lines": [], "stdout": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\na30361101b64   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_head", "stdout_lines": ["CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES", "a30361101b64   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_head"]}
2025-07-07 09:35:02,248 p=87684 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] *******************************************
2025-07-07 09:35:02,273 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\na30361101b64   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_head"
}
2025-07-07 09:35:02,291 p=87684 u=gpadmin n=ansible | TASK [ray_head : Wait for a moment to let Ray head node initialize] *************************
2025-07-07 09:35:02,311 p=87684 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 09:35:02,311 p=87684 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 09:35:12,316 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "delta": 10, "echo": true, "rc": 0, "start": "2025-07-07 09:35:02.310383", "stderr": "", "stdout": "Paused for 10.0 seconds", "stop": "2025-07-07 09:35:12.313752", "user_input": ""}
2025-07-07 09:35:12,332 p=87684 u=gpadmin n=ansible | TASK [ray_head : Check Ray head node status] ************************************************
2025-07-07 09:35:14,426 p=87684 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_head", "ray", "status"], "delta": "0:00:01.811723", "end": "2025-07-07 09:35:14.388145", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 09:35:12.576422", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:35:14,427 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:14,442 p=87684 u=gpadmin n=ansible | TASK [ray_head : Print Ray head node status] ************************************************
2025-07-07 09:35:14,486 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": []
}
2025-07-07 09:35:14,545 p=87684 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] ***********************************************************
2025-07-07 09:35:14,554 p=87684 u=gpadmin n=ansible | TASK [Gathering Facts] **********************************************************************
2025-07-07 09:35:15,324 p=87684 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:35:15,399 p=87684 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:35:15,426 p=87684 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:35:15,445 p=87684 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:35:15,511 p=87684 u=gpadmin n=ansible | TASK [ray_worker : Ensure Ray temporary directory exists on worker node] ********************
2025-07-07 09:35:15,680 p=87684 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:35:15,693 p=87684 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:35:15,709 p=87684 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:35:15,735 p=87684 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "gid": 1000, "group": "gpadmin", "mode": "0777", "owner": "gpadmin", "path": "/home/gpadmin/ray_temp", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:35:15,748 p=87684 u=gpadmin n=ansible | TASK [ray_worker : Stop and remove existing Ray worker container (idempotency)] *************
2025-07-07 09:35:16,134 p=87684 u=gpadmin n=ansible | changed: [G-241] => {"changed": true}
2025-07-07 09:35:16,151 p=87684 u=gpadmin n=ansible | changed: [G-243] => {"changed": true}
2025-07-07 09:35:16,170 p=87684 u=gpadmin n=ansible | changed: [G-242] => {"changed": true}
2025-07-07 09:35:16,206 p=87684 u=gpadmin n=ansible | changed: [G-244] => {"changed": true}
2025-07-07 09:35:16,220 p=87684 u=gpadmin n=ansible | TASK [ray_worker : Remove old Ray worker start script (idempotency)] ************************
2025-07-07 09:35:16,378 p=87684 u=gpadmin n=ansible | changed: [G-241] => {"changed": true, "path": "/usr/local/bin/start_ray_worker.sh", "state": "absent"}
2025-07-07 09:35:16,404 p=87684 u=gpadmin n=ansible | changed: [G-242] => {"changed": true, "path": "/usr/local/bin/start_ray_worker.sh", "state": "absent"}
2025-07-07 09:35:16,420 p=87684 u=gpadmin n=ansible | changed: [G-243] => {"changed": true, "path": "/usr/local/bin/start_ray_worker.sh", "state": "absent"}
2025-07-07 09:35:16,446 p=87684 u=gpadmin n=ansible | changed: [G-244] => {"changed": true, "path": "/usr/local/bin/start_ray_worker.sh", "state": "absent"}
2025-07-07 09:35:16,459 p=87684 u=gpadmin n=ansible | TASK [ray_worker : Copy Ray worker start script to worker node] *****************************
2025-07-07 09:35:16,924 p=87684 u=gpadmin n=ansible | changed: [G-242] => {"changed": true, "checksum": "92f939e5cfb91a9ce79a140e19f545977825c01d", "dest": "/usr/local/bin/start_ray_worker.sh", "gid": 1000, "group": "gpadmin", "md5sum": "d6ef0746d839e2dd5a82213555cc02b1", "mode": "0755", "owner": "gpadmin", "size": 1142, "src": "/home/gpadmin/.ansible/tmp/ansible-tmp-1751895316.5302498-89611-120317198854815/source", "state": "file", "uid": 1000}
2025-07-07 09:35:16,926 p=87684 u=gpadmin n=ansible | changed: [G-241] => {"changed": true, "checksum": "92f939e5cfb91a9ce79a140e19f545977825c01d", "dest": "/usr/local/bin/start_ray_worker.sh", "gid": 1000, "group": "gpadmin", "md5sum": "d6ef0746d839e2dd5a82213555cc02b1", "mode": "0755", "owner": "gpadmin", "size": 1142, "src": "/home/gpadmin/.ansible/tmp/ansible-tmp-1751895316.5140283-89609-170921904934792/source", "state": "file", "uid": 1000}
2025-07-07 09:35:16,959 p=87684 u=gpadmin n=ansible | changed: [G-243] => {"changed": true, "checksum": "92f939e5cfb91a9ce79a140e19f545977825c01d", "dest": "/usr/local/bin/start_ray_worker.sh", "gid": 1000, "group": "gpadmin", "md5sum": "d6ef0746d839e2dd5a82213555cc02b1", "mode": "0755", "owner": "gpadmin", "size": 1142, "src": "/home/gpadmin/.ansible/tmp/ansible-tmp-1751895316.5445998-89614-18444695550531/source", "state": "file", "uid": 1000}
2025-07-07 09:35:16,996 p=87684 u=gpadmin n=ansible | changed: [G-244] => {"changed": true, "checksum": "92f939e5cfb91a9ce79a140e19f545977825c01d", "dest": "/usr/local/bin/start_ray_worker.sh", "gid": 1000, "group": "gpadmin", "md5sum": "d6ef0746d839e2dd5a82213555cc02b1", "mode": "0755", "owner": "gpadmin", "size": 1142, "src": "/home/gpadmin/.ansible/tmp/ansible-tmp-1751895316.5635855-89620-150838151048334/source", "state": "file", "uid": 1000}
2025-07-07 09:35:17,009 p=87684 u=gpadmin n=ansible | TASK [ray_worker : Start Ray worker container] **********************************************
2025-07-07 09:35:17,536 p=87684 u=gpadmin n=ansible | changed: [G-242] => {"changed": true, "cmd": ["/usr/local/bin/start_ray_worker.sh"], "delta": "0:00:00.336939", "end": "2025-07-07 13:35:17.522249", "msg": "", "rc": 0, "start": "2025-07-07 13:35:17.185310", "stderr": "", "stderr_lines": [], "stdout": "7fbaa9e64aadb24607a4f8224a8bedbfb5aff1e32f6c98b28a91177565464392\nRay worker node started on g-242 with container name ray_worker", "stdout_lines": ["7fbaa9e64aadb24607a4f8224a8bedbfb5aff1e32f6c98b28a91177565464392", "Ray worker node started on g-242 with container name ray_worker"]}
2025-07-07 09:35:17,538 p=87684 u=gpadmin n=ansible | changed: [G-241] => {"changed": true, "cmd": ["/usr/local/bin/start_ray_worker.sh"], "delta": "0:00:00.348471", "end": "2025-07-07 13:35:17.521956", "msg": "", "rc": 0, "start": "2025-07-07 13:35:17.173485", "stderr": "", "stderr_lines": [], "stdout": "ae21478cc53a1a6958f6260a9fe3bc5f0eff99a14e6e15d42201696da3a9e7dc\nRay worker node started on g-241 with container name ray_worker", "stdout_lines": ["ae21478cc53a1a6958f6260a9fe3bc5f0eff99a14e6e15d42201696da3a9e7dc", "Ray worker node started on g-241 with container name ray_worker"]}
2025-07-07 09:35:17,551 p=87684 u=gpadmin n=ansible | changed: [G-243] => {"changed": true, "cmd": ["/usr/local/bin/start_ray_worker.sh"], "delta": "0:00:00.336156", "end": "2025-07-07 13:35:17.541016", "msg": "", "rc": 0, "start": "2025-07-07 13:35:17.204860", "stderr": "", "stderr_lines": [], "stdout": "28d337ef57bdd2fda6c8b2ae80c42bcbae4915333511a2fd3bb336c0c3406bd6\nRay worker node started on g-243 with container name ray_worker", "stdout_lines": ["28d337ef57bdd2fda6c8b2ae80c42bcbae4915333511a2fd3bb336c0c3406bd6", "Ray worker node started on g-243 with container name ray_worker"]}
2025-07-07 09:35:17,601 p=87684 u=gpadmin n=ansible | changed: [G-244] => {"changed": true, "cmd": ["/usr/local/bin/start_ray_worker.sh"], "delta": "0:00:00.364115", "end": "2025-07-07 13:35:17.587382", "msg": "", "rc": 0, "start": "2025-07-07 13:35:17.223267", "stderr": "", "stderr_lines": [], "stdout": "56ce0f34b799c145f6d7cbea6f63e7201fccaf91624b5cff3bf9f368ff52b913\nRay worker node started on g-244 with container name ray_worker", "stdout_lines": ["56ce0f34b799c145f6d7cbea6f63e7201fccaf91624b5cff3bf9f368ff52b913", "Ray worker node started on g-244 with container name ray_worker"]}
2025-07-07 09:35:17,614 p=87684 u=gpadmin n=ansible | TASK [ray_worker : Display Ray worker container status] *************************************
2025-07-07 09:35:17,799 p=87684 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "cmd": ["docker", "ps", "-f", "name=ray_worker"], "delta": "0:00:00.036138", "end": "2025-07-07 13:35:17.782658", "msg": "", "rc": 0, "start": "2025-07-07 13:35:17.746520", "stderr": "", "stderr_lines": [], "stdout": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\nae21478cc53a   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker", "stdout_lines": ["CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES", "ae21478cc53a   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"]}
2025-07-07 09:35:17,823 p=87684 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "cmd": ["docker", "ps", "-f", "name=ray_worker"], "delta": "0:00:00.040622", "end": "2025-07-07 13:35:17.810808", "msg": "", "rc": 0, "start": "2025-07-07 13:35:17.770186", "stderr": "", "stderr_lines": [], "stdout": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n7fbaa9e64aad   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker", "stdout_lines": ["CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES", "7fbaa9e64aad   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"]}
2025-07-07 09:35:17,826 p=87684 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "cmd": ["docker", "ps", "-f", "name=ray_worker"], "delta": "0:00:00.035327", "end": "2025-07-07 13:35:17.815381", "msg": "", "rc": 0, "start": "2025-07-07 13:35:17.780054", "stderr": "", "stderr_lines": [], "stdout": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n28d337ef57bd   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker", "stdout_lines": ["CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES", "28d337ef57bd   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"]}
2025-07-07 09:35:17,854 p=87684 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "cmd": ["docker", "ps", "-f", "name=ray_worker"], "delta": "0:00:00.046146", "end": "2025-07-07 13:35:17.841359", "msg": "", "rc": 0, "start": "2025-07-07 13:35:17.795213", "stderr": "", "stderr_lines": [], "stdout": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n56ce0f34b799   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker", "stdout_lines": ["CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES", "56ce0f34b799   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"]}
2025-07-07 09:35:17,867 p=87684 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker container status] ***************************************
2025-07-07 09:35:17,898 p=87684 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\nae21478cc53a   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:35:17,938 p=87684 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n7fbaa9e64aad   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:35:17,940 p=87684 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n28d337ef57bd   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:35:17,950 p=87684 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n56ce0f34b799   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:35:17,959 p=87684 u=gpadmin n=ansible | TASK [ray_worker : Wait for a moment to let Ray worker node initialize] *********************
2025-07-07 09:35:17,974 p=87684 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 09:35:17,975 p=87684 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 09:35:27,980 p=87684 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "delta": 10, "echo": true, "rc": 0, "start": "2025-07-07 09:35:17.973855", "stderr": "", "stdout": "Paused for 10.0 seconds", "stop": "2025-07-07 09:35:27.977154", "user_input": ""}
2025-07-07 09:35:27,992 p=87684 u=gpadmin n=ansible | TASK [ray_worker : Check Ray worker node status] ********************************************
2025-07-07 09:35:29,039 p=87684 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.881248", "end": "2025-07-07 13:35:29.022815", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:35:28.141567", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:35:29,039 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:29,062 p=87684 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.874895", "end": "2025-07-07 13:35:29.051077", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:35:28.176182", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:35:29,062 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:29,101 p=87684 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.920107", "end": "2025-07-07 13:35:29.088720", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:35:28.168613", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:35:29,101 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:29,124 p=87684 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_worker", "ray", "status"], "delta": "0:00:00.901185", "end": "2025-07-07 13:35:29.110648", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:35:28.209463", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:35:29,124 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:29,137 p=87684 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker node status] ********************************************
2025-07-07 09:35:29,198 p=87684 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": []
}
2025-07-07 09:35:29,201 p=87684 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": []
}
2025-07-07 09:35:29,222 p=87684 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": []
}
2025-07-07 09:35:29,238 p=87684 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": []
}
2025-07-07 09:35:29,397 p=87684 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] ********************************************************
2025-07-07 09:35:29,418 p=87684 u=gpadmin n=ansible | TASK [Check Ray head node status] ***********************************************************
2025-07-07 09:35:31,457 p=87684 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "cmd": ["docker", "exec", "ray_head", "ray", "status"], "delta": "0:00:01.753728", "end": "2025-07-07 09:35:31.408412", "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 09:35:29.654684", "stderr": "Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.", "stderr_lines": ["Traceback (most recent call last):", "  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>", "    sys.exit(main())", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main", "    return cli()", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__", "    return self.main(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main", "    rv = self.invoke(ctx)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke", "    return _process_result(sub_ctx.command.invoke(sub_ctx))", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke", "    return ctx.invoke(self.callback, **ctx.params)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke", "    return __callback(*args, **kwargs)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status", "    address = services.canonicalize_bootstrap_address_or_die(address)", "  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die", "    raise ConnectionError(", "ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."], "stdout": "", "stdout_lines": []}
2025-07-07 09:35:31,458 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:31,471 p=87684 u=gpadmin n=ansible | TASK [Display Ray cluster status] ***********************************************************
2025-07-07 09:35:31,492 p=87684 u=gpadmin n=ansible | skipping: [MASTER] => {"false_condition": "ray_status.rc == 0"}
2025-07-07 09:35:31,506 p=87684 u=gpadmin n=ansible | TASK [Display Ray cluster error] ************************************************************
2025-07-07 09:35:31,528 p=87684 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Ray cluster status check failed: Traceback (most recent call last):\n  File \"/home/ray/anaconda3/bin/ray\", line 8, in <module>\n    sys.exit(main())\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 2498, in main\n    return cli()\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1157, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1078, in main\n    rv = self.invoke(ctx)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1688, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 1434, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py\", line 783, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py\", line 1973, in status\n    address = services.canonicalize_bootstrap_address_or_die(address)\n  File \"/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py\", line 566, in canonicalize_bootstrap_address_or_die\n    raise ConnectionError(\nConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable."
}
2025-07-07 09:35:31,572 p=87684 u=gpadmin n=ansible | PLAY [Deploy Monitoring Stack] **************************************************************
2025-07-07 09:35:31,583 p=87684 u=gpadmin n=ansible | TASK [Gathering Facts] **********************************************************************
2025-07-07 09:35:32,411 p=87684 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:35:32,415 p=87684 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:35:32,417 p=87684 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:35:32,460 p=87684 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:35:32,820 p=87684 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:35:32,896 p=87684 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus data directory] ****************************************
2025-07-07 09:35:33,093 p=87684 u=gpadmin n=ansible | changed: [G-241] => {"changed": true, "gid": 1000, "group": "gpadmin", "mode": "0755", "owner": "gpadmin", "path": "/home/gpadmin/prometheus_data", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:35:33,102 p=87684 u=gpadmin n=ansible | changed: [G-242] => {"changed": true, "gid": 1000, "group": "gpadmin", "mode": "0755", "owner": "gpadmin", "path": "/home/gpadmin/prometheus_data", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:35:33,124 p=87684 u=gpadmin n=ansible | changed: [G-244] => {"changed": true, "gid": 1000, "group": "gpadmin", "mode": "0755", "owner": "gpadmin", "path": "/home/gpadmin/prometheus_data", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:35:33,124 p=87684 u=gpadmin n=ansible | changed: [G-243] => {"changed": true, "gid": 1000, "group": "gpadmin", "mode": "0755", "owner": "gpadmin", "path": "/home/gpadmin/prometheus_data", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:35:33,221 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true, "gid": 1000, "group": "gpadmin", "mode": "0755", "owner": "gpadmin", "path": "/home/gpadmin/prometheus_data", "size": 4096, "state": "directory", "uid": 1000}
2025-07-07 09:35:33,236 p=87684 u=gpadmin n=ansible | TASK [monitoring : Create Grafana data directory] *******************************************
2025-07-07 09:35:33,422 p=87684 u=gpadmin n=ansible | changed: [G-241] => {"changed": true, "gid": 472, "group": "472", "mode": "0755", "owner": "472", "path": "/home/gpadmin/grafana_data", "size": 4096, "state": "directory", "uid": 472}
2025-07-07 09:35:33,444 p=87684 u=gpadmin n=ansible | changed: [G-242] => {"changed": true, "gid": 472, "group": "472", "mode": "0755", "owner": "472", "path": "/home/gpadmin/grafana_data", "size": 4096, "state": "directory", "uid": 472}
2025-07-07 09:35:33,464 p=87684 u=gpadmin n=ansible | changed: [G-243] => {"changed": true, "gid": 472, "group": "472", "mode": "0755", "owner": "472", "path": "/home/gpadmin/grafana_data", "size": 4096, "state": "directory", "uid": 472}
2025-07-07 09:35:33,483 p=87684 u=gpadmin n=ansible | changed: [G-244] => {"changed": true, "gid": 472, "group": "472", "mode": "0755", "owner": "472", "path": "/home/gpadmin/grafana_data", "size": 4096, "state": "directory", "uid": 472}
2025-07-07 09:35:33,552 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true, "gid": 472, "group": "472", "mode": "0755", "owner": "472", "path": "/home/gpadmin/grafana_data", "size": 4096, "state": "directory", "uid": 472}
2025-07-07 09:35:33,566 p=87684 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Node Exporter container (idempotency)] **********
2025-07-07 09:35:33,856 p=87684 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:33,857 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:33,859 p=87684 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:33,859 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:33,868 p=87684 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:33,868 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:33,893 p=87684 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:33,893 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:34,070 p=87684 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:34,070 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:34,083 p=87684 u=gpadmin n=ansible | TASK [monitoring : Start Node Exporter container] *******************************************
2025-07-07 09:35:36,766 p=87684 u=gpadmin n=ansible | changed: [G-243] => {"changed": true, "container": {"AppArmorProfile": "docker-default", "Args": ["--path.procfs=/host/proc", "--path.sysfs=/host/sys", "--path.rootfs=/rootfs", "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)", "--web.listen-address=0.0.0.0:9100"], "Config": {"AttachStderr": true, "AttachStdin": false, "AttachStdout": true, "Cmd": ["--path.procfs=/host/proc", "--path.sysfs=/host/sys", "--path.rootfs=/rootfs", "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)", "--web.listen-address=0.0.0.0:9100"], "Domainname": "", "Entrypoint": ["/bin/node_exporter"], "Env": ["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"], "ExposedPorts": {"9100/tcp": {}}, "Hostname": "g-243", "Image": "prom/node-exporter:v1.6.1", "Labels": {"maintainer": "The Prometheus Authors <prometheus-developers@googlegroups.com>"}, "OnBuild": null, "OpenStdin": false, "StdinOnce": false, "Tty": false, "User": "nobody", "Volumes": null, "WorkingDir": ""}, "Created": "2025-07-07T13:35:36.509308639Z", "Driver": "overlay2", "ExecIDs": null, "GraphDriver": {"Data": {"ID": "35590ea5ea6807263ad85053af5a180851692fdcec724b6a0a5cd131ead4976c", "LowerDir": "/var/snap/docker/common/var-lib-docker/overlay2/f9199a3874982070c75cae3fd5eb3f41d79efc8502b6c0fdef1d3a4fbc5f2d59-init/diff:/var/snap/docker/common/var-lib-docker/overlay2/dc66be33e737d27c3da253d552258f0c3a773c6905a61a7636d3736ba819f63b/diff:/var/snap/docker/common/var-lib-docker/overlay2/01c024c4f0e2c9d9172440726741617ee148bd0991459b285320d4c785cb9da2/diff:/var/snap/docker/common/var-lib-docker/overlay2/537976c2b43430401afd5671dba15a3e9140831bb1f56d8bab6d211303a1f6f2/diff", "MergedDir": "/var/snap/docker/common/var-lib-docker/overlay2/f9199a3874982070c75cae3fd5eb3f41d79efc8502b6c0fdef1d3a4fbc5f2d59/merged", "UpperDir": "/var/snap/docker/common/var-lib-docker/overlay2/f9199a3874982070c75cae3fd5eb3f41d79efc8502b6c0fdef1d3a4fbc5f2d59/diff", "WorkDir": "/var/snap/docker/common/var-lib-docker/overlay2/f9199a3874982070c75cae3fd5eb3f41d79efc8502b6c0fdef1d3a4fbc5f2d59/work"}, "Name": "overlay2"}, "HostConfig": {"AutoRemove": false, "Binds": ["/proc:/host/proc:ro", "/sys:/host/sys:ro", "/:/rootfs:ro"], "BlkioDeviceReadBps": null, "BlkioDeviceReadIOps": null, "BlkioDeviceWriteBps": null, "BlkioDeviceWriteIOps": null, "BlkioWeight": 0, "BlkioWeightDevice": null, "CapAdd": null, "CapDrop": null, "Cgroup": "", "CgroupParent": "", "CgroupnsMode": "private", "ConsoleSize": [0, 0], "ContainerIDFile": "", "CpuCount": 0, "CpuPercent": 0, "CpuPeriod": 0, "CpuQuota": 0, "CpuRealtimePeriod": 0, "CpuRealtimeRuntime": 0, "CpuShares": 0, "CpusetCpus": "", "CpusetMems": "", "DeviceCgroupRules": null, "DeviceRequests": null, "Devices": null, "Dns": null, "DnsOptions": null, "DnsSearch": null, "ExtraHosts": null, "GroupAdd": null, "IOMaximumBandwidth": 0, "IOMaximumIOps": 0, "IpcMode": "private", "Isolation": "", "Links": null, "LogConfig": {"Config": {}, "Type": "json-file"}, "MaskedPaths": ["/proc/asound", "/proc/acpi", "/proc/interrupts", "/proc/kcore", "/proc/keys", "/proc/latency_stats", "/proc/timer_list", "/proc/timer_stats", "/proc/sched_debug", "/proc/scsi", "/sys/firmware", "/sys/devices/virtual/powercap", "/sys/devices/system/cpu/cpu0/thermal_throttle", "/sys/devices/system/cpu/cpu1/thermal_throttle", "/sys/devices/system/cpu/cpu2/thermal_throttle", "/sys/devices/system/cpu/cpu3/thermal_throttle", "/sys/devices/system/cpu/cpu4/thermal_throttle", "/sys/devices/system/cpu/cpu5/thermal_throttle", "/sys/devices/system/cpu/cpu6/thermal_throttle", "/sys/devices/system/cpu/cpu7/thermal_throttle", "/sys/devices/system/cpu/cpu8/thermal_throttle", "/sys/devices/system/cpu/cpu9/thermal_throttle", "/sys/devices/system/cpu/cpu10/thermal_throttle", "/sys/devices/system/cpu/cpu11/thermal_throttle", "/sys/devices/system/cpu/cpu12/thermal_throttle", "/sys/devices/system/cpu/cpu13/thermal_throttle", "/sys/devices/system/cpu/cpu14/thermal_throttle", "/sys/devices/system/cpu/cpu15/thermal_throttle"], "Memory": 0, "MemoryReservation": 0, "MemorySwap": 0, "MemorySwappiness": null, "NanoCpus": 0, "NetworkMode": "host", "OomKillDisable": null, "OomScoreAdj": 0, "PidMode": "", "PidsLimit": null, "PortBindings": null, "Privileged": false, "PublishAllPorts": false, "ReadonlyPaths": ["/proc/bus", "/proc/fs", "/proc/irq", "/proc/sys", "/proc/sysrq-trigger"], "ReadonlyRootfs": false, "RestartPolicy": {"MaximumRetryCount": 0, "Name": "unless-stopped"}, "Runtime": "runc", "SecurityOpt": null, "ShmSize": 67108864, "UTSMode": "", "Ulimits": null, "UsernsMode": "", "VolumeDriver": "", "VolumesFrom": null}, "HostnamePath": "/var/snap/docker/common/var-lib-docker/containers/35590ea5ea6807263ad85053af5a180851692fdcec724b6a0a5cd131ead4976c/hostname", "HostsPath": "/var/snap/docker/common/var-lib-docker/containers/35590ea5ea6807263ad85053af5a180851692fdcec724b6a0a5cd131ead4976c/hosts", "Id": "35590ea5ea6807263ad85053af5a180851692fdcec724b6a0a5cd131ead4976c", "Image": "sha256:458e026e6aa62a8da4522cb09766da69d7365ebeb456d5a43a214fc6bd232a3c", "LogPath": "/var/snap/docker/common/var-lib-docker/containers/35590ea5ea6807263ad85053af5a180851692fdcec724b6a0a5cd131ead4976c/35590ea5ea6807263ad85053af5a180851692fdcec724b6a0a5cd131ead4976c-json.log", "MountLabel": "", "Mounts": [{"Destination": "/host/proc", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/proc", "Type": "bind"}, {"Destination": "/host/sys", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/sys", "Type": "bind"}, {"Destination": "/rootfs", "Mode": "ro", "Propagation": "rslave", "RW": false, "Source": "/", "Type": "bind"}], "Name": "/node-exporter", "NetworkSettings": {"Bridge": "", "EndpointID": "", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "HairpinMode": false, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "LinkLocalIPv6Address": "", "LinkLocalIPv6PrefixLen": 0, "MacAddress": "", "Networks": {"host": {"Aliases": null, "DNSNames": null, "DriverOpts": null, "EndpointID": "b6eb6afe4f65950d35e566fa84213210288ae8cda9d8d9d8b1b776aa11281d14", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "GwPriority": 0, "IPAMConfig": null, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "Links": null, "MacAddress": "", "NetworkID": "08942515bd33d690388bcee97cc5b6a4ea027bcdf50469e8d8b1c7df2afd7835"}}, "Ports": {}, "SandboxID": "8d74bd24aa60d22e9b58148ce449b78107b63c3e5174e4437b6ea409ebaa7b64", "SandboxKey": "/run/snap.docker/netns/default", "SecondaryIPAddresses": null, "SecondaryIPv6Addresses": null}, "Path": "/bin/node_exporter", "Platform": "linux", "ProcessLabel": "", "ResolvConfPath": "/var/snap/docker/common/var-lib-docker/containers/35590ea5ea6807263ad85053af5a180851692fdcec724b6a0a5cd131ead4976c/resolv.conf", "RestartCount": 0, "State": {"Dead": false, "Error": "", "ExitCode": 0, "FinishedAt": "0001-01-01T00:00:00Z", "OOMKilled": false, "Paused": false, "Pid": 26543, "Restarting": false, "Running": true, "StartedAt": "2025-07-07T13:35:36.654760129Z", "Status": "running"}}}
2025-07-07 09:35:36,843 p=87684 u=gpadmin n=ansible | changed: [G-244] => {"changed": true, "container": {"AppArmorProfile": "docker-default", "Args": ["--path.procfs=/host/proc", "--path.sysfs=/host/sys", "--path.rootfs=/rootfs", "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)", "--web.listen-address=0.0.0.0:9100"], "Config": {"AttachStderr": true, "AttachStdin": false, "AttachStdout": true, "Cmd": ["--path.procfs=/host/proc", "--path.sysfs=/host/sys", "--path.rootfs=/rootfs", "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)", "--web.listen-address=0.0.0.0:9100"], "Domainname": "", "Entrypoint": ["/bin/node_exporter"], "Env": ["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"], "ExposedPorts": {"9100/tcp": {}}, "Hostname": "g-244", "Image": "prom/node-exporter:v1.6.1", "Labels": {"maintainer": "The Prometheus Authors <prometheus-developers@googlegroups.com>"}, "OnBuild": null, "OpenStdin": false, "StdinOnce": false, "Tty": false, "User": "nobody", "Volumes": null, "WorkingDir": ""}, "Created": "2025-07-07T13:35:36.586801554Z", "Driver": "overlay2", "ExecIDs": null, "GraphDriver": {"Data": {"ID": "54fb99bb27ad20cd8a229195efa6580b906d207f218a765de9f17958f4b76cd1", "LowerDir": "/var/snap/docker/common/var-lib-docker/overlay2/f3d9e5384ee2ec3b39a9e2b3c04612d2af3a0e30b12a6ccd596c974839267c96-init/diff:/var/snap/docker/common/var-lib-docker/overlay2/d37a8afa5afa9bde0eb2a216bb0b72db026c6a0709ce633f680f51239b829ed1/diff:/var/snap/docker/common/var-lib-docker/overlay2/2eb5d23819025c70f4cd67d6788cf988dbb2c0dbec7fff8b6a0226b3b1af7f82/diff:/var/snap/docker/common/var-lib-docker/overlay2/4d4b1ade00b6031249d67a896b117fb4f7484bf8190621211a196f63aed04ea2/diff", "MergedDir": "/var/snap/docker/common/var-lib-docker/overlay2/f3d9e5384ee2ec3b39a9e2b3c04612d2af3a0e30b12a6ccd596c974839267c96/merged", "UpperDir": "/var/snap/docker/common/var-lib-docker/overlay2/f3d9e5384ee2ec3b39a9e2b3c04612d2af3a0e30b12a6ccd596c974839267c96/diff", "WorkDir": "/var/snap/docker/common/var-lib-docker/overlay2/f3d9e5384ee2ec3b39a9e2b3c04612d2af3a0e30b12a6ccd596c974839267c96/work"}, "Name": "overlay2"}, "HostConfig": {"AutoRemove": false, "Binds": ["/proc:/host/proc:ro", "/sys:/host/sys:ro", "/:/rootfs:ro"], "BlkioDeviceReadBps": null, "BlkioDeviceReadIOps": null, "BlkioDeviceWriteBps": null, "BlkioDeviceWriteIOps": null, "BlkioWeight": 0, "BlkioWeightDevice": null, "CapAdd": null, "CapDrop": null, "Cgroup": "", "CgroupParent": "", "CgroupnsMode": "private", "ConsoleSize": [0, 0], "ContainerIDFile": "", "CpuCount": 0, "CpuPercent": 0, "CpuPeriod": 0, "CpuQuota": 0, "CpuRealtimePeriod": 0, "CpuRealtimeRuntime": 0, "CpuShares": 0, "CpusetCpus": "", "CpusetMems": "", "DeviceCgroupRules": null, "DeviceRequests": null, "Devices": null, "Dns": null, "DnsOptions": null, "DnsSearch": null, "ExtraHosts": null, "GroupAdd": null, "IOMaximumBandwidth": 0, "IOMaximumIOps": 0, "IpcMode": "private", "Isolation": "", "Links": null, "LogConfig": {"Config": {}, "Type": "json-file"}, "MaskedPaths": ["/proc/asound", "/proc/acpi", "/proc/interrupts", "/proc/kcore", "/proc/keys", "/proc/latency_stats", "/proc/timer_list", "/proc/timer_stats", "/proc/sched_debug", "/proc/scsi", "/sys/firmware", "/sys/devices/virtual/powercap", "/sys/devices/system/cpu/cpu0/thermal_throttle", "/sys/devices/system/cpu/cpu1/thermal_throttle", "/sys/devices/system/cpu/cpu2/thermal_throttle", "/sys/devices/system/cpu/cpu3/thermal_throttle", "/sys/devices/system/cpu/cpu4/thermal_throttle", "/sys/devices/system/cpu/cpu5/thermal_throttle", "/sys/devices/system/cpu/cpu6/thermal_throttle", "/sys/devices/system/cpu/cpu7/thermal_throttle", "/sys/devices/system/cpu/cpu8/thermal_throttle", "/sys/devices/system/cpu/cpu9/thermal_throttle", "/sys/devices/system/cpu/cpu10/thermal_throttle", "/sys/devices/system/cpu/cpu11/thermal_throttle", "/sys/devices/system/cpu/cpu12/thermal_throttle", "/sys/devices/system/cpu/cpu13/thermal_throttle", "/sys/devices/system/cpu/cpu14/thermal_throttle", "/sys/devices/system/cpu/cpu15/thermal_throttle"], "Memory": 0, "MemoryReservation": 0, "MemorySwap": 0, "MemorySwappiness": null, "NanoCpus": 0, "NetworkMode": "host", "OomKillDisable": null, "OomScoreAdj": 0, "PidMode": "", "PidsLimit": null, "PortBindings": null, "Privileged": false, "PublishAllPorts": false, "ReadonlyPaths": ["/proc/bus", "/proc/fs", "/proc/irq", "/proc/sys", "/proc/sysrq-trigger"], "ReadonlyRootfs": false, "RestartPolicy": {"MaximumRetryCount": 0, "Name": "unless-stopped"}, "Runtime": "runc", "SecurityOpt": null, "ShmSize": 67108864, "UTSMode": "", "Ulimits": null, "UsernsMode": "", "VolumeDriver": "", "VolumesFrom": null}, "HostnamePath": "/var/snap/docker/common/var-lib-docker/containers/54fb99bb27ad20cd8a229195efa6580b906d207f218a765de9f17958f4b76cd1/hostname", "HostsPath": "/var/snap/docker/common/var-lib-docker/containers/54fb99bb27ad20cd8a229195efa6580b906d207f218a765de9f17958f4b76cd1/hosts", "Id": "54fb99bb27ad20cd8a229195efa6580b906d207f218a765de9f17958f4b76cd1", "Image": "sha256:458e026e6aa62a8da4522cb09766da69d7365ebeb456d5a43a214fc6bd232a3c", "LogPath": "/var/snap/docker/common/var-lib-docker/containers/54fb99bb27ad20cd8a229195efa6580b906d207f218a765de9f17958f4b76cd1/54fb99bb27ad20cd8a229195efa6580b906d207f218a765de9f17958f4b76cd1-json.log", "MountLabel": "", "Mounts": [{"Destination": "/host/proc", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/proc", "Type": "bind"}, {"Destination": "/host/sys", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/sys", "Type": "bind"}, {"Destination": "/rootfs", "Mode": "ro", "Propagation": "rslave", "RW": false, "Source": "/", "Type": "bind"}], "Name": "/node-exporter", "NetworkSettings": {"Bridge": "", "EndpointID": "", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "HairpinMode": false, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "LinkLocalIPv6Address": "", "LinkLocalIPv6PrefixLen": 0, "MacAddress": "", "Networks": {"host": {"Aliases": null, "DNSNames": null, "DriverOpts": null, "EndpointID": "885e179a26cb11781e90c0cc2460321d2ac055c6fc58c93f7412c757b78d4bf9", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "GwPriority": 0, "IPAMConfig": null, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "Links": null, "MacAddress": "", "NetworkID": "5d96122e262cf1ec764846a437b033348d3f336a9e4556edca067907ca50b2a2"}}, "Ports": {}, "SandboxID": "0bfd61c7dcb4960a67a438a471d628e0b3da589be0fd0821dc10c17aa5873796", "SandboxKey": "/run/snap.docker/netns/default", "SecondaryIPAddresses": null, "SecondaryIPv6Addresses": null}, "Path": "/bin/node_exporter", "Platform": "linux", "ProcessLabel": "", "ResolvConfPath": "/var/snap/docker/common/var-lib-docker/containers/54fb99bb27ad20cd8a229195efa6580b906d207f218a765de9f17958f4b76cd1/resolv.conf", "RestartCount": 0, "State": {"Dead": false, "Error": "", "ExitCode": 0, "FinishedAt": "0001-01-01T00:00:00Z", "OOMKilled": false, "Paused": false, "Pid": 26424, "Restarting": false, "Running": true, "StartedAt": "2025-07-07T13:35:36.740449231Z", "Status": "running"}}}
2025-07-07 09:35:36,861 p=87684 u=gpadmin n=ansible | changed: [G-241] => {"changed": true, "container": {"AppArmorProfile": "docker-default", "Args": ["--path.procfs=/host/proc", "--path.sysfs=/host/sys", "--path.rootfs=/rootfs", "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)", "--web.listen-address=0.0.0.0:9100"], "Config": {"AttachStderr": true, "AttachStdin": false, "AttachStdout": true, "Cmd": ["--path.procfs=/host/proc", "--path.sysfs=/host/sys", "--path.rootfs=/rootfs", "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)", "--web.listen-address=0.0.0.0:9100"], "Domainname": "", "Entrypoint": ["/bin/node_exporter"], "Env": ["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"], "ExposedPorts": {"9100/tcp": {}}, "Hostname": "g-241", "Image": "prom/node-exporter:v1.6.1", "Labels": {"maintainer": "The Prometheus Authors <prometheus-developers@googlegroups.com>"}, "OnBuild": null, "OpenStdin": false, "StdinOnce": false, "Tty": false, "User": "nobody", "Volumes": null, "WorkingDir": ""}, "Created": "2025-07-07T13:35:36.615425486Z", "Driver": "overlay2", "ExecIDs": null, "GraphDriver": {"Data": {"ID": "9b79793063ead7116357e2a6b987ddef302c4067c5df91863532078534abefb4", "LowerDir": "/var/snap/docker/common/var-lib-docker/overlay2/ff26b8e309bd4997a490495147565ed787d90ff4f035ce4e18f131b290441141-init/diff:/var/snap/docker/common/var-lib-docker/overlay2/fe33a44ba16a9145affe7672e726dee5aacb652c3d8fd5f05bca2b6f0dc2967b/diff:/var/snap/docker/common/var-lib-docker/overlay2/87a9e01e421e108201467fa1f79ae55bd1cacb52b2cb85fb54519b86485d0a07/diff:/var/snap/docker/common/var-lib-docker/overlay2/f0a0713b364f30f941d8e182c915e726d41c7217bf27efb96fe4e72d572ba758/diff", "MergedDir": "/var/snap/docker/common/var-lib-docker/overlay2/ff26b8e309bd4997a490495147565ed787d90ff4f035ce4e18f131b290441141/merged", "UpperDir": "/var/snap/docker/common/var-lib-docker/overlay2/ff26b8e309bd4997a490495147565ed787d90ff4f035ce4e18f131b290441141/diff", "WorkDir": "/var/snap/docker/common/var-lib-docker/overlay2/ff26b8e309bd4997a490495147565ed787d90ff4f035ce4e18f131b290441141/work"}, "Name": "overlay2"}, "HostConfig": {"AutoRemove": false, "Binds": ["/proc:/host/proc:ro", "/sys:/host/sys:ro", "/:/rootfs:ro"], "BlkioDeviceReadBps": null, "BlkioDeviceReadIOps": null, "BlkioDeviceWriteBps": null, "BlkioDeviceWriteIOps": null, "BlkioWeight": 0, "BlkioWeightDevice": null, "CapAdd": null, "CapDrop": null, "Cgroup": "", "CgroupParent": "", "CgroupnsMode": "private", "ConsoleSize": [0, 0], "ContainerIDFile": "", "CpuCount": 0, "CpuPercent": 0, "CpuPeriod": 0, "CpuQuota": 0, "CpuRealtimePeriod": 0, "CpuRealtimeRuntime": 0, "CpuShares": 0, "CpusetCpus": "", "CpusetMems": "", "DeviceCgroupRules": null, "DeviceRequests": null, "Devices": null, "Dns": null, "DnsOptions": null, "DnsSearch": null, "ExtraHosts": null, "GroupAdd": null, "IOMaximumBandwidth": 0, "IOMaximumIOps": 0, "IpcMode": "private", "Isolation": "", "Links": null, "LogConfig": {"Config": {}, "Type": "json-file"}, "MaskedPaths": ["/proc/asound", "/proc/acpi", "/proc/interrupts", "/proc/kcore", "/proc/keys", "/proc/latency_stats", "/proc/timer_list", "/proc/timer_stats", "/proc/sched_debug", "/proc/scsi", "/sys/firmware", "/sys/devices/virtual/powercap", "/sys/devices/system/cpu/cpu0/thermal_throttle", "/sys/devices/system/cpu/cpu1/thermal_throttle", "/sys/devices/system/cpu/cpu2/thermal_throttle", "/sys/devices/system/cpu/cpu3/thermal_throttle", "/sys/devices/system/cpu/cpu4/thermal_throttle", "/sys/devices/system/cpu/cpu5/thermal_throttle", "/sys/devices/system/cpu/cpu6/thermal_throttle", "/sys/devices/system/cpu/cpu7/thermal_throttle", "/sys/devices/system/cpu/cpu8/thermal_throttle", "/sys/devices/system/cpu/cpu9/thermal_throttle", "/sys/devices/system/cpu/cpu10/thermal_throttle", "/sys/devices/system/cpu/cpu11/thermal_throttle", "/sys/devices/system/cpu/cpu12/thermal_throttle", "/sys/devices/system/cpu/cpu13/thermal_throttle", "/sys/devices/system/cpu/cpu14/thermal_throttle", "/sys/devices/system/cpu/cpu15/thermal_throttle"], "Memory": 0, "MemoryReservation": 0, "MemorySwap": 0, "MemorySwappiness": null, "NanoCpus": 0, "NetworkMode": "host", "OomKillDisable": null, "OomScoreAdj": 0, "PidMode": "", "PidsLimit": null, "PortBindings": null, "Privileged": false, "PublishAllPorts": false, "ReadonlyPaths": ["/proc/bus", "/proc/fs", "/proc/irq", "/proc/sys", "/proc/sysrq-trigger"], "ReadonlyRootfs": false, "RestartPolicy": {"MaximumRetryCount": 0, "Name": "unless-stopped"}, "Runtime": "runc", "SecurityOpt": null, "ShmSize": 67108864, "UTSMode": "", "Ulimits": null, "UsernsMode": "", "VolumeDriver": "", "VolumesFrom": null}, "HostnamePath": "/var/snap/docker/common/var-lib-docker/containers/9b79793063ead7116357e2a6b987ddef302c4067c5df91863532078534abefb4/hostname", "HostsPath": "/var/snap/docker/common/var-lib-docker/containers/9b79793063ead7116357e2a6b987ddef302c4067c5df91863532078534abefb4/hosts", "Id": "9b79793063ead7116357e2a6b987ddef302c4067c5df91863532078534abefb4", "Image": "sha256:458e026e6aa62a8da4522cb09766da69d7365ebeb456d5a43a214fc6bd232a3c", "LogPath": "/var/snap/docker/common/var-lib-docker/containers/9b79793063ead7116357e2a6b987ddef302c4067c5df91863532078534abefb4/9b79793063ead7116357e2a6b987ddef302c4067c5df91863532078534abefb4-json.log", "MountLabel": "", "Mounts": [{"Destination": "/host/proc", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/proc", "Type": "bind"}, {"Destination": "/host/sys", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/sys", "Type": "bind"}, {"Destination": "/rootfs", "Mode": "ro", "Propagation": "rslave", "RW": false, "Source": "/", "Type": "bind"}], "Name": "/node-exporter", "NetworkSettings": {"Bridge": "", "EndpointID": "", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "HairpinMode": false, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "LinkLocalIPv6Address": "", "LinkLocalIPv6PrefixLen": 0, "MacAddress": "", "Networks": {"host": {"Aliases": null, "DNSNames": null, "DriverOpts": null, "EndpointID": "0060b4972b54f1b67783f255155f5adf4decf3bb9aee0aca0d8f4a14645d82ab", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "GwPriority": 0, "IPAMConfig": null, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "Links": null, "MacAddress": "", "NetworkID": "8cecfccbd5549f17327be430b864641aa1ac55d73cad72c3a9aba84c615d7f79"}}, "Ports": {}, "SandboxID": "7aa2fb872b40b3737df73285253f06692ae9151016fc52b89e4b0720aba8b96b", "SandboxKey": "/run/snap.docker/netns/default", "SecondaryIPAddresses": null, "SecondaryIPv6Addresses": null}, "Path": "/bin/node_exporter", "Platform": "linux", "ProcessLabel": "", "ResolvConfPath": "/var/snap/docker/common/var-lib-docker/containers/9b79793063ead7116357e2a6b987ddef302c4067c5df91863532078534abefb4/resolv.conf", "RestartCount": 0, "State": {"Dead": false, "Error": "", "ExitCode": 0, "FinishedAt": "0001-01-01T00:00:00Z", "OOMKilled": false, "Paused": false, "Pid": 15104, "Restarting": false, "Running": true, "StartedAt": "2025-07-07T13:35:36.755180325Z", "Status": "running"}}}
2025-07-07 09:35:36,991 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true, "container": {"AppArmorProfile": "docker-default", "Args": ["--path.procfs=/host/proc", "--path.sysfs=/host/sys", "--path.rootfs=/rootfs", "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)", "--web.listen-address=0.0.0.0:9100"], "Config": {"AttachStderr": true, "AttachStdin": false, "AttachStdout": true, "Cmd": ["--path.procfs=/host/proc", "--path.sysfs=/host/sys", "--path.rootfs=/rootfs", "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)", "--web.listen-address=0.0.0.0:9100"], "Domainname": "", "Entrypoint": ["/bin/node_exporter"], "Env": ["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"], "ExposedPorts": {"9100/tcp": {}}, "Hostname": "G-K3S-Master", "Image": "prom/node-exporter:v1.6.1", "Labels": {"maintainer": "The Prometheus Authors <prometheus-developers@googlegroups.com>"}, "OnBuild": null, "OpenStdin": false, "StdinOnce": false, "Tty": false, "User": "nobody", "Volumes": null, "WorkingDir": ""}, "Created": "2025-07-07T13:35:36.725794975Z", "Driver": "overlay2", "ExecIDs": null, "GraphDriver": {"Data": {"ID": "bd67311143320a4bd5c445f0c18c9839e6da17c9dadbbe526f4390faab8c8f55", "LowerDir": "/var/lib/docker/overlay2/2198fcf483ab5d90d14060ea806ae29bfb77630187940544be6065c1a09227ed-init/diff:/var/lib/docker/overlay2/8af842d98d0a2cb3e2e2798e113754883ed0a054eb815c1b619dc9b76166758d/diff:/var/lib/docker/overlay2/e8909c2c1a6f393357e0123864392e8ef4e82bac695968bfb63064bb251897ce/diff:/var/lib/docker/overlay2/bd10e72442062f9751b8ff22cb9c75f181a7e40a8364c1b0f34769dc742aa9f4/diff", "MergedDir": "/var/lib/docker/overlay2/2198fcf483ab5d90d14060ea806ae29bfb77630187940544be6065c1a09227ed/merged", "UpperDir": "/var/lib/docker/overlay2/2198fcf483ab5d90d14060ea806ae29bfb77630187940544be6065c1a09227ed/diff", "WorkDir": "/var/lib/docker/overlay2/2198fcf483ab5d90d14060ea806ae29bfb77630187940544be6065c1a09227ed/work"}, "Name": "overlay2"}, "HostConfig": {"AutoRemove": false, "Binds": ["/proc:/host/proc:ro", "/sys:/host/sys:ro", "/:/rootfs:ro"], "BlkioDeviceReadBps": null, "BlkioDeviceReadIOps": null, "BlkioDeviceWriteBps": null, "BlkioDeviceWriteIOps": null, "BlkioWeight": 0, "BlkioWeightDevice": null, "CapAdd": null, "CapDrop": null, "Cgroup": "", "CgroupParent": "", "CgroupnsMode": "private", "ConsoleSize": [0, 0], "ContainerIDFile": "", "CpuCount": 0, "CpuPercent": 0, "CpuPeriod": 0, "CpuQuota": 0, "CpuRealtimePeriod": 0, "CpuRealtimeRuntime": 0, "CpuShares": 0, "CpusetCpus": "", "CpusetMems": "", "DeviceCgroupRules": null, "DeviceRequests": null, "Devices": null, "Dns": null, "DnsOptions": null, "DnsSearch": null, "ExtraHosts": null, "GroupAdd": null, "IOMaximumBandwidth": 0, "IOMaximumIOps": 0, "IpcMode": "private", "Isolation": "", "Links": null, "LogConfig": {"Config": {}, "Type": "json-file"}, "MaskedPaths": ["/proc/asound", "/proc/acpi", "/proc/interrupts", "/proc/kcore", "/proc/keys", "/proc/latency_stats", "/proc/timer_list", "/proc/timer_stats", "/proc/sched_debug", "/proc/scsi", "/sys/firmware", "/sys/devices/virtual/powercap"], "Memory": 0, "MemoryReservation": 0, "MemorySwap": 0, "MemorySwappiness": null, "NanoCpus": 0, "NetworkMode": "host", "OomKillDisable": null, "OomScoreAdj": 0, "PidMode": "", "PidsLimit": null, "PortBindings": null, "Privileged": false, "PublishAllPorts": false, "ReadonlyPaths": ["/proc/bus", "/proc/fs", "/proc/irq", "/proc/sys", "/proc/sysrq-trigger"], "ReadonlyRootfs": false, "RestartPolicy": {"MaximumRetryCount": 0, "Name": "unless-stopped"}, "Runtime": "runc", "SecurityOpt": null, "ShmSize": 67108864, "UTSMode": "", "Ulimits": null, "UsernsMode": "", "VolumeDriver": "", "VolumesFrom": null}, "HostnamePath": "/var/lib/docker/containers/bd67311143320a4bd5c445f0c18c9839e6da17c9dadbbe526f4390faab8c8f55/hostname", "HostsPath": "/var/lib/docker/containers/bd67311143320a4bd5c445f0c18c9839e6da17c9dadbbe526f4390faab8c8f55/hosts", "Id": "bd67311143320a4bd5c445f0c18c9839e6da17c9dadbbe526f4390faab8c8f55", "Image": "sha256:458e026e6aa62a8da4522cb09766da69d7365ebeb456d5a43a214fc6bd232a3c", "LogPath": "/var/lib/docker/containers/bd67311143320a4bd5c445f0c18c9839e6da17c9dadbbe526f4390faab8c8f55/bd67311143320a4bd5c445f0c18c9839e6da17c9dadbbe526f4390faab8c8f55-json.log", "MountLabel": "", "Mounts": [{"Destination": "/host/proc", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/proc", "Type": "bind"}, {"Destination": "/host/sys", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/sys", "Type": "bind"}, {"Destination": "/rootfs", "Mode": "ro", "Propagation": "rslave", "RW": false, "Source": "/", "Type": "bind"}], "Name": "/node-exporter", "NetworkSettings": {"Bridge": "", "EndpointID": "", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "HairpinMode": false, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "LinkLocalIPv6Address": "", "LinkLocalIPv6PrefixLen": 0, "MacAddress": "", "Networks": {"host": {"Aliases": null, "DNSNames": null, "DriverOpts": null, "EndpointID": "7b3556d02fa751a3cb356299fb3e741e99184a5b2a448b274cdbbe7be5f98a6a", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "GwPriority": 0, "IPAMConfig": null, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "Links": null, "MacAddress": "", "NetworkID": "268de73ef511b2a43769cd935946989dc6b40e46e052a834cd7d3cb969014641"}}, "Ports": {}, "SandboxID": "bdb0742a43eddf3878e2765acf6f8d2f3e06244d28b3edd66ae7cdaaacb384fd", "SandboxKey": "/var/run/docker/netns/default", "SecondaryIPAddresses": null, "SecondaryIPv6Addresses": null}, "Path": "/bin/node_exporter", "Platform": "linux", "ProcessLabel": "", "ResolvConfPath": "/var/lib/docker/containers/bd67311143320a4bd5c445f0c18c9839e6da17c9dadbbe526f4390faab8c8f55/resolv.conf", "RestartCount": 0, "State": {"Dead": false, "Error": "", "ExitCode": 0, "FinishedAt": "0001-01-01T00:00:00Z", "OOMKilled": false, "Paused": false, "Pid": 90527, "Restarting": false, "Running": true, "StartedAt": "2025-07-07T13:35:36.803312619Z", "Status": "running"}}}
2025-07-07 09:35:37,740 p=87684 u=gpadmin n=ansible | changed: [G-242] => {"changed": true, "container": {"AppArmorProfile": "docker-default", "Args": ["--path.procfs=/host/proc", "--path.sysfs=/host/sys", "--path.rootfs=/rootfs", "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)", "--web.listen-address=0.0.0.0:9100"], "Config": {"AttachStderr": true, "AttachStdin": false, "AttachStdout": true, "Cmd": ["--path.procfs=/host/proc", "--path.sysfs=/host/sys", "--path.rootfs=/rootfs", "--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)", "--web.listen-address=0.0.0.0:9100"], "Domainname": "", "Entrypoint": ["/bin/node_exporter"], "Env": ["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"], "ExposedPorts": {"9100/tcp": {}}, "Hostname": "g-242", "Image": "prom/node-exporter:v1.6.1", "Labels": {"maintainer": "The Prometheus Authors <prometheus-developers@googlegroups.com>"}, "OnBuild": null, "OpenStdin": false, "StdinOnce": false, "Tty": false, "User": "nobody", "Volumes": null, "WorkingDir": ""}, "Created": "2025-07-07T13:35:37.489841808Z", "Driver": "overlay2", "ExecIDs": null, "GraphDriver": {"Data": {"ID": "c04598c908e57bfff47b793c13e9d467030d5145d52c8f1878668ae6e444ca4b", "LowerDir": "/var/snap/docker/common/var-lib-docker/overlay2/db601580b9bb19f33f6d48bb0d63e5907df17bcce80863a66efbe630194ef400-init/diff:/var/snap/docker/common/var-lib-docker/overlay2/f4d1504100ba1d73f39ffb996b3062039f317c1a08ab6460b8a9cb5d83888108/diff:/var/snap/docker/common/var-lib-docker/overlay2/f1c62109c72e0cd873b94fc7e718c8e9c7b5e465c761e9ec9a7acc881aa239b4/diff:/var/snap/docker/common/var-lib-docker/overlay2/5d7a1f73deb71855a191659f4ce5fdf17c1bda9f22085737718b956785cf42c8/diff", "MergedDir": "/var/snap/docker/common/var-lib-docker/overlay2/db601580b9bb19f33f6d48bb0d63e5907df17bcce80863a66efbe630194ef400/merged", "UpperDir": "/var/snap/docker/common/var-lib-docker/overlay2/db601580b9bb19f33f6d48bb0d63e5907df17bcce80863a66efbe630194ef400/diff", "WorkDir": "/var/snap/docker/common/var-lib-docker/overlay2/db601580b9bb19f33f6d48bb0d63e5907df17bcce80863a66efbe630194ef400/work"}, "Name": "overlay2"}, "HostConfig": {"AutoRemove": false, "Binds": ["/proc:/host/proc:ro", "/sys:/host/sys:ro", "/:/rootfs:ro"], "BlkioDeviceReadBps": null, "BlkioDeviceReadIOps": null, "BlkioDeviceWriteBps": null, "BlkioDeviceWriteIOps": null, "BlkioWeight": 0, "BlkioWeightDevice": null, "CapAdd": null, "CapDrop": null, "Cgroup": "", "CgroupParent": "", "CgroupnsMode": "private", "ConsoleSize": [0, 0], "ContainerIDFile": "", "CpuCount": 0, "CpuPercent": 0, "CpuPeriod": 0, "CpuQuota": 0, "CpuRealtimePeriod": 0, "CpuRealtimeRuntime": 0, "CpuShares": 0, "CpusetCpus": "", "CpusetMems": "", "DeviceCgroupRules": null, "DeviceRequests": null, "Devices": null, "Dns": null, "DnsOptions": null, "DnsSearch": null, "ExtraHosts": null, "GroupAdd": null, "IOMaximumBandwidth": 0, "IOMaximumIOps": 0, "IpcMode": "private", "Isolation": "", "Links": null, "LogConfig": {"Config": {}, "Type": "json-file"}, "MaskedPaths": ["/proc/asound", "/proc/acpi", "/proc/interrupts", "/proc/kcore", "/proc/keys", "/proc/latency_stats", "/proc/timer_list", "/proc/timer_stats", "/proc/sched_debug", "/proc/scsi", "/sys/firmware", "/sys/devices/virtual/powercap", "/sys/devices/system/cpu/cpu0/thermal_throttle", "/sys/devices/system/cpu/cpu1/thermal_throttle", "/sys/devices/system/cpu/cpu2/thermal_throttle", "/sys/devices/system/cpu/cpu3/thermal_throttle", "/sys/devices/system/cpu/cpu4/thermal_throttle", "/sys/devices/system/cpu/cpu5/thermal_throttle", "/sys/devices/system/cpu/cpu6/thermal_throttle", "/sys/devices/system/cpu/cpu7/thermal_throttle", "/sys/devices/system/cpu/cpu8/thermal_throttle", "/sys/devices/system/cpu/cpu9/thermal_throttle", "/sys/devices/system/cpu/cpu10/thermal_throttle", "/sys/devices/system/cpu/cpu11/thermal_throttle", "/sys/devices/system/cpu/cpu12/thermal_throttle", "/sys/devices/system/cpu/cpu13/thermal_throttle", "/sys/devices/system/cpu/cpu14/thermal_throttle", "/sys/devices/system/cpu/cpu15/thermal_throttle"], "Memory": 0, "MemoryReservation": 0, "MemorySwap": 0, "MemorySwappiness": null, "NanoCpus": 0, "NetworkMode": "host", "OomKillDisable": null, "OomScoreAdj": 0, "PidMode": "", "PidsLimit": null, "PortBindings": null, "Privileged": false, "PublishAllPorts": false, "ReadonlyPaths": ["/proc/bus", "/proc/fs", "/proc/irq", "/proc/sys", "/proc/sysrq-trigger"], "ReadonlyRootfs": false, "RestartPolicy": {"MaximumRetryCount": 0, "Name": "unless-stopped"}, "Runtime": "runc", "SecurityOpt": null, "ShmSize": 67108864, "UTSMode": "", "Ulimits": null, "UsernsMode": "", "VolumeDriver": "", "VolumesFrom": null}, "HostnamePath": "/var/snap/docker/common/var-lib-docker/containers/c04598c908e57bfff47b793c13e9d467030d5145d52c8f1878668ae6e444ca4b/hostname", "HostsPath": "/var/snap/docker/common/var-lib-docker/containers/c04598c908e57bfff47b793c13e9d467030d5145d52c8f1878668ae6e444ca4b/hosts", "Id": "c04598c908e57bfff47b793c13e9d467030d5145d52c8f1878668ae6e444ca4b", "Image": "sha256:458e026e6aa62a8da4522cb09766da69d7365ebeb456d5a43a214fc6bd232a3c", "LogPath": "/var/snap/docker/common/var-lib-docker/containers/c04598c908e57bfff47b793c13e9d467030d5145d52c8f1878668ae6e444ca4b/c04598c908e57bfff47b793c13e9d467030d5145d52c8f1878668ae6e444ca4b-json.log", "MountLabel": "", "Mounts": [{"Destination": "/host/proc", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/proc", "Type": "bind"}, {"Destination": "/host/sys", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/sys", "Type": "bind"}, {"Destination": "/rootfs", "Mode": "ro", "Propagation": "rslave", "RW": false, "Source": "/", "Type": "bind"}], "Name": "/node-exporter", "NetworkSettings": {"Bridge": "", "EndpointID": "", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "HairpinMode": false, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "LinkLocalIPv6Address": "", "LinkLocalIPv6PrefixLen": 0, "MacAddress": "", "Networks": {"host": {"Aliases": null, "DNSNames": null, "DriverOpts": null, "EndpointID": "6432bb9e93609d1cbbcd824561771add2155bb52cab2b6a1831c0b2b7f46e437", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "GwPriority": 0, "IPAMConfig": null, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "Links": null, "MacAddress": "", "NetworkID": "4344c50ff342c397162b0b5e0b1b235b2c99773c584620f9c3741f098fec875a"}}, "Ports": {}, "SandboxID": "618cf53480a87ec6140c55d0d31e52657e84e45b840e100e45b5d945a37cb6d2", "SandboxKey": "/run/snap.docker/netns/default", "SecondaryIPAddresses": null, "SecondaryIPv6Addresses": null}, "Path": "/bin/node_exporter", "Platform": "linux", "ProcessLabel": "", "ResolvConfPath": "/var/snap/docker/common/var-lib-docker/containers/c04598c908e57bfff47b793c13e9d467030d5145d52c8f1878668ae6e444ca4b/resolv.conf", "RestartCount": 0, "State": {"Dead": false, "Error": "", "ExitCode": 0, "FinishedAt": "0001-01-01T00:00:00Z", "OOMKilled": false, "Paused": false, "Pid": 24346, "Restarting": false, "Running": true, "StartedAt": "2025-07-07T13:35:37.633071158Z", "Status": "running"}}}
2025-07-07 09:35:37,754 p=87684 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing cAdvisor container (idempotency)] ***************
2025-07-07 09:35:38,032 p=87684 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:38,032 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:38,061 p=87684 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:38,061 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:38,068 p=87684 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:38,068 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:38,087 p=87684 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:38,088 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:38,254 p=87684 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:38,255 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:38,268 p=87684 u=gpadmin n=ansible | TASK [monitoring : Start cAdvisor container] ************************************************
2025-07-07 09:35:42,536 p=87684 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Error starting container 8514c0b79f1cc76121424008f90713f53d9ad925337a9cfd85bff986549c6c3d: 500 Server Error for http+docker://localhost/v1.49/containers/8514c0b79f1cc76121424008f90713f53d9ad925337a9cfd85bff986549c6c3d/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 09:35:42,642 p=87684 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Error starting container 578af5eb61cbefd4de0e6da9e537f8499e05797d26b5120e762d6e605a336e26: 500 Server Error for http+docker://localhost/v1.49/containers/578af5eb61cbefd4de0e6da9e537f8499e05797d26b5120e762d6e605a336e26/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 09:35:42,894 p=87684 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Error starting container a65e974292adf5978a7a6ae102ec60009435bc3ddad3fe52e44f323b802e7bbb: 500 Server Error for http+docker://localhost/v1.49/containers/a65e974292adf5978a7a6ae102ec60009435bc3ddad3fe52e44f323b802e7bbb/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 09:35:43,324 p=87684 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Error starting container 2e50b1058a6328d87c4ea1811d4acc36d38270b4b93492695c3246e3686f2181: 500 Server Error for http+docker://localhost/v1.49/containers/2e50b1058a6328d87c4ea1811d4acc36d38270b4b93492695c3246e3686f2181/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 09:35:43,550 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true, "container": {"AppArmorProfile": "unconfined", "Args": ["-logtostderr", "--port=8081", "--housekeeping_interval=10s", "--docker_only=true"], "Config": {"AttachStderr": true, "AttachStdin": false, "AttachStdout": true, "Cmd": ["--port=8081", "--housekeeping_interval=10s", "--docker_only=true"], "Domainname": "", "Entrypoint": ["/usr/bin/cadvisor", "-logtostderr"], "Env": ["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin", "CADVISOR_HEALTHCHECK_URL=http://localhost:8080/healthz"], "ExposedPorts": {"8080/tcp": {}}, "Healthcheck": {"Interval": 30000000000, "Test": ["CMD-SHELL", "wget --quiet --tries=1 --spider $CADVISOR_HEALTHCHECK_URL || exit 1"], "Timeout": 3000000000}, "Hostname": "G-K3S-Master", "Image": "gcr.io/cadvisor/cadvisor:v0.47.2", "Labels": {}, "OnBuild": null, "OpenStdin": false, "StdinOnce": false, "Tty": false, "User": "", "Volumes": null, "WorkingDir": ""}, "Created": "2025-07-07T13:35:43.255878415Z", "Driver": "overlay2", "ExecIDs": null, "GraphDriver": {"Data": {"ID": "2d12943bbe8b7f23a1855bbf212d55eb160a7c9c08f95b3589cbb7b4f0ee3b4e", "LowerDir": "/var/lib/docker/overlay2/169f8ba631b1382527a7a19c02a89879dad33c651384a4921dec893d36a55472-init/diff:/var/lib/docker/overlay2/ac4792ea7ad941e2b71ba5e92efc7e8466206e4aaeb339017905526845030872/diff:/var/lib/docker/overlay2/97aa822f886798d46089ad932d9487775d6c23d7b91b208e0bd9b9639e73d3dc/diff:/var/lib/docker/overlay2/dadfda306de3ca54dcc99e2cb040c0f8aece56ec59492821e565c5830ac1d653/diff:/var/lib/docker/overlay2/7fe4d9f6687db9dcffe7d065ba709508582f9a34ceed1c8ec55285eb6b6ffb20/diff:/var/lib/docker/overlay2/b512d3d732e49136a0b760084416de6bded7b3848fdbaafdb2d4dd7f007ebc77/diff", "MergedDir": "/var/lib/docker/overlay2/169f8ba631b1382527a7a19c02a89879dad33c651384a4921dec893d36a55472/merged", "UpperDir": "/var/lib/docker/overlay2/169f8ba631b1382527a7a19c02a89879dad33c651384a4921dec893d36a55472/diff", "WorkDir": "/var/lib/docker/overlay2/169f8ba631b1382527a7a19c02a89879dad33c651384a4921dec893d36a55472/work"}, "Name": "overlay2"}, "HostConfig": {"AutoRemove": false, "Binds": ["/:/rootfs:ro", "/var/run:/var/run:ro", "/sys:/sys:ro", "/var/lib/docker/:/var/lib/docker:ro", "/dev/disk/:/dev/disk:ro"], "BlkioDeviceReadBps": null, "BlkioDeviceReadIOps": null, "BlkioDeviceWriteBps": null, "BlkioDeviceWriteIOps": null, "BlkioWeight": 0, "BlkioWeightDevice": null, "CapAdd": null, "CapDrop": null, "Cgroup": "", "CgroupParent": "", "CgroupnsMode": "private", "ConsoleSize": [0, 0], "ContainerIDFile": "", "CpuCount": 0, "CpuPercent": 0, "CpuPeriod": 0, "CpuQuota": 0, "CpuRealtimePeriod": 0, "CpuRealtimeRuntime": 0, "CpuShares": 0, "CpusetCpus": "", "CpusetMems": "", "DeviceCgroupRules": null, "DeviceRequests": null, "Devices": null, "Dns": null, "DnsOptions": null, "DnsSearch": null, "ExtraHosts": null, "GroupAdd": null, "IOMaximumBandwidth": 0, "IOMaximumIOps": 0, "IpcMode": "private", "Isolation": "", "Links": null, "LogConfig": {"Config": {}, "Type": "json-file"}, "MaskedPaths": null, "Memory": 0, "MemoryReservation": 0, "MemorySwap": 0, "MemorySwappiness": null, "NanoCpus": 0, "NetworkMode": "host", "OomKillDisable": null, "OomScoreAdj": 0, "PidMode": "", "PidsLimit": null, "PortBindings": null, "Privileged": true, "PublishAllPorts": false, "ReadonlyPaths": null, "ReadonlyRootfs": false, "RestartPolicy": {"MaximumRetryCount": 0, "Name": "unless-stopped"}, "Runtime": "runc", "SecurityOpt": ["label=disable"], "ShmSize": 67108864, "UTSMode": "", "Ulimits": null, "UsernsMode": "", "VolumeDriver": "", "VolumesFrom": null}, "HostnamePath": "/var/lib/docker/containers/2d12943bbe8b7f23a1855bbf212d55eb160a7c9c08f95b3589cbb7b4f0ee3b4e/hostname", "HostsPath": "/var/lib/docker/containers/2d12943bbe8b7f23a1855bbf212d55eb160a7c9c08f95b3589cbb7b4f0ee3b4e/hosts", "Id": "2d12943bbe8b7f23a1855bbf212d55eb160a7c9c08f95b3589cbb7b4f0ee3b4e", "Image": "sha256:69c7eeee9744e45394bf0eb5f1c726fd59aee5ec84297029af453fbc6de4fa98", "LogPath": "/var/lib/docker/containers/2d12943bbe8b7f23a1855bbf212d55eb160a7c9c08f95b3589cbb7b4f0ee3b4e/2d12943bbe8b7f23a1855bbf212d55eb160a7c9c08f95b3589cbb7b4f0ee3b4e-json.log", "MountLabel": "", "Mounts": [{"Destination": "/rootfs", "Mode": "ro", "Propagation": "rslave", "RW": false, "Source": "/", "Type": "bind"}, {"Destination": "/var/run", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/var/run", "Type": "bind"}, {"Destination": "/sys", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/sys", "Type": "bind"}, {"Destination": "/var/lib/docker", "Mode": "ro", "Propagation": "rslave", "RW": false, "Source": "/var/lib/docker", "Type": "bind"}, {"Destination": "/dev/disk", "Mode": "ro", "Propagation": "rprivate", "RW": false, "Source": "/dev/disk", "Type": "bind"}], "Name": "/cadvisor", "NetworkSettings": {"Bridge": "", "EndpointID": "", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "HairpinMode": false, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "LinkLocalIPv6Address": "", "LinkLocalIPv6PrefixLen": 0, "MacAddress": "", "Networks": {"host": {"Aliases": null, "DNSNames": null, "DriverOpts": null, "EndpointID": "c501881cd70e45634dce5fe1161f0fa3ee93df67699916c5aee61ae92924747d", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "GwPriority": 0, "IPAMConfig": null, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "Links": null, "MacAddress": "", "NetworkID": "268de73ef511b2a43769cd935946989dc6b40e46e052a834cd7d3cb969014641"}}, "Ports": {}, "SandboxID": "0630201c99967772b617233102dda578393b9b631e435f06e28358ff41d85a44", "SandboxKey": "/var/run/docker/netns/default", "SecondaryIPAddresses": null, "SecondaryIPv6Addresses": null}, "Path": "/usr/bin/cadvisor", "Platform": "linux", "ProcessLabel": "", "ResolvConfPath": "/var/lib/docker/containers/2d12943bbe8b7f23a1855bbf212d55eb160a7c9c08f95b3589cbb7b4f0ee3b4e/resolv.conf", "RestartCount": 0, "State": {"Dead": false, "Error": "", "ExitCode": 0, "FinishedAt": "0001-01-01T00:00:00Z", "Health": {"FailingStreak": 0, "Log": [], "Status": "starting"}, "OOMKilled": false, "Paused": false, "Pid": 90827, "Restarting": false, "Running": true, "StartedAt": "2025-07-07T13:35:43.357683271Z", "Status": "running"}}}
2025-07-07 09:35:43,563 p=87684 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Prometheus container (idempotency)] *************
2025-07-07 09:35:44,040 p=87684 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:44,041 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:44,053 p=87684 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus configuration] *****************************************
2025-07-07 09:35:44,664 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true, "checksum": "de8739e5810d4faa8dc979cf39733369c004be59", "dest": "/home/gpadmin/prometheus_data/prometheus.yml", "gid": 1000, "group": "gpadmin", "md5sum": "5a66b6b8be7624374b718d41caf0e5d4", "mode": "0644", "owner": "gpadmin", "size": 1174, "src": "/home/gpadmin/.ansible/tmp/ansible-tmp-1751895344.086496-90961-162674673733447/source", "state": "file", "uid": 1000}
2025-07-07 09:35:44,679 p=87684 u=gpadmin n=ansible | TASK [monitoring : Start Prometheus container] **********************************************
2025-07-07 09:35:50,320 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true, "container": {"AppArmorProfile": "docker-default", "Args": ["--config.file=/etc/prometheus/prometheus.yml", "--storage.tsdb.path=/prometheus", "--web.console.libraries=/usr/share/prometheus/console_libraries", "--web.console.templates=/usr/share/prometheus/consoles", "--web.listen-address=0.0.0.0:9090"], "Config": {"AttachStderr": true, "AttachStdin": false, "AttachStdout": true, "Cmd": ["--config.file=/etc/prometheus/prometheus.yml", "--storage.tsdb.path=/prometheus", "--web.console.libraries=/usr/share/prometheus/console_libraries", "--web.console.templates=/usr/share/prometheus/consoles", "--web.listen-address=0.0.0.0:9090"], "Domainname": "", "Entrypoint": ["/bin/prometheus"], "Env": ["PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"], "ExposedPorts": {"9090/tcp": {}}, "Hostname": "G-K3S-Master", "Image": "prom/prometheus:v2.45.0", "Labels": {"maintainer": "The Prometheus Authors <prometheus-developers@googlegroups.com>"}, "OnBuild": null, "OpenStdin": false, "StdinOnce": false, "Tty": false, "User": "nobody", "Volumes": {"/prometheus": {}}, "WorkingDir": "/prometheus"}, "Created": "2025-07-07T13:35:49.906082576Z", "Driver": "overlay2", "ExecIDs": null, "GraphDriver": {"Data": {"ID": "335acfbb607384d0e37ad40e811f606747f9f8749e727307829f1164697fb634", "LowerDir": "/var/lib/docker/overlay2/53c02454e8badef7a3dc30547a96c5074b56c9867ab1cec5dce8169707bcdb71-init/diff:/var/lib/docker/overlay2/8451a45171c644f12c7a1ac18496ae0a09f2d629adf38bb7b2cd8b0068661265/diff:/var/lib/docker/overlay2/2182c3c93d556e074f76fa5856e3cba991c25e41b9d405376503523399e1ccef/diff:/var/lib/docker/overlay2/bbe6a7185993d26e669de13aff7e7216dd0a40e6fdb2c438114b86a7f55602b0/diff:/var/lib/docker/overlay2/08b725874d0af6b34c1181e953db6c6e862da9d6e8015218101cb0e02bc859b1/diff:/var/lib/docker/overlay2/ddf44b888773a931069d6346b6651db4f5a6ecf441fd57295fe48c989201c951/diff:/var/lib/docker/overlay2/4d48cfc24b97211279956d8c5a464d3bed5299df2dc42c7e93a0ab4ec4950796/diff:/var/lib/docker/overlay2/ebe22bfe95e736075a35bb6c1cadd27bbb292415d585e8c3cc1764a44c016d5c/diff:/var/lib/docker/overlay2/aa1da3a8a575a79681262993df1474b4440f6e32fa53994af7512785f77ab3aa/diff:/var/lib/docker/overlay2/258dde30e12d2ffffd8d1b4f588b76850af3d996609fd2ff3c77e9a41f8b8ca4/diff:/var/lib/docker/overlay2/7dca87ba3eaf0bf0445e53bb81d9ffaa7b4f41fb4f1b379cbfed2df152958006/diff:/var/lib/docker/overlay2/ce48a2c9342792fe3b218db0817b645989e46c2f4bd873febd9bf6cc884a693e/diff:/var/lib/docker/overlay2/e46680c5a6c73440ae7c042a9a00d16450b50148d51c4bd490b91757a2697b8a/diff", "MergedDir": "/var/lib/docker/overlay2/53c02454e8badef7a3dc30547a96c5074b56c9867ab1cec5dce8169707bcdb71/merged", "UpperDir": "/var/lib/docker/overlay2/53c02454e8badef7a3dc30547a96c5074b56c9867ab1cec5dce8169707bcdb71/diff", "WorkDir": "/var/lib/docker/overlay2/53c02454e8badef7a3dc30547a96c5074b56c9867ab1cec5dce8169707bcdb71/work"}, "Name": "overlay2"}, "HostConfig": {"AutoRemove": false, "Binds": ["/home/gpadmin/prometheus_data/prometheus.yml:/etc/prometheus/prometheus.yml:rw", "/home/gpadmin/prometheus_data:/prometheus:rw"], "BlkioDeviceReadBps": null, "BlkioDeviceReadIOps": null, "BlkioDeviceWriteBps": null, "BlkioDeviceWriteIOps": null, "BlkioWeight": 0, "BlkioWeightDevice": null, "CapAdd": null, "CapDrop": null, "Cgroup": "", "CgroupParent": "", "CgroupnsMode": "private", "ConsoleSize": [0, 0], "ContainerIDFile": "", "CpuCount": 0, "CpuPercent": 0, "CpuPeriod": 0, "CpuQuota": 0, "CpuRealtimePeriod": 0, "CpuRealtimeRuntime": 0, "CpuShares": 0, "CpusetCpus": "", "CpusetMems": "", "DeviceCgroupRules": null, "DeviceRequests": null, "Devices": null, "Dns": null, "DnsOptions": null, "DnsSearch": null, "ExtraHosts": null, "GroupAdd": null, "IOMaximumBandwidth": 0, "IOMaximumIOps": 0, "IpcMode": "private", "Isolation": "", "Links": null, "LogConfig": {"Config": {}, "Type": "json-file"}, "MaskedPaths": ["/proc/asound", "/proc/acpi", "/proc/interrupts", "/proc/kcore", "/proc/keys", "/proc/latency_stats", "/proc/timer_list", "/proc/timer_stats", "/proc/sched_debug", "/proc/scsi", "/sys/firmware", "/sys/devices/virtual/powercap"], "Memory": 0, "MemoryReservation": 0, "MemorySwap": 0, "MemorySwappiness": null, "NanoCpus": 0, "NetworkMode": "host", "OomKillDisable": null, "OomScoreAdj": 0, "PidMode": "", "PidsLimit": null, "PortBindings": null, "Privileged": false, "PublishAllPorts": false, "ReadonlyPaths": ["/proc/bus", "/proc/fs", "/proc/irq", "/proc/sys", "/proc/sysrq-trigger"], "ReadonlyRootfs": false, "RestartPolicy": {"MaximumRetryCount": 0, "Name": "unless-stopped"}, "Runtime": "runc", "SecurityOpt": null, "ShmSize": 67108864, "UTSMode": "", "Ulimits": null, "UsernsMode": "", "VolumeDriver": "", "VolumesFrom": null}, "HostnamePath": "/var/lib/docker/containers/335acfbb607384d0e37ad40e811f606747f9f8749e727307829f1164697fb634/hostname", "HostsPath": "/var/lib/docker/containers/335acfbb607384d0e37ad40e811f606747f9f8749e727307829f1164697fb634/hosts", "Id": "335acfbb607384d0e37ad40e811f606747f9f8749e727307829f1164697fb634", "Image": "sha256:e1fbd49323c628ccc8c11aea113eddc294c4bd1887dc627d175331c325df10fd", "LogPath": "/var/lib/docker/containers/335acfbb607384d0e37ad40e811f606747f9f8749e727307829f1164697fb634/335acfbb607384d0e37ad40e811f606747f9f8749e727307829f1164697fb634-json.log", "MountLabel": "", "Mounts": [{"Destination": "/etc/prometheus/prometheus.yml", "Mode": "rw", "Propagation": "rprivate", "RW": true, "Source": "/home/gpadmin/prometheus_data/prometheus.yml", "Type": "bind"}, {"Destination": "/prometheus", "Mode": "rw", "Propagation": "rprivate", "RW": true, "Source": "/home/gpadmin/prometheus_data", "Type": "bind"}], "Name": "/prometheus", "NetworkSettings": {"Bridge": "", "EndpointID": "", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "HairpinMode": false, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "LinkLocalIPv6Address": "", "LinkLocalIPv6PrefixLen": 0, "MacAddress": "", "Networks": {"host": {"Aliases": null, "DNSNames": null, "DriverOpts": null, "EndpointID": "afcc208f89c94de52eef0572667e726b6af0b2c010bce17cf5389344d714df6a", "Gateway": "", "GlobalIPv6Address": "", "GlobalIPv6PrefixLen": 0, "GwPriority": 0, "IPAMConfig": null, "IPAddress": "", "IPPrefixLen": 0, "IPv6Gateway": "", "Links": null, "MacAddress": "", "NetworkID": "268de73ef511b2a43769cd935946989dc6b40e46e052a834cd7d3cb969014641"}}, "Ports": {}, "SandboxID": "eebc1eabef9a40ae0033cb4479dffa446dbc6b58299d71376a192d6ac2d20977", "SandboxKey": "/var/run/docker/netns/default", "SecondaryIPAddresses": null, "SecondaryIPv6Addresses": null}, "Path": "/bin/prometheus", "Platform": "linux", "ProcessLabel": "", "ResolvConfPath": "/var/lib/docker/containers/335acfbb607384d0e37ad40e811f606747f9f8749e727307829f1164697fb634/resolv.conf", "RestartCount": 0, "State": {"Dead": false, "Error": "", "ExitCode": 0, "FinishedAt": "0001-01-01T00:00:00Z", "OOMKilled": false, "Paused": false, "Pid": 91228, "Restarting": false, "Running": true, "StartedAt": "2025-07-07T13:35:50.12405555Z", "Status": "running"}}}
2025-07-07 09:35:50,329 p=87684 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Grafana container (idempotency)] ****************
2025-07-07 09:35:50,826 p=87684 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:35:50,826 p=87684 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:35:50,840 p=87684 u=gpadmin n=ansible | TASK [monitoring : Create Grafana provisioning directories] *********************************
2025-07-07 09:35:51,139 p=87684 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/datasources) => {"ansible_loop_var": "item", "changed": true, "gid": 472, "group": "472", "item": "/home/gpadmin/grafana_data/provisioning/datasources", "mode": "0755", "owner": "472", "path": "/home/gpadmin/grafana_data/provisioning/datasources", "size": 4096, "state": "directory", "uid": 472}
2025-07-07 09:35:51,423 p=87684 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/dashboards) => {"ansible_loop_var": "item", "changed": true, "gid": 472, "group": "472", "item": "/home/gpadmin/grafana_data/provisioning/dashboards", "mode": "0755", "owner": "472", "path": "/home/gpadmin/grafana_data/provisioning/dashboards", "size": 4096, "state": "directory", "uid": 472}
2025-07-07 09:35:51,731 p=87684 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/dashboards) => {"ansible_loop_var": "item", "changed": true, "gid": 472, "group": "472", "item": "/home/gpadmin/grafana_data/dashboards", "mode": "0755", "owner": "472", "path": "/home/gpadmin/grafana_data/dashboards", "size": 4096, "state": "directory", "uid": 472}
2025-07-07 09:35:51,746 p=87684 u=gpadmin n=ansible | TASK [monitoring : Create Grafana datasource configuration] *********************************
2025-07-07 09:35:52,266 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true, "checksum": "cbc017b8085b7a05ad5acb8002bcc32b9defd608", "dest": "/home/gpadmin/grafana_data/provisioning/datasources/datasource.yml", "gid": 472, "group": "472", "md5sum": "4101ad7f2e415b1f02de9b1322f48c74", "mode": "0644", "owner": "472", "size": 158, "src": "/home/gpadmin/.ansible/tmp/ansible-tmp-1751895351.7829616-91659-177041756117763/source", "state": "file", "uid": 472}
2025-07-07 09:35:52,281 p=87684 u=gpadmin n=ansible | TASK [monitoring : Create Grafana dashboard provider configuration] *************************
2025-07-07 09:35:52,811 p=87684 u=gpadmin n=ansible | changed: [MASTER] => {"changed": true, "checksum": "937090cb704688e1cfaecb332ce8ec1c1e0a21a5", "dest": "/home/gpadmin/grafana_data/provisioning/dashboards/provider.yml", "gid": 472, "group": "472", "md5sum": "271e720b420040ec7f70e29f0725cac3", "mode": "0644", "owner": "472", "size": 242, "src": "/home/gpadmin/.ansible/tmp/ansible-tmp-1751895352.3175917-91709-198623402914050/source", "state": "file", "uid": 472}
2025-07-07 09:35:52,825 p=87684 u=gpadmin n=ansible | TASK [monitoring : Copy Grafana dashboards] *************************************************
2025-07-07 09:35:52,906 p=87684 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'instance' is undefined. 'instance' is undefined
2025-07-07 09:35:52,906 p=87684 u=gpadmin n=ansible | failed: [MASTER] (item=node-exporter-dashboard.json) => {"ansible_loop_var": "item", "changed": false, "item": "node-exporter-dashboard.json", "msg": "AnsibleUndefinedVariable: 'instance' is undefined. 'instance' is undefined"}
2025-07-07 09:35:52,955 p=87684 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'name' is undefined. 'name' is undefined
2025-07-07 09:35:52,955 p=87684 u=gpadmin n=ansible | failed: [MASTER] (item=docker-container-dashboard.json) => {"ansible_loop_var": "item", "changed": false, "item": "docker-container-dashboard.json", "msg": "AnsibleUndefinedVariable: 'name' is undefined. 'name' is undefined"}
2025-07-07 09:35:53,006 p=87684 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'name' is undefined. 'name' is undefined
2025-07-07 09:35:53,006 p=87684 u=gpadmin n=ansible | failed: [MASTER] (item=ray-cluster-dashboard.json) => {"ansible_loop_var": "item", "changed": false, "item": "ray-cluster-dashboard.json", "msg": "AnsibleUndefinedVariable: 'name' is undefined. 'name' is undefined"}
2025-07-07 09:35:53,009 p=87684 u=gpadmin n=ansible | PLAY RECAP **********************************************************************************
2025-07-07 09:35:53,009 p=87684 u=gpadmin n=ansible | G-241                      : ok=27   changed=8    unreachable=0    failed=1    skipped=5    rescued=0    ignored=4   
2025-07-07 09:35:53,009 p=87684 u=gpadmin n=ansible | G-242                      : ok=26   changed=8    unreachable=0    failed=1    skipped=5    rescued=0    ignored=4   
2025-07-07 09:35:53,010 p=87684 u=gpadmin n=ansible | G-243                      : ok=26   changed=8    unreachable=0    failed=1    skipped=5    rescued=0    ignored=4   
2025-07-07 09:35:53,010 p=87684 u=gpadmin n=ansible | G-244                      : ok=26   changed=8    unreachable=0    failed=1    skipped=5    rescued=0    ignored=4   
2025-07-07 09:35:53,010 p=87684 u=gpadmin n=ansible | MASTER                     : ok=36   changed=13   unreachable=0    failed=1    skipped=7    rescued=0    ignored=6   
2025-07-07 09:36:04,276 p=92188 u=gpadmin n=ansible | Using /home/gpadmin/cursor-projects/02-Ray-Deploy/ansible.cfg as config file
2025-07-07 09:36:04,561 p=92188 u=gpadmin n=ansible | PLAY [Test Docker Installation] ************************************************
2025-07-07 09:36:04,571 p=92188 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************
2025-07-07 09:36:05,644 p=92188 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:36:05,684 p=92188 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:36:05,704 p=92188 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:36:05,724 p=92188 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:36:06,033 p=92188 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:36:06,112 p=92188 u=gpadmin n=ansible | TASK [docker : Check if Docker is available via which command] *****************
2025-07-07 09:36:06,389 p=92188 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "cmd": ["which", "docker"], "delta": "0:00:00.001331", "end": "2025-07-07 13:36:06.378430", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 13:36:06.377099", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:36:06,391 p=92188 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "cmd": ["which", "docker"], "delta": "0:00:00.001495", "end": "2025-07-07 13:36:06.378554", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 13:36:06.377059", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:36:06,392 p=92188 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "cmd": ["which", "docker"], "delta": "0:00:00.001393", "end": "2025-07-07 13:36:06.377780", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 13:36:06.376387", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:36:06,397 p=92188 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "cmd": ["which", "docker"], "delta": "0:00:00.001595", "end": "2025-07-07 13:36:06.385045", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 13:36:06.383450", "stderr": "", "stderr_lines": [], "stdout": "/snap/bin/docker", "stdout_lines": ["/snap/bin/docker"]}
2025-07-07 09:36:06,509 p=92188 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "cmd": ["which", "docker"], "delta": "0:00:00.005763", "end": "2025-07-07 09:36:06.467045", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 09:36:06.461282", "stderr": "", "stderr_lines": [], "stdout": "/usr/local/bin/docker", "stdout_lines": ["/usr/local/bin/docker"]}
2025-07-07 09:36:06,517 p=92188 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via apt] ***************************
2025-07-07 09:36:06,706 p=92188 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "cmd": ["dpkg", "-l", "docker-ce"], "delta": "0:00:00.005675", "end": "2025-07-07 13:36:06.690392", "failed_when_result": false, "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:36:06.684717", "stderr": "dpkg-query: no packages found matching docker-ce", "stderr_lines": ["dpkg-query: no packages found matching docker-ce"], "stdout": "", "stdout_lines": []}
2025-07-07 09:36:06,715 p=92188 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "cmd": ["dpkg", "-l", "docker-ce"], "delta": "0:00:00.005398", "end": "2025-07-07 13:36:06.703556", "failed_when_result": false, "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:36:06.698158", "stderr": "dpkg-query: no packages found matching docker-ce", "stderr_lines": ["dpkg-query: no packages found matching docker-ce"], "stdout": "", "stdout_lines": []}
2025-07-07 09:36:06,721 p=92188 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "cmd": ["dpkg", "-l", "docker-ce"], "delta": "0:00:00.005232", "end": "2025-07-07 13:36:06.711604", "failed_when_result": false, "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:36:06.706372", "stderr": "dpkg-query: no packages found matching docker-ce", "stderr_lines": ["dpkg-query: no packages found matching docker-ce"], "stdout": "", "stdout_lines": []}
2025-07-07 09:36:06,755 p=92188 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "cmd": ["dpkg", "-l", "docker-ce"], "delta": "0:00:00.005085", "end": "2025-07-07 13:36:06.742977", "failed_when_result": false, "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 13:36:06.737892", "stderr": "dpkg-query: no packages found matching docker-ce", "stderr_lines": ["dpkg-query: no packages found matching docker-ce"], "stdout": "", "stdout_lines": []}
2025-07-07 09:36:06,841 p=92188 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "cmd": ["dpkg", "-l", "docker-ce"], "delta": "0:00:00.033474", "end": "2025-07-07 09:36:06.797697", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 09:36:06.764223", "stderr": "dpkg-query: warning: parsing file '/var/lib/dpkg/status' near line 8100 package 'goose':\n missing 'Maintainer' field", "stderr_lines": ["dpkg-query: warning: parsing file '/var/lib/dpkg/status' near line 8100 package 'goose':", " missing 'Maintainer' field"], "stdout": "Desired=Unknown/Install/Remove/Purge/Hold\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n||/ Name           Version                       Architecture Description\n+++-==============-=============================-============-====================================================\nii  docker-ce      5:28.3.1-1~ubuntu.24.04~noble amd64        Docker: the open-source application container engine", "stdout_lines": ["Desired=Unknown/Install/Remove/Purge/Hold", "| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend", "|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)", "||/ Name           Version                       Architecture Description", "+++-==============-=============================-============-====================================================", "ii  docker-ce      5:28.3.1-1~ubuntu.24.04~noble amd64        Docker: the open-source application container engine"]}
2025-07-07 09:36:06,854 p=92188 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via snap] **************************
2025-07-07 09:36:07,054 p=92188 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "cmd": ["snap", "list", "docker"], "delta": "0:00:00.019819", "end": "2025-07-07 13:36:07.038306", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 13:36:07.018487", "stderr": "", "stderr_lines": [], "stdout": "Name    Version   Rev   Tracking       Publisher    Notes\ndocker  28.1.1+1  3265  latest/stable  canonical**  -", "stdout_lines": ["Name    Version   Rev   Tracking       Publisher    Notes", "docker  28.1.1+1  3265  latest/stable  canonical**  -"]}
2025-07-07 09:36:07,082 p=92188 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "cmd": ["snap", "list", "docker"], "delta": "0:00:00.020526", "end": "2025-07-07 13:36:07.069793", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 13:36:07.049267", "stderr": "", "stderr_lines": [], "stdout": "Name    Version   Rev   Tracking       Publisher    Notes\ndocker  28.1.1+1  3265  latest/stable  canonical**  -", "stdout_lines": ["Name    Version   Rev   Tracking       Publisher    Notes", "docker  28.1.1+1  3265  latest/stable  canonical**  -"]}
2025-07-07 09:36:07,092 p=92188 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "cmd": ["snap", "list", "docker"], "delta": "0:00:00.020219", "end": "2025-07-07 13:36:07.078090", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 13:36:07.057871", "stderr": "", "stderr_lines": [], "stdout": "Name    Version   Rev   Tracking       Publisher    Notes\ndocker  28.1.1+1  3265  latest/stable  canonical**  -", "stdout_lines": ["Name    Version   Rev   Tracking       Publisher    Notes", "docker  28.1.1+1  3265  latest/stable  canonical**  -"]}
2025-07-07 09:36:07,101 p=92188 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "cmd": ["snap", "list", "docker"], "delta": "0:00:00.020275", "end": "2025-07-07 13:36:07.090546", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 13:36:07.070271", "stderr": "", "stderr_lines": [], "stdout": "Name    Version   Rev   Tracking       Publisher    Notes\ndocker  28.1.1+1  3265  latest/stable  canonical**  -", "stdout_lines": ["Name    Version   Rev   Tracking       Publisher    Notes", "docker  28.1.1+1  3265  latest/stable  canonical**  -"]}
2025-07-07 09:36:07,190 p=92188 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "cmd": ["snap", "list", "docker"], "delta": "0:00:00.049066", "end": "2025-07-07 09:36:07.153351", "failed_when_result": false, "msg": "non-zero return code", "rc": 1, "start": "2025-07-07 09:36:07.104285", "stderr": "error: no matching snaps installed", "stderr_lines": ["error: no matching snaps installed"], "stdout": "", "stdout_lines": []}
2025-07-07 09:36:07,208 p=92188 u=gpadmin n=ansible | TASK [docker : Set Docker installation status facts] ***************************
2025-07-07 09:36:07,243 p=92188 u=gpadmin n=ansible | ok: [MASTER] => {"ansible_facts": {"docker_available": true, "docker_via_apt": true, "docker_via_snap": false}, "changed": false}
2025-07-07 09:36:07,257 p=92188 u=gpadmin n=ansible | ok: [G-241] => {"ansible_facts": {"docker_available": true, "docker_via_apt": false, "docker_via_snap": true}, "changed": false}
2025-07-07 09:36:07,273 p=92188 u=gpadmin n=ansible | ok: [G-242] => {"ansible_facts": {"docker_available": true, "docker_via_apt": false, "docker_via_snap": true}, "changed": false}
2025-07-07 09:36:07,276 p=92188 u=gpadmin n=ansible | ok: [G-243] => {"ansible_facts": {"docker_available": true, "docker_via_apt": false, "docker_via_snap": true}, "changed": false}
2025-07-07 09:36:07,287 p=92188 u=gpadmin n=ansible | ok: [G-244] => {"ansible_facts": {"docker_available": true, "docker_via_apt": false, "docker_via_snap": true}, "changed": false}
2025-07-07 09:36:07,298 p=92188 u=gpadmin n=ansible | TASK [docker : Display Docker installation status] *****************************
2025-07-07 09:36:07,333 p=92188 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: True",
        "Docker installed via snap: False"
    ]
}
2025-07-07 09:36:07,349 p=92188 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 09:36:07,365 p=92188 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 09:36:07,367 p=92188 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 09:36:07,384 p=92188 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 09:36:07,393 p=92188 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] *********************************************
2025-07-07 09:36:07,427 p=92188 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,443 p=92188 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,457 p=92188 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,459 p=92188 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,467 p=92188 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,476 p=92188 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] **************************************
2025-07-07 09:36:07,514 p=92188 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,529 p=92188 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,546 p=92188 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,547 p=92188 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,559 p=92188 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,568 p=92188 u=gpadmin n=ansible | TASK [docker : Update apt cache after adding Docker repository] ****************
2025-07-07 09:36:07,602 p=92188 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,617 p=92188 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,632 p=92188 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,632 p=92188 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,643 p=92188 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,652 p=92188 u=gpadmin n=ansible | TASK [docker : Install Docker Engine via APT] **********************************
2025-07-07 09:36:07,686 p=92188 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,703 p=92188 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,723 p=92188 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,723 p=92188 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,736 p=92188 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "not docker_available and not docker_via_snap", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:07,745 p=92188 u=gpadmin n=ansible | TASK [docker : Check Docker service status (systemd)] **************************
2025-07-07 09:36:07,917 p=92188 u=gpadmin n=ansible | ok: [G-241] => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003183", "end": "2025-07-07 13:36:07.902067", "failed_when_result": false, "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:36:07.898884", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:36:07,944 p=92188 u=gpadmin n=ansible | ok: [G-243] => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003090", "end": "2025-07-07 13:36:07.935145", "failed_when_result": false, "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:36:07.932055", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:36:07,945 p=92188 u=gpadmin n=ansible | ok: [G-242] => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003227", "end": "2025-07-07 13:36:07.933690", "failed_when_result": false, "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:36:07.930463", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:36:07,959 p=92188 u=gpadmin n=ansible | ok: [G-244] => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.003178", "end": "2025-07-07 13:36:07.946843", "failed_when_result": false, "msg": "non-zero return code", "rc": 4, "start": "2025-07-07 13:36:07.943665", "stderr": "", "stderr_lines": [], "stdout": "inactive", "stdout_lines": ["inactive"]}
2025-07-07 09:36:08,059 p=92188 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "cmd": ["systemctl", "is-active", "docker"], "delta": "0:00:00.011250", "end": "2025-07-07 09:36:08.015223", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 09:36:08.003973", "stderr": "", "stderr_lines": [], "stdout": "active", "stdout_lines": ["active"]}
2025-07-07 09:36:08,071 p=92188 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] *********************************
2025-07-07 09:36:08,129 p=92188 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "docker_via_apt or (docker_apt_install is defined and docker_apt_install.changed)", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:08,144 p=92188 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "docker_via_apt or (docker_apt_install is defined and docker_apt_install.changed)", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:08,148 p=92188 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "docker_via_apt or (docker_apt_install is defined and docker_apt_install.changed)", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:08,157 p=92188 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "docker_via_apt or (docker_apt_install is defined and docker_apt_install.changed)", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:08,791 p=92188 u=gpadmin n=ansible | ok: [MASTER] => {"changed": false, "enabled": true, "name": "docker", "state": "started", "status": {"ActiveEnterTimestamp": "Sat 2025-07-05 12:10:24 EDT", "ActiveEnterTimestampMonotonic": "14500501", "ActiveExitTimestampMonotonic": "0", "ActiveState": "active", "After": "time-set.target network-online.target system.slice basic.target nss-lookup.target docker.socket firewalld.service systemd-journald.socket containerd.service sysinit.target", "AllowIsolate": "no", "AssertResult": "yes", "AssertTimestamp": "Sat 2025-07-05 12:10:23 EDT", "AssertTimestampMonotonic": "13516583", "Before": "shutdown.target multi-user.target", "BlockIOAccounting": "no", "BlockIOWeight": "[not set]", "CPUAccounting": "yes", "CPUAffinityFromNUMA": "no", "CPUQuotaPerSecUSec": "infinity", "CPUQuotaPeriodUSec": "infinity", "CPUSchedulingPolicy": "0", "CPUSchedulingPriority": "0", "CPUSchedulingResetOnFork": "no", "CPUShares": "[not set]", "CPUUsageNSec": "100987328000", "CPUWeight": "[not set]", "CacheDirectoryMode": "0755", "CanFreeze": "yes", "CanIsolate": "no", "CanReload": "yes", "CanStart": "yes", "CanStop": "yes", "CapabilityBoundingSet": "cap_chown cap_dac_override cap_dac_read_search cap_fowner cap_fsetid cap_kill cap_setgid cap_setuid cap_setpcap cap_linux_immutable cap_net_bind_service cap_net_broadcast cap_net_admin cap_net_raw cap_ipc_lock cap_ipc_owner cap_sys_module cap_sys_rawio cap_sys_chroot cap_sys_ptrace cap_sys_pacct cap_sys_admin cap_sys_boot cap_sys_nice cap_sys_resource cap_sys_time cap_sys_tty_config cap_mknod cap_lease cap_audit_write cap_audit_control cap_setfcap cap_mac_override cap_mac_admin cap_syslog cap_wake_alarm cap_block_suspend cap_audit_read cap_perfmon cap_bpf cap_checkpoint_restore", "CleanResult": "success", "CollectMode": "inactive", "ConditionResult": "yes", "ConditionTimestamp": "Sat 2025-07-05 12:10:23 EDT", "ConditionTimestampMonotonic": "13516581", "ConfigurationDirectoryMode": "0755", "Conflicts": "shutdown.target", "ControlGroup": "/system.slice/docker.service", "ControlGroupId": "7651", "ControlPID": "0", "CoredumpFilter": "0x33", "CoredumpReceive": "no", "DefaultDependencies": "yes", "DefaultMemoryLow": "0", "DefaultMemoryMin": "0", "DefaultStartupMemoryLow": "0", "Delegate": "yes", "DelegateControllers": "cpu cpuset io memory pids", "Description": "Docker Application Container Engine", "DevicePolicy": "auto", "Documentation": "https://docs.docker.com", "DynamicUser": "no", "EffectiveCPUs": "0-31", "EffectiveMemoryNodes": "0", "ExecMainCode": "0", "ExecMainExitTimestampMonotonic": "0", "ExecMainPID": "2018", "ExecMainStartTimestamp": "Sat 2025-07-05 12:10:23 EDT", "ExecMainStartTimestampMonotonic": "13518391", "ExecMainStatus": "0", "ExecReload": "{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecReloadEx": "{ path=/bin/kill ; argv[]=/bin/kill -s HUP $MAINPID ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStart": "{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; ignore_errors=no ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExecStartEx": "{ path=/usr/bin/dockerd ; argv[]=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock ; flags= ; start_time=[n/a] ; stop_time=[n/a] ; pid=0 ; code=(null) ; status=0/0 }", "ExitType": "main", "ExtensionImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "FailureAction": "none", "FileDescriptorStoreMax": "0", "FileDescriptorStorePreserve": "restart", "FinalKillSignal": "9", "FragmentPath": "/usr/lib/systemd/system/docker.service", "FreezerState": "running", "GID": "[not set]", "GuessMainPID": "yes", "IOAccounting": "no", "IOReadBytes": "[not set]", "IOReadOperations": "[not set]", "IOSchedulingClass": "2", "IOSchedulingPriority": "4", "IOWeight": "[not set]", "IOWriteBytes": "[not set]", "IOWriteOperations": "[not set]", "IPAccounting": "no", "IPEgressBytes": "[no data]", "IPEgressPackets": "[no data]", "IPIngressBytes": "[no data]", "IPIngressPackets": "[no data]", "Id": "docker.service", "IgnoreOnIsolate": "no", "IgnoreSIGPIPE": "yes", "InactiveEnterTimestampMonotonic": "0", "InactiveExitTimestamp": "Sat 2025-07-05 12:10:23 EDT", "InactiveExitTimestampMonotonic": "13518558", "InvocationID": "efe8e95911d1408482a24e5be07cc2d7", "JobRunningTimeoutUSec": "infinity", "JobTimeoutAction": "none", "JobTimeoutUSec": "infinity", "KeyringMode": "private", "KillMode": "process", "KillSignal": "15", "LimitAS": "infinity", "LimitASSoft": "infinity", "LimitCORE": "infinity", "LimitCORESoft": "infinity", "LimitCPU": "infinity", "LimitCPUSoft": "infinity", "LimitDATA": "infinity", "LimitDATASoft": "infinity", "LimitFSIZE": "infinity", "LimitFSIZESoft": "infinity", "LimitLOCKS": "infinity", "LimitLOCKSSoft": "infinity", "LimitMEMLOCK": "8388608", "LimitMEMLOCKSoft": "8388608", "LimitMSGQUEUE": "819200", "LimitMSGQUEUESoft": "819200", "LimitNICE": "0", "LimitNICESoft": "0", "LimitNOFILE": "524288", "LimitNOFILESoft": "1024", "LimitNPROC": "infinity", "LimitNPROCSoft": "infinity", "LimitRSS": "infinity", "LimitRSSSoft": "infinity", "LimitRTPRIO": "0", "LimitRTPRIOSoft": "0", "LimitRTTIME": "infinity", "LimitRTTIMESoft": "infinity", "LimitSIGPENDING": "514099", "LimitSIGPENDINGSoft": "514099", "LimitSTACK": "infinity", "LimitSTACKSoft": "8388608", "LoadState": "loaded", "LockPersonality": "no", "LogLevelMax": "-1", "LogRateLimitBurst": "0", "LogRateLimitIntervalUSec": "0", "LogsDirectoryMode": "0755", "MainPID": "2018", "ManagedOOMMemoryPressure": "auto", "ManagedOOMMemoryPressureLimit": "0", "ManagedOOMPreference": "none", "ManagedOOMSwap": "auto", "MemoryAccounting": "yes", "MemoryAvailable": "125916033024", "MemoryCurrent": "3118706688", "MemoryDenyWriteExecute": "no", "MemoryHigh": "infinity", "MemoryKSM": "no", "MemoryLimit": "infinity", "MemoryLow": "0", "MemoryMax": "infinity", "MemoryMin": "0", "MemoryPeak": "3166384128", "MemoryPressureThresholdUSec": "200ms", "MemoryPressureWatch": "auto", "MemorySwapCurrent": "0", "MemorySwapMax": "infinity", "MemorySwapPeak": "0", "MemoryZSwapCurrent": "0", "MemoryZSwapMax": "infinity", "MountAPIVFS": "no", "MountImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "NFileDescriptorStore": "0", "NRestarts": "0", "NUMAPolicy": "n/a", "Names": "docker.service", "NeedDaemonReload": "yes", "Nice": "0", "NoNewPrivileges": "no", "NonBlocking": "no", "NotifyAccess": "main", "OOMPolicy": "continue", "OOMScoreAdjust": "-500", "OnFailureJobMode": "replace", "OnSuccessJobMode": "fail", "Perpetual": "no", "PrivateDevices": "no", "PrivateIPC": "no", "PrivateMounts": "no", "PrivateNetwork": "no", "PrivateTmp": "no", "PrivateUsers": "no", "ProcSubset": "all", "ProtectClock": "no", "ProtectControlGroups": "no", "ProtectHome": "no", "ProtectHostname": "no", "ProtectKernelLogs": "no", "ProtectKernelModules": "no", "ProtectKernelTunables": "no", "ProtectProc": "default", "ProtectSystem": "no", "RefuseManualStart": "no", "RefuseManualStop": "no", "ReloadResult": "success", "ReloadSignal": "1", "RemainAfterExit": "no", "RemoveIPC": "no", "Requires": "sysinit.target docker.socket system.slice", "Restart": "always", "RestartKillSignal": "15", "RestartMaxDelayUSec": "infinity", "RestartMode": "normal", "RestartSteps": "0", "RestartUSec": "2s", "RestartUSecNext": "2s", "RestrictNamespaces": "no", "RestrictRealtime": "no", "RestrictSUIDSGID": "no", "Result": "success", "RootDirectoryStartOnly": "no", "RootEphemeral": "no", "RootImagePolicy": "root=verity+signed+encrypted+unprotected+absent:usr=verity+signed+encrypted+unprotected+absent:home=encrypted+unprotected+absent:srv=encrypted+unprotected+absent:tmp=encrypted+unprotected+absent:var=encrypted+unprotected+absent", "RuntimeDirectoryMode": "0755", "RuntimeDirectoryPreserve": "no", "RuntimeMaxUSec": "infinity", "RuntimeRandomizedExtraUSec": "0", "SameProcessGroup": "no", "SecureBits": "0", "SendSIGHUP": "no", "SendSIGKILL": "yes", "SetLoginEnvironment": "no", "Slice": "system.slice", "StandardError": "inherit", "StandardInput": "null", "StandardOutput": "journal", "StartLimitAction": "none", "StartLimitBurst": "3", "StartLimitIntervalUSec": "1min", "StartupBlockIOWeight": "[not set]", "StartupCPUShares": "[not set]", "StartupCPUWeight": "[not set]", "StartupIOWeight": "[not set]", "StartupMemoryHigh": "infinity", "StartupMemoryLow": "0", "StartupMemoryMax": "infinity", "StartupMemorySwapMax": "infinity", "StartupMemoryZSwapMax": "infinity", "StateChangeTimestamp": "Sat 2025-07-05 12:10:24 EDT", "StateChangeTimestampMonotonic": "14500501", "StateDirectoryMode": "0755", "StatusErrno": "0", "StopWhenUnneeded": "no", "SubState": "running", "SuccessAction": "none", "SurviveFinalKillSignal": "no", "SyslogFacility": "3", "SyslogLevel": "6", "SyslogLevelPrefix": "yes", "SyslogPriority": "30", "SystemCallErrorNumber": "2147483646", "TTYReset": "no", "TTYVHangup": "no", "TTYVTDisallocate": "no", "TasksAccounting": "yes", "TasksCurrent": "34", "TasksMax": "infinity", "TimeoutAbortUSec": "1min 30s", "TimeoutCleanUSec": "infinity", "TimeoutStartFailureMode": "terminate", "TimeoutStartUSec": "infinity", "TimeoutStopFailureMode": "terminate", "TimeoutStopUSec": "1min 30s", "TimerSlackNSec": "50000", "Transient": "no", "TriggeredBy": "docker.socket", "Type": "notify", "UID": "[not set]", "UMask": "0022", "UnitFilePreset": "enabled", "UnitFileState": "enabled", "UtmpMode": "init", "WantedBy": "multi-user.target", "Wants": "containerd.service network-online.target", "WatchdogSignal": "6", "WatchdogTimestampMonotonic": "0", "WatchdogUSec": "0"}}
2025-07-07 09:36:08,808 p=92188 u=gpadmin n=ansible | TASK [docker : Install Docker via snap if not installed via apt] ***************
2025-07-07 09:36:08,836 p=92188 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "not docker_available and not docker_via_apt", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:08,852 p=92188 u=gpadmin n=ansible | skipping: [G-241] => {"changed": false, "false_condition": "not docker_available and not docker_via_apt", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:08,884 p=92188 u=gpadmin n=ansible | skipping: [G-242] => {"changed": false, "false_condition": "not docker_available and not docker_via_apt", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:08,884 p=92188 u=gpadmin n=ansible | skipping: [G-243] => {"changed": false, "false_condition": "not docker_available and not docker_via_apt", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:08,896 p=92188 u=gpadmin n=ansible | skipping: [G-244] => {"changed": false, "false_condition": "not docker_available and not docker_via_apt", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:08,906 p=92188 u=gpadmin n=ansible | TASK [docker : Wait for snap Docker installation to complete] ******************
2025-07-07 09:36:08,925 p=92188 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_snap_install is defined and docker_snap_install.changed", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:08,934 p=92188 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] ***********
2025-07-07 09:36:08,968 p=92188 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_via_snap or (docker_snap_install is defined and docker_snap_install.changed)", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:09,128 p=92188 u=gpadmin n=ansible | ok: [G-241] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.013732", "end": "2025-07-07 13:36:09.110927", "msg": "", "rc": 0, "start": "2025-07-07 13:36:09.097195", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:36:09,149 p=92188 u=gpadmin n=ansible | ok: [G-242] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.014286", "end": "2025-07-07 13:36:09.136122", "msg": "", "rc": 0, "start": "2025-07-07 13:36:09.121836", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:36:09,153 p=92188 u=gpadmin n=ansible | ok: [G-243] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.013834", "end": "2025-07-07 13:36:09.141738", "msg": "", "rc": 0, "start": "2025-07-07 13:36:09.127904", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:36:09,178 p=92188 u=gpadmin n=ansible | ok: [G-244] => {"attempts": 1, "changed": false, "cmd": "snap changes | grep -q 'Doing.*docker' && sleep 5 || echo 'No in-progress changes'", "delta": "0:00:00.013917", "end": "2025-07-07 13:36:09.163961", "msg": "", "rc": 0, "start": "2025-07-07 13:36:09.150044", "stderr": "", "stderr_lines": [], "stdout": "No in-progress changes", "stdout_lines": ["No in-progress changes"]}
2025-07-07 09:36:09,192 p=92188 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ************************************
2025-07-07 09:36:09,227 p=92188 u=gpadmin n=ansible | skipping: [MASTER] => {"changed": false, "false_condition": "docker_via_snap or (docker_snap_install is defined and docker_snap_install.changed)", "skip_reason": "Conditional result was False"}
2025-07-07 09:36:09,550 p=92188 u=gpadmin n=ansible | changed: [G-241] => {"changed": true, "cmd": ["snap", "start", "docker"], "delta": "0:00:00.154072", "end": "2025-07-07 13:36:09.534312", "failed_when_result": false, "msg": "", "rc": 0, "start": "2025-07-07 13:36:09.380240", "stderr": "", "stderr_lines": [], "stdout": "Started.", "stdout_lines": ["Started."]}
2025-07-07 09:36:30,596 p=92188 u=gpadmin n=ansible |  [ERROR]: User interrupted execution

2025-07-07 09:37:00,640 p=93542 u=gpadmin n=ansible | G-241 | CHANGED | rc=0 >>
Docker version 28.1.1+1, build 068a01e

2025-07-07 09:37:00,642 p=93542 u=gpadmin n=ansible | G-243 | CHANGED | rc=0 >>
Docker version 28.1.1+1, build 068a01e

2025-07-07 09:37:00,655 p=93542 u=gpadmin n=ansible | G-244 | CHANGED | rc=0 >>
Docker version 28.1.1+1, build 068a01e

2025-07-07 09:37:00,672 p=93542 u=gpadmin n=ansible | G-242 | CHANGED | rc=0 >>
Docker version 28.1.1+1, build 068a01e

2025-07-07 09:37:00,720 p=93542 u=gpadmin n=ansible | MASTER | CHANGED | rc=0 >>
Docker version 28.3.1, build 38b7060

2025-07-07 09:37:06,151 p=93676 u=gpadmin n=ansible | MASTER | CHANGED | rc=0 >>
Client: Docker Engine - Community
 Version:    28.3.1
 Context:    default
 Debug Mode: false
 Plugins:
  ai: Docker AI Agent - Ask Gordon (Docker Inc.)
    Version:  v1.6.0
    Path:     /usr/lib/docker/cli-plugins/docker-ai
  buildx: Docker Buildx (Docker Inc.)
    Version:  v0.25.0-desktop.1
    Path:     /usr/lib/docker/cli-plugins/docker-buildx
  cloud: Docker Cloud (Docker Inc.)
    Version:  v0.4.2
    Path:     /usr/lib/docker/cli-plugins/docker-cloud
  compose: Docker Compose (Docker Inc.)
    Version:  v2.38.1-desktop.1
    Path:     /usr/lib/docker/cli-plugins/docker-compose
  debug: Get a shell into any image or container (Docker Inc.)
    Version:  0.0.41
    Path:     /usr/lib/docker/cli-plugins/docker-debug
  desktop: Docker Desktop commands (Docker Inc.)
    Version:  v0.1.11
    Path:     /usr/lib/docker/cli-plugins/docker-desktop
  extension: Manages Docker extensions (Docker Inc.)
    Version:  v0.2.29
    Path:     /usr/lib/docker/cli-plugins/docker-extension
  init: Creates Docker-related starter files for your project (Docker Inc.)
    Version:  v1.4.0
    Path:     /usr/lib/docker/cli-plugins/docker-init
  mcp: Docker MCP Plugin (Docker Inc.)
    Version:  v0.9.3
    Path:     /usr/lib/docker/cli-plugins/docker-mcp
  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)
    Version:  0.6.0
    Path:     /usr/lib/docker/cli-plugins/docker-sbom
  scout: Docker Scout (Docker Inc.)
    Version:  v1.18.1
    Path:     /usr/lib/docker/cli-plugins/docker-scout

Server:
 Containers: 4
  Running: 3
  Paused: 0
  Stopped: 1
 Images: 5
 Server Version: 28.3.1
 Storage Driver: overlay2
  Backing Filesystem: extfs
  Supports d_type: true
  Using metacopy: false
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: systemd
 Cgroup Version: 2
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog
 CDI spec directories:
  /etc/cdi
  /var/run/cdi
 Swarm: inactive
 Runtimes: nvidia runc io.containerd.runc.v2
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da
 runc version: v1.2.5-0-g59923ef
 init version: de40ad0
 Security Options:
  apparmor
  seccomp
   Profile: builtin
  cgroupns
 Kernel Version: 6.11.0-29-generic
 Operating System: Ubuntu 24.04.2 LTS
 OSType: linux
 Architecture: x86_64
 CPUs: 32
 Total Memory: 125.7GiB
 Name: G-K3S-Master
 ID: af7f080b-452c-4cde-ba9f-5a831e784149
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 Experimental: false
 Insecure Registries:
  ::1/128
  127.0.0.0/8
 Live Restore Enabled: false

2025-07-07 09:37:07,302 p=93676 u=gpadmin n=ansible | G-244 | CHANGED | rc=0 >>
Client:
 Version:    28.1.1+1
 Context:    default
 Debug Mode: false
 Plugins:
  buildx: Docker Buildx (Docker Inc.)
    Version:  v0.20.1
    Path:     /usr/libexec/docker/cli-plugins/docker-buildx
  compose: Docker Compose (Docker Inc.)
    Version:  v2.33.1
    Path:     /usr/libexec/docker/cli-plugins/docker-compose

Server:
 Containers: 3
  Running: 2
  Paused: 0
  Stopped: 1
 Images: 3
 Server Version: 28.1.1+1
 Storage Driver: overlay2
  Backing Filesystem: extfs
  Supports d_type: true
  Using metacopy: false
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: systemd
 Cgroup Version: 2
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog
 Swarm: inactive
 Runtimes: io.containerd.runc.v2 runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da
 runc version: 
 init version: de40ad0
 Security Options:
  apparmor
  seccomp
   Profile: builtin
  cgroupns
 Kernel Version: 6.8.0-63-generic
 Operating System: Ubuntu Core 22
 OSType: linux
 Architecture: x86_64
 CPUs: 16
 Total Memory: 125.6GiB
 Name: g-244
 ID: 1848662b-1893-4835-914c-845e72adb198
 Docker Root Dir: /var/snap/docker/common/var-lib-docker
 Debug Mode: false
 Experimental: false
 Insecure Registries:
  ::1/128
  127.0.0.0/8
 Live Restore Enabled: false

2025-07-07 09:37:07,303 p=93676 u=gpadmin n=ansible | G-243 | CHANGED | rc=0 >>
Client:
 Version:    28.1.1+1
 Context:    default
 Debug Mode: false
 Plugins:
  buildx: Docker Buildx (Docker Inc.)
    Version:  v0.20.1
    Path:     /usr/libexec/docker/cli-plugins/docker-buildx
  compose: Docker Compose (Docker Inc.)
    Version:  v2.33.1
    Path:     /usr/libexec/docker/cli-plugins/docker-compose

Server:
 Containers: 3
  Running: 2
  Paused: 0
  Stopped: 1
 Images: 3
 Server Version: 28.1.1+1
 Storage Driver: overlay2
  Backing Filesystem: extfs
  Supports d_type: true
  Using metacopy: false
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: systemd
 Cgroup Version: 2
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog
 Swarm: inactive
 Runtimes: io.containerd.runc.v2 runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da
 runc version: 
 init version: de40ad0
 Security Options:
  apparmor
  seccomp
   Profile: builtin
  cgroupns
 Kernel Version: 6.8.0-63-generic
 Operating System: Ubuntu Core 22
 OSType: linux
 Architecture: x86_64
 CPUs: 16
 Total Memory: 125.6GiB
 Name: g-243
 ID: 02b692aa-98e9-472a-939b-51124f2e78b0
 Docker Root Dir: /var/snap/docker/common/var-lib-docker
 Debug Mode: false
 Experimental: false
 Insecure Registries:
  ::1/128
  127.0.0.0/8
 Live Restore Enabled: false

2025-07-07 09:37:07,304 p=93676 u=gpadmin n=ansible | G-241 | CHANGED | rc=0 >>
Client:
 Version:    28.1.1+1
 Context:    default
 Debug Mode: false
 Plugins:
  buildx: Docker Buildx (Docker Inc.)
    Version:  v0.20.1
    Path:     /usr/libexec/docker/cli-plugins/docker-buildx
  compose: Docker Compose (Docker Inc.)
    Version:  v2.33.1
    Path:     /usr/libexec/docker/cli-plugins/docker-compose

Server:
 Containers: 3
  Running: 2
  Paused: 0
  Stopped: 1
 Images: 3
 Server Version: 28.1.1+1
 Storage Driver: overlay2
  Backing Filesystem: extfs
  Supports d_type: true
  Using metacopy: false
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: systemd
 Cgroup Version: 2
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog
 Swarm: inactive
 Runtimes: runc io.containerd.runc.v2 nvidia
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da
 runc version: 
 init version: de40ad0
 Security Options:
  apparmor
  seccomp
   Profile: builtin
  cgroupns
 Kernel Version: 6.8.0-63-generic
 Operating System: Ubuntu Core 22
 OSType: linux
 Architecture: x86_64
 CPUs: 16
 Total Memory: 125.6GiB
 Name: g-241
 ID: 33a898e7-6247-45e3-8105-8fc4cb4c9a8d
 Docker Root Dir: /var/snap/docker/common/var-lib-docker
 Debug Mode: false
 Experimental: false
 Insecure Registries:
  ::1/128
  127.0.0.0/8
 Live Restore Enabled: false

2025-07-07 09:37:07,314 p=93676 u=gpadmin n=ansible | G-242 | CHANGED | rc=0 >>
Client:
 Version:    28.1.1+1
 Context:    default
 Debug Mode: false
 Plugins:
  buildx: Docker Buildx (Docker Inc.)
    Version:  v0.20.1
    Path:     /usr/libexec/docker/cli-plugins/docker-buildx
  compose: Docker Compose (Docker Inc.)
    Version:  v2.33.1
    Path:     /usr/libexec/docker/cli-plugins/docker-compose

Server:
 Containers: 3
  Running: 2
  Paused: 0
  Stopped: 1
 Images: 3
 Server Version: 28.1.1+1
 Storage Driver: overlay2
  Backing Filesystem: extfs
  Supports d_type: true
  Using metacopy: false
  Native Overlay Diff: true
  userxattr: false
 Logging Driver: json-file
 Cgroup Driver: systemd
 Cgroup Version: 2
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog
 Swarm: inactive
 Runtimes: io.containerd.runc.v2 runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da
 runc version: 
 init version: de40ad0
 Security Options:
  apparmor
  seccomp
   Profile: builtin
  cgroupns
 Kernel Version: 6.8.0-63-generic
 Operating System: Ubuntu Core 22
 OSType: linux
 Architecture: x86_64
 CPUs: 16
 Total Memory: 125.6GiB
 Name: g-242
 ID: 8e4ab29c-4a92-464b-a056-1f2f03739d0a
 Docker Root Dir: /var/snap/docker/common/var-lib-docker
 Debug Mode: false
 Experimental: false
 Insecure Registries:
  ::1/128
  127.0.0.0/8
 Live Restore Enabled: false

2025-07-07 09:37:16,034 p=94080 u=gpadmin n=ansible | G-242 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                       COMMAND                  CREATED              STATUS              PORTS     NAMES
c04598c908e5   prom/node-exporter:v1.6.1   "/bin/node_exporter …"   About a minute ago   Up About a minute             node-exporter
7fbaa9e64aad   rayproject/ray:2.9.0        "/home/gpadmin/ray_t…"   About a minute ago   Up About a minute             ray_worker

2025-07-07 09:37:16,038 p=94080 u=gpadmin n=ansible | G-241 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                       COMMAND                  CREATED              STATUS              PORTS     NAMES
9b79793063ea   prom/node-exporter:v1.6.1   "/bin/node_exporter …"   About a minute ago   Up About a minute             node-exporter
ae21478cc53a   rayproject/ray:2.9.0        "/home/gpadmin/ray_t…"   About a minute ago   Up About a minute             ray_worker

2025-07-07 09:37:16,039 p=94080 u=gpadmin n=ansible | G-243 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                       COMMAND                  CREATED              STATUS              PORTS     NAMES
35590ea5ea68   prom/node-exporter:v1.6.1   "/bin/node_exporter …"   About a minute ago   Up About a minute             node-exporter
28d337ef57bd   rayproject/ray:2.9.0        "/home/gpadmin/ray_t…"   About a minute ago   Up About a minute             ray_worker

2025-07-07 09:37:16,063 p=94080 u=gpadmin n=ansible | G-244 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                       COMMAND                  CREATED              STATUS              PORTS     NAMES
54fb99bb27ad   prom/node-exporter:v1.6.1   "/bin/node_exporter …"   About a minute ago   Up About a minute             node-exporter
56ce0f34b799   rayproject/ray:2.9.0        "/home/gpadmin/ray_t…"   About a minute ago   Up About a minute             ray_worker

2025-07-07 09:37:16,112 p=94080 u=gpadmin n=ansible | MASTER | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                              COMMAND                  CREATED              STATUS                          PORTS     NAMES
335acfbb6073   prom/prometheus:v2.45.0            "/bin/prometheus --c…"   About a minute ago   Restarting (2) 32 seconds ago             prometheus
2d12943bbe8b   gcr.io/cadvisor/cadvisor:v0.47.2   "/usr/bin/cadvisor -…"   About a minute ago   Up About a minute (unhealthy)             cadvisor
bd6731114332   prom/node-exporter:v1.6.1          "/bin/node_exporter …"   About a minute ago   Up About a minute                         node-exporter
a30361101b64   rayproject/ray:2.9.0               "/home/gpadmin/ray_t…"   2 minutes ago        Up 2 minutes                              ray_head

2025-07-07 09:37:27,894 p=94310 u=gpadmin n=ansible | MASTER | CHANGED | rc=0 >>
ts=2025-07-07T13:35:50.264Z caller=main.go:534 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2025-07-07T13:35:50.264Z caller=main.go:578 level=info msg="Starting Prometheus Server" mode=server version="(version=2.45.0, branch=HEAD, revision=8ef767e396bf8445f009f945b0162fd71827f445)"
ts=2025-07-07T13:35:50.264Z caller=main.go:583 level=info build_context="(go=go1.20.5, platform=linux/amd64, user=root@920118f645b7, date=20230623-15:09:49, tags=netgo,builtinassets,stringlabels)"
ts=2025-07-07T13:35:50.264Z caller=main.go:584 level=info host_details="(Linux 6.11.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2 x86_64 G-K3S-Master (none))"
ts=2025-07-07T13:35:50.264Z caller=main.go:585 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-07-07T13:35:50.264Z caller=main.go:586 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-07-07T13:35:50.265Z caller=query_logger.go:91 level=error component=activeQueryTracker msg="Error opening query log file" file=/prometheus/queries.active err="open /prometheus/queries.active: permission denied"
panic: Unable to create mmap-ed active query log

goroutine 1 [running]:
github.com/prometheus/prometheus/promql.NewActiveQueryTracker({0x7ffe15f63ee0, 0xb}, 0x14, {0x3d0af20, 0xc000c83270})
	/app/promql/query_logger.go:121 +0x3cd
main.main()
	/app/cmd/prometheus/main.go:640 +0x7013
ts=2025-07-07T13:35:50.580Z caller=main.go:534 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2025-07-07T13:35:50.580Z caller=main.go:578 level=info msg="Starting Prometheus Server" mode=server version="(version=2.45.0, branch=HEAD, revision=8ef767e396bf8445f009f945b0162fd71827f445)"
ts=2025-07-07T13:35:50.580Z caller=main.go:583 level=info build_context="(go=go1.20.5, platform=linux/amd64, user=root@920118f645b7, date=20230623-15:09:49, tags=netgo,builtinassets,stringlabels)"
ts=2025-07-07T13:35:50.580Z caller=main.go:584 level=info host_details="(Linux 6.11.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2 x86_64 G-K3S-Master (none))"
ts=2025-07-07T13:35:50.581Z caller=main.go:585 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-07-07T13:35:50.581Z caller=main.go:586 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-07-07T13:35:50.581Z caller=query_logger.go:91 level=error component=activeQueryTracker msg="Error opening query log file" file=/prometheus/queries.active err="open /prometheus/queries.active: permission denied"
panic: Unable to create mmap-ed active query log

goroutine 1 [running]:
github.com/prometheus/prometheus/promql.NewActiveQueryTracker({0x7ffea3a6fee0, 0xb}, 0x14, {0x3d0af20, 0xc000d8f3b0})
	/app/promql/query_logger.go:121 +0x3cd
main.main()
	/app/cmd/prometheus/main.go:640 +0x7013
ts=2025-07-07T13:35:50.977Z caller=main.go:534 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2025-07-07T13:35:50.977Z caller=main.go:578 level=info msg="Starting Prometheus Server" mode=server version="(version=2.45.0, branch=HEAD, revision=8ef767e396bf8445f009f945b0162fd71827f445)"
ts=2025-07-07T13:35:50.977Z caller=main.go:583 level=info build_context="(go=go1.20.5, platform=linux/amd64, user=root@920118f645b7, date=20230623-15:09:49, tags=netgo,builtinassets,stringlabels)"
ts=2025-07-07T13:35:50.977Z caller=main.go:584 level=info host_details="(Linux 6.11.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2 x86_64 G-K3S-Master (none))"
ts=2025-07-07T13:35:50.977Z caller=main.go:585 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-07-07T13:35:50.977Z caller=main.go:586 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-07-07T13:35:50.978Z caller=query_logger.go:91 level=error component=activeQueryTracker msg="Error opening query log file" file=/prometheus/queries.active err="open /prometheus/queries.active: permission denied"
panic: Unable to create mmap-ed active query log

goroutine 1 [running]:
github.com/prometheus/prometheus/promql.NewActiveQueryTracker({0x7fff91dfeee0, 0xb}, 0x14, {0x3d0af20, 0xc000a82dc0})
	/app/promql/query_logger.go:121 +0x3cd
main.main()
	/app/cmd/prometheus/main.go:640 +0x7013
ts=2025-07-07T13:35:51.575Z caller=main.go:534 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2025-07-07T13:35:51.575Z caller=main.go:578 level=info msg="Starting Prometheus Server" mode=server version="(version=2.45.0, branch=HEAD, revision=8ef767e396bf8445f009f945b0162fd71827f445)"
ts=2025-07-07T13:35:51.575Z caller=main.go:583 level=info build_context="(go=go1.20.5, platform=linux/amd64, user=root@920118f645b7, date=20230623-15:09:49, tags=netgo,builtinassets,stringlabels)"
ts=2025-07-07T13:35:51.575Z caller=main.go:584 level=info host_details="(Linux 6.11.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2 x86_64 G-K3S-Master (none))"
ts=2025-07-07T13:35:51.575Z caller=main.go:585 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-07-07T13:35:51.575Z caller=main.go:586 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-07-07T13:35:51.575Z caller=query_logger.go:91 level=error component=activeQueryTracker msg="Error opening query log file" file=/prometheus/queries.active err="open /prometheus/queries.active: permission denied"
panic: Unable to create mmap-ed active query log

goroutine 1 [running]:
github.com/prometheus/prometheus/promql.NewActiveQueryTracker({0x7fff543abee0, 0xb}, 0x14, {0x3d0af20, 0xc0005b06e0})
	/app/promql/query_logger.go:121 +0x3cd
main.main()
	/app/cmd/prometheus/main.go:640 +0x7013
ts=2025-07-07T13:35:52.572Z caller=main.go:534 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2025-07-07T13:35:52.572Z caller=main.go:578 level=info msg="Starting Prometheus Server" mode=server version="(version=2.45.0, branch=HEAD, revision=8ef767e396bf8445f009f945b0162fd71827f445)"
ts=2025-07-07T13:35:52.572Z caller=main.go:583 level=info build_context="(go=go1.20.5, platform=linux/amd64, user=root@920118f645b7, date=20230623-15:09:49, tags=netgo,builtinassets,stringlabels)"
ts=2025-07-07T13:35:52.572Z caller=main.go:584 level=info host_details="(Linux 6.11.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2 x86_64 G-K3S-Master (none))"
ts=2025-07-07T13:35:52.572Z caller=main.go:585 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-07-07T13:35:52.572Z caller=main.go:586 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-07-07T13:35:52.572Z caller=query_logger.go:91 level=error component=activeQueryTracker msg="Error opening query log file" file=/prometheus/queries.active err="open /prometheus/queries.active: permission denied"
panic: Unable to create mmap-ed active query log

goroutine 1 [running]:
github.com/prometheus/prometheus/promql.NewActiveQueryTracker({0x7ffca8f62ee0, 0xb}, 0x14, {0x3d0af20, 0xc000c9b040})
	/app/promql/query_logger.go:121 +0x3cd
main.main()
	/app/cmd/prometheus/main.go:640 +0x7013
ts=2025-07-07T13:35:54.362Z caller=main.go:534 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2025-07-07T13:35:54.362Z caller=main.go:578 level=info msg="Starting Prometheus Server" mode=server version="(version=2.45.0, branch=HEAD, revision=8ef767e396bf8445f009f945b0162fd71827f445)"
ts=2025-07-07T13:35:54.362Z caller=main.go:583 level=info build_context="(go=go1.20.5, platform=linux/amd64, user=root@920118f645b7, date=20230623-15:09:49, tags=netgo,builtinassets,stringlabels)"
ts=2025-07-07T13:35:54.362Z caller=main.go:584 level=info host_details="(Linux 6.11.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2 x86_64 G-K3S-Master (none))"
ts=2025-07-07T13:35:54.362Z caller=main.go:585 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-07-07T13:35:54.362Z caller=main.go:586 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-07-07T13:35:54.362Z caller=query_logger.go:91 level=error component=activeQueryTracker msg="Error opening query log file" file=/prometheus/queries.active err="open /prometheus/queries.active: permission denied"
panic: Unable to create mmap-ed active query log

goroutine 1 [running]:
github.com/prometheus/prometheus/promql.NewActiveQueryTracker({0x7ffe128cbee0, 0xb}, 0x14, {0x3d0af20, 0xc000da3310})
	/app/promql/query_logger.go:121 +0x3cd
main.main()
	/app/cmd/prometheus/main.go:640 +0x7013
ts=2025-07-07T13:35:57.784Z caller=main.go:534 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2025-07-07T13:35:57.784Z caller=main.go:578 level=info msg="Starting Prometheus Server" mode=server version="(version=2.45.0, branch=HEAD, revision=8ef767e396bf8445f009f945b0162fd71827f445)"
ts=2025-07-07T13:35:57.784Z caller=main.go:583 level=info build_context="(go=go1.20.5, platform=linux/amd64, user=root@920118f645b7, date=20230623-15:09:49, tags=netgo,builtinassets,stringlabels)"
ts=2025-07-07T13:35:57.784Z caller=main.go:584 level=info host_details="(Linux 6.11.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2 x86_64 G-K3S-Master (none))"
ts=2025-07-07T13:35:57.784Z caller=main.go:585 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-07-07T13:35:57.784Z caller=main.go:586 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-07-07T13:35:57.784Z caller=query_logger.go:91 level=error component=activeQueryTracker msg="Error opening query log file" file=/prometheus/queries.active err="open /prometheus/queries.active: permission denied"
panic: Unable to create mmap-ed active query log

goroutine 1 [running]:
github.com/prometheus/prometheus/promql.NewActiveQueryTracker({0x7ffef24c3ee0, 0xb}, 0x14, {0x3d0af20, 0xc000532870})
	/app/promql/query_logger.go:121 +0x3cd
main.main()
	/app/cmd/prometheus/main.go:640 +0x7013
ts=2025-07-07T13:36:04.389Z caller=main.go:534 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2025-07-07T13:36:04.389Z caller=main.go:578 level=info msg="Starting Prometheus Server" mode=server version="(version=2.45.0, branch=HEAD, revision=8ef767e396bf8445f009f945b0162fd71827f445)"
ts=2025-07-07T13:36:04.389Z caller=main.go:583 level=info build_context="(go=go1.20.5, platform=linux/amd64, user=root@920118f645b7, date=20230623-15:09:49, tags=netgo,builtinassets,stringlabels)"
ts=2025-07-07T13:36:04.389Z caller=main.go:584 level=info host_details="(Linux 6.11.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2 x86_64 G-K3S-Master (none))"
ts=2025-07-07T13:36:04.389Z caller=main.go:585 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-07-07T13:36:04.389Z caller=main.go:586 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-07-07T13:36:04.389Z caller=query_logger.go:91 level=error component=activeQueryTracker msg="Error opening query log file" file=/prometheus/queries.active err="open /prometheus/queries.active: permission denied"
panic: Unable to create mmap-ed active query log

goroutine 1 [running]:
github.com/prometheus/prometheus/promql.NewActiveQueryTracker({0x7ffd329daee0, 0xb}, 0x14, {0x3d0af20, 0xc000286870})
	/app/promql/query_logger.go:121 +0x3cd
main.main()
	/app/cmd/prometheus/main.go:640 +0x7013
ts=2025-07-07T13:36:17.397Z caller=main.go:534 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2025-07-07T13:36:17.397Z caller=main.go:578 level=info msg="Starting Prometheus Server" mode=server version="(version=2.45.0, branch=HEAD, revision=8ef767e396bf8445f009f945b0162fd71827f445)"
ts=2025-07-07T13:36:17.397Z caller=main.go:583 level=info build_context="(go=go1.20.5, platform=linux/amd64, user=root@920118f645b7, date=20230623-15:09:49, tags=netgo,builtinassets,stringlabels)"
ts=2025-07-07T13:36:17.397Z caller=main.go:584 level=info host_details="(Linux 6.11.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2 x86_64 G-K3S-Master (none))"
ts=2025-07-07T13:36:17.397Z caller=main.go:585 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-07-07T13:36:17.397Z caller=main.go:586 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-07-07T13:36:17.397Z caller=query_logger.go:91 level=error component=activeQueryTracker msg="Error opening query log file" file=/prometheus/queries.active err="open /prometheus/queries.active: permission denied"
panic: Unable to create mmap-ed active query log

goroutine 1 [running]:
github.com/prometheus/prometheus/promql.NewActiveQueryTracker({0x7fff7b03aee0, 0xb}, 0x14, {0x3d0af20, 0xc000e0d360})
	/app/promql/query_logger.go:121 +0x3cd
main.main()
	/app/cmd/prometheus/main.go:640 +0x7013
ts=2025-07-07T13:36:43.204Z caller=main.go:534 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2025-07-07T13:36:43.204Z caller=main.go:578 level=info msg="Starting Prometheus Server" mode=server version="(version=2.45.0, branch=HEAD, revision=8ef767e396bf8445f009f945b0162fd71827f445)"
ts=2025-07-07T13:36:43.204Z caller=main.go:583 level=info build_context="(go=go1.20.5, platform=linux/amd64, user=root@920118f645b7, date=20230623-15:09:49, tags=netgo,builtinassets,stringlabels)"
ts=2025-07-07T13:36:43.205Z caller=main.go:584 level=info host_details="(Linux 6.11.0-29-generic #29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2 x86_64 G-K3S-Master (none))"
ts=2025-07-07T13:36:43.205Z caller=main.go:585 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-07-07T13:36:43.205Z caller=main.go:586 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-07-07T13:36:43.205Z caller=query_logger.go:91 level=error component=activeQueryTracker msg="Error opening query log file" file=/prometheus/queries.active err="open /prometheus/queries.active: permission denied"
panic: Unable to create mmap-ed active query log

goroutine 1 [running]:
github.com/prometheus/prometheus/promql.NewActiveQueryTracker({0x7ffd39d34ee0, 0xb}, 0x14, {0x3d0af20, 0xc00073f770})
	/app/promql/query_logger.go:121 +0x3cd
main.main()
	/app/cmd/prometheus/main.go:640 +0x7013

2025-07-07 09:38:12,571 p=94823 u=gpadmin n=ansible | ERROR! 'ansible.builtin.file' is not a valid attribute for a Play

The error appears to be in '/home/gpadmin/cursor-projects/02-Ray-Deploy/roles/monitoring/tasks/main.yml': line 4, column 3, but may
be elsewhere in the file depending on the exact syntax problem.

The offending line appears to be:


- name: Create Prometheus data directory
  ^ here

2025-07-07 09:39:46,138 p=95771 u=gpadmin n=ansible | PLAY [Fix Monitoring Permissions] *********************************************************
2025-07-07 09:39:46,147 p=95771 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:39:47,617 p=95771 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:39:47,653 p=95771 u=gpadmin n=ansible | TASK [Stop all monitoring containers] *****************************************************
2025-07-07 09:39:48,305 p=95771 u=gpadmin n=ansible | failed: [MASTER] (item=prometheus) => {"ansible_loop_var": "item", "changed": false, "item": "prometheus", "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:39:48,749 p=95771 u=gpadmin n=ansible | failed: [MASTER] (item=grafana) => {"ansible_loop_var": "item", "changed": false, "item": "grafana", "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:39:48,751 p=95771 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:39:48,765 p=95771 u=gpadmin n=ansible | TASK [Fix Prometheus data directory permissions] ******************************************
2025-07-07 09:39:49,200 p=95771 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:39:49,208 p=95771 u=gpadmin n=ansible | TASK [Fix Grafana data directory permissions] *********************************************
2025-07-07 09:39:49,485 p=95771 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:39:49,499 p=95771 u=gpadmin n=ansible | TASK [Restart monitoring containers] ******************************************************
2025-07-07 09:39:49,560 p=95771 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus data directory] **************************************
2025-07-07 09:39:49,881 p=95771 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:39:49,895 p=95771 u=gpadmin n=ansible | TASK [monitoring : Create Grafana data directory] *****************************************
2025-07-07 09:39:50,206 p=95771 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:39:50,220 p=95771 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Node Exporter container (idempotency)] ********
2025-07-07 09:39:50,667 p=95771 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:39:50,667 p=95771 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:39:50,681 p=95771 u=gpadmin n=ansible | TASK [monitoring : Start Node Exporter container] *****************************************
2025-07-07 09:39:51,188 p=95771 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:39:51,201 p=95771 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing cAdvisor container (idempotency)] *************
2025-07-07 09:39:51,698 p=95771 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:39:51,699 p=95771 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:39:51,712 p=95771 u=gpadmin n=ansible | TASK [monitoring : Start cAdvisor container] **********************************************
2025-07-07 09:39:52,240 p=95771 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:39:52,257 p=95771 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Prometheus container (idempotency)] ***********
2025-07-07 09:39:52,736 p=95771 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:39:52,736 p=95771 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:39:52,750 p=95771 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus configuration] ***************************************
2025-07-07 09:39:53,407 p=95771 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:39:53,422 p=95771 u=gpadmin n=ansible | TASK [monitoring : Start Prometheus container] ********************************************
2025-07-07 09:39:53,941 p=95771 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:39:53,952 p=95771 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Grafana container (idempotency)] **************
2025-07-07 09:39:54,440 p=95771 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:39:54,440 p=95771 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:39:54,455 p=95771 u=gpadmin n=ansible | TASK [monitoring : Create Grafana provisioning directories] *******************************
2025-07-07 09:39:54,749 p=95771 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/datasources)
2025-07-07 09:39:55,013 p=95771 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/dashboards)
2025-07-07 09:39:55,302 p=95771 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/dashboards)
2025-07-07 09:39:55,317 p=95771 u=gpadmin n=ansible | TASK [monitoring : Create Grafana datasource configuration] *******************************
2025-07-07 09:39:55,877 p=95771 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:39:55,892 p=95771 u=gpadmin n=ansible | TASK [monitoring : Create Grafana dashboard provider configuration] ***********************
2025-07-07 09:39:56,423 p=95771 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:39:56,436 p=95771 u=gpadmin n=ansible | TASK [monitoring : Copy Grafana dashboards] ***********************************************
2025-07-07 09:39:56,511 p=95771 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'instance' is undefined. 'instance' is undefined
2025-07-07 09:39:56,512 p=95771 u=gpadmin n=ansible | failed: [MASTER] (item=node-exporter-dashboard.json) => {"ansible_loop_var": "item", "changed": false, "item": "node-exporter-dashboard.json", "msg": "AnsibleUndefinedVariable: 'instance' is undefined. 'instance' is undefined"}
2025-07-07 09:39:56,556 p=95771 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'name' is undefined. 'name' is undefined
2025-07-07 09:39:56,556 p=95771 u=gpadmin n=ansible | failed: [MASTER] (item=docker-container-dashboard.json) => {"ansible_loop_var": "item", "changed": false, "item": "docker-container-dashboard.json", "msg": "AnsibleUndefinedVariable: 'name' is undefined. 'name' is undefined"}
2025-07-07 09:39:56,600 p=95771 u=gpadmin n=ansible | An exception occurred during task execution. To see the full traceback, use -vvv. The error was: ansible.errors.AnsibleUndefinedVariable: 'name' is undefined. 'name' is undefined
2025-07-07 09:39:56,601 p=95771 u=gpadmin n=ansible | failed: [MASTER] (item=ray-cluster-dashboard.json) => {"ansible_loop_var": "item", "changed": false, "item": "ray-cluster-dashboard.json", "msg": "AnsibleUndefinedVariable: 'name' is undefined. 'name' is undefined"}
2025-07-07 09:39:56,604 p=95771 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 09:39:56,604 p=95771 u=gpadmin n=ansible | MASTER                     : ok=17   changed=7    unreachable=0    failed=1    skipped=0    rescued=0    ignored=5   
2025-07-07 09:41:43,233 p=97667 u=gpadmin n=ansible | PLAY [Fix Dashboard Templates] ************************************************************
2025-07-07 09:41:43,241 p=97667 u=gpadmin n=ansible | TASK [Fix Prometheus template variables in dashboards] ************************************
2025-07-07 09:41:43,260 p=97667 u=gpadmin n=ansible | fatal: [localhost]: FAILED! => {"msg": "template error while templating string: unexpected char '^' at 4. String: {{([^}]+)}}. unexpected char '^' at 4"}
2025-07-07 09:41:43,260 p=97667 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 09:41:43,261 p=97667 u=gpadmin n=ansible | localhost                  : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-07-07 09:45:08,504 p=99441 u=gpadmin n=ansible | PLAY [Fix Monitoring Permissions] *********************************************************
2025-07-07 09:45:08,511 p=99441 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:45:10,003 p=99441 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:45:10,035 p=99441 u=gpadmin n=ansible | TASK [Stop all monitoring containers] *****************************************************
2025-07-07 09:45:10,772 p=99441 u=gpadmin n=ansible | failed: [MASTER] (item=prometheus) => {"ansible_loop_var": "item", "changed": false, "item": "prometheus", "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:45:11,223 p=99441 u=gpadmin n=ansible | failed: [MASTER] (item=grafana) => {"ansible_loop_var": "item", "changed": false, "item": "grafana", "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 09:45:11,224 p=99441 u=gpadmin n=ansible | ...ignoring
2025-07-07 09:45:11,237 p=99441 u=gpadmin n=ansible | TASK [Fix Prometheus data directory permissions] ******************************************
2025-07-07 09:45:11,649 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:11,662 p=99441 u=gpadmin n=ansible | TASK [Fix Grafana data directory permissions] *********************************************
2025-07-07 09:45:11,964 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:11,977 p=99441 u=gpadmin n=ansible | TASK [Restart monitoring containers] ******************************************************
2025-07-07 09:45:12,038 p=99441 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus data directory] **************************************
2025-07-07 09:45:12,316 p=99441 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:45:12,329 p=99441 u=gpadmin n=ansible | TASK [monitoring : Create Grafana data directory] *****************************************
2025-07-07 09:45:12,633 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:12,646 p=99441 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Node Exporter container (idempotency)] ********
2025-07-07 09:45:13,215 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:13,229 p=99441 u=gpadmin n=ansible | TASK [monitoring : Start Node Exporter container] *****************************************
2025-07-07 09:45:13,897 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:13,910 p=99441 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing cAdvisor container (idempotency)] *************
2025-07-07 09:45:14,499 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:14,512 p=99441 u=gpadmin n=ansible | TASK [monitoring : Start cAdvisor container] **********************************************
2025-07-07 09:45:15,201 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:15,213 p=99441 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Prometheus container (idempotency)] ***********
2025-07-07 09:45:15,805 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:15,819 p=99441 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus configuration] ***************************************
2025-07-07 09:45:16,475 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:16,488 p=99441 u=gpadmin n=ansible | TASK [monitoring : Start Prometheus container] ********************************************
2025-07-07 09:45:17,178 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:17,192 p=99441 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Grafana container (idempotency)] **************
2025-07-07 09:45:17,715 p=99441 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:45:17,729 p=99441 u=gpadmin n=ansible | TASK [monitoring : Create Grafana provisioning directories] *******************************
2025-07-07 09:45:18,042 p=99441 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/datasources)
2025-07-07 09:45:18,321 p=99441 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/dashboards)
2025-07-07 09:45:18,614 p=99441 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/dashboards)
2025-07-07 09:45:18,629 p=99441 u=gpadmin n=ansible | TASK [monitoring : Create Grafana datasource configuration] *******************************
2025-07-07 09:45:19,237 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:19,251 p=99441 u=gpadmin n=ansible | TASK [monitoring : Create Grafana dashboard provider configuration] ***********************
2025-07-07 09:45:19,832 p=99441 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:45:19,845 p=99441 u=gpadmin n=ansible | TASK [monitoring : Copy Grafana dashboards] ***********************************************
2025-07-07 09:45:20,532 p=99441 u=gpadmin n=ansible | changed: [MASTER] => (item=node-exporter-dashboard.json)
2025-07-07 09:45:21,038 p=99441 u=gpadmin n=ansible | changed: [MASTER] => (item=docker-container-dashboard.json)
2025-07-07 09:45:21,566 p=99441 u=gpadmin n=ansible | changed: [MASTER] => (item=ray-cluster-dashboard.json)
2025-07-07 09:45:21,581 p=99441 u=gpadmin n=ansible | TASK [monitoring : Start Grafana container] ***********************************************
2025-07-07 09:45:22,079 p=99441 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Non-string value found for env option. Ambiguous env options must be wrapped in quotes to avoid them being interpreted. Key: GF_SERVER_HTTP_PORT"}
2025-07-07 09:45:22,081 p=99441 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 09:45:22,081 p=99441 u=gpadmin n=ansible | MASTER                     : ok=18   changed=14   unreachable=0    failed=1    skipped=0    rescued=0    ignored=1   
2025-07-07 09:45:52,318 p=101173 u=gpadmin n=ansible | MASTER | FAILED | rc=1 >>
Error response from daemon: No such container: grafananon-zero return code

2025-07-07 09:46:08,327 p=101289 u=gpadmin n=ansible | MASTER | CHANGED | rc=0 >>
d8cf16192cbac3fe1e91550662566cb4c775b33ee5e3ec85fd6185f82ae663a5Unable to find image 'grafana/grafana:latest' locally
latest: Pulling from grafana/grafana
f18232174bc9: Pulling fs layer
9183b65e90ee: Pulling fs layer
3f8d5c908dcc: Pulling fs layer
30bb92ff0608: Pulling fs layer
807a2e881ecd: Pulling fs layer
4a4d0948b0bf: Pulling fs layer
04f6155c873d: Pulling fs layer
85dde7dceb0a: Pulling fs layer
7009d5001b77: Pulling fs layer
538deb30e80c: Pulling fs layer
4a4d0948b0bf: Waiting
04f6155c873d: Waiting
85dde7dceb0a: Waiting
538deb30e80c: Waiting
30bb92ff0608: Waiting
7009d5001b77: Waiting
807a2e881ecd: Waiting
9183b65e90ee: Verifying Checksum
9183b65e90ee: Download complete
3f8d5c908dcc: Download complete
f18232174bc9: Verifying Checksum
f18232174bc9: Download complete
f18232174bc9: Pull complete
9183b65e90ee: Pull complete
807a2e881ecd: Download complete
4a4d0948b0bf: Download complete
30bb92ff0608: Verifying Checksum
30bb92ff0608: Download complete
3f8d5c908dcc: Pull complete
7009d5001b77: Verifying Checksum
7009d5001b77: Download complete
30bb92ff0608: Pull complete
807a2e881ecd: Pull complete
4a4d0948b0bf: Pull complete
538deb30e80c: Verifying Checksum
538deb30e80c: Download complete
85dde7dceb0a: Verifying Checksum
85dde7dceb0a: Download complete
04f6155c873d: Verifying Checksum
04f6155c873d: Download complete
04f6155c873d: Pull complete
85dde7dceb0a: Pull complete
7009d5001b77: Pull complete
538deb30e80c: Pull complete
Digest: sha256:b5b59bfc7561634c2d7b136c4543d702ebcc94a3da477f21ff26f89ffd4214fa
Status: Downloaded newer image for grafana/grafana:latest

2025-07-07 09:46:13,762 p=101700 u=gpadmin n=ansible | MASTER | FAILED | rc=-1 >>
template error while templating string: unexpected '.'. String: docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'. unexpected '.'

2025-07-07 09:46:13,768 p=101700 u=gpadmin n=ansible | G-241 | FAILED | rc=-1 >>
template error while templating string: unexpected '.'. String: docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'. unexpected '.'

2025-07-07 09:46:13,768 p=101700 u=gpadmin n=ansible | G-242 | FAILED | rc=-1 >>
template error while templating string: unexpected '.'. String: docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'. unexpected '.'

2025-07-07 09:46:13,771 p=101700 u=gpadmin n=ansible | G-243 | FAILED | rc=-1 >>
template error while templating string: unexpected '.'. String: docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'. unexpected '.'

2025-07-07 09:46:13,775 p=101700 u=gpadmin n=ansible | G-244 | FAILED | rc=-1 >>
template error while templating string: unexpected '.'. String: docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Ports}}'. unexpected '.'

2025-07-07 09:46:20,408 p=101796 u=gpadmin n=ansible | MASTER | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                              COMMAND                  CREATED              STATUS                                 PORTS     NAMES
d8cf16192cba   grafana/grafana:latest             "/run.sh"                13 seconds ago       Up 12 seconds                                    grafana
ccaf50521155   prom/prometheus:v2.45.0            "/bin/prometheus --c…"   About a minute ago   Up About a minute                                prometheus
15250f98d68a   gcr.io/cadvisor/cadvisor:v0.47.2   "/usr/bin/cadvisor -…"   About a minute ago   Up About a minute (health: starting)             cadvisor
b467d7b01dd5   prom/node-exporter:v1.6.1          "/bin/node_exporter …"   About a minute ago   Up About a minute                                node-exporter
a30361101b64   rayproject/ray:2.9.0               "/home/gpadmin/ray_t…"   11 minutes ago       Up 11 minutes                                    ray_head

2025-07-07 09:46:20,890 p=101796 u=gpadmin n=ansible | G-244 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                       COMMAND                  CREATED          STATUS          PORTS     NAMES
54fb99bb27ad   prom/node-exporter:v1.6.1   "/bin/node_exporter …"   10 minutes ago   Up 10 minutes             node-exporter
56ce0f34b799   rayproject/ray:2.9.0        "/home/gpadmin/ray_t…"   11 minutes ago   Up 11 minutes             ray_worker

2025-07-07 09:46:20,905 p=101796 u=gpadmin n=ansible | G-242 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                       COMMAND                  CREATED          STATUS          PORTS     NAMES
c04598c908e5   prom/node-exporter:v1.6.1   "/bin/node_exporter …"   10 minutes ago   Up 10 minutes             node-exporter
7fbaa9e64aad   rayproject/ray:2.9.0        "/home/gpadmin/ray_t…"   11 minutes ago   Up 11 minutes             ray_worker

2025-07-07 09:46:20,915 p=101796 u=gpadmin n=ansible | G-241 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                       COMMAND                  CREATED          STATUS          PORTS     NAMES
9b79793063ea   prom/node-exporter:v1.6.1   "/bin/node_exporter …"   10 minutes ago   Up 10 minutes             node-exporter
ae21478cc53a   rayproject/ray:2.9.0        "/home/gpadmin/ray_t…"   11 minutes ago   Up 11 minutes             ray_worker

2025-07-07 09:46:20,925 p=101796 u=gpadmin n=ansible | G-243 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                       COMMAND                  CREATED          STATUS          PORTS     NAMES
35590ea5ea68   prom/node-exporter:v1.6.1   "/bin/node_exporter …"   10 minutes ago   Up 10 minutes             node-exporter
28d337ef57bd   rayproject/ray:2.9.0        "/home/gpadmin/ray_t…"   11 minutes ago   Up 11 minutes             ray_worker

2025-07-07 09:46:28,698 p=101992 u=gpadmin n=ansible | MASTER | FAILED | rc=1 >>
Traceback (most recent call last):
  File "/home/ray/anaconda3/bin/ray", line 8, in <module>
    sys.exit(main())
  File "/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py", line 2498, in main
    return cli()
  File "/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/ray/anaconda3/lib/python3.8/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/home/ray/anaconda3/lib/python3.8/site-packages/ray/scripts/scripts.py", line 1973, in status
    address = services.canonicalize_bootstrap_address_or_die(address)
  File "/home/ray/anaconda3/lib/python3.8/site-packages/ray/_private/services.py", line 566, in canonicalize_bootstrap_address_or_die
    raise ConnectionError(
ConnectionError: Could not find any running Ray instance. Please specify the one to connect to by setting the `--address` flag or `RAY_ADDRESS` environment variable.non-zero return code

2025-07-07 09:46:36,206 p=102206 u=gpadmin n=ansible | MASTER | FAILED | rc=1 >>
Ray cluster is not found at 192.168.40.240:6379Traceback (most recent call last):
  File "python/ray/_raylet.pyx", line 3168, in ray._raylet.check_health
  File "python/ray/_raylet.pyx", line 580, in ray._raylet.check_status
ray.exceptions.RpcError: failed to connect to all addresses; last error: UNKNOWN: ipv4:192.168.40.240:6379: Failed to connect to remote host: Connection refusednon-zero return code

2025-07-07 09:46:42,830 p=102440 u=gpadmin n=ansible | MASTER | CHANGED | rc=0 >>
Usage: ray start [OPTIONS]
Try 'ray start --help' for help.

Error: Invalid value for '--memory': '102945MiB' is not a valid integer.

2025-07-07 09:47:07,854 p=102726 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 09:47:07,865 p=102726 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:47:09,338 p=102726 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:47:09,408 p=102726 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] ************************************************************
2025-07-07 09:47:09,418 p=102726 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:47:10,612 p=102726 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:47:10,676 p=102726 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] *********************************************************
2025-07-07 09:47:10,676 p=102726 u=gpadmin n=ansible | skipping: no hosts matched
2025-07-07 09:47:10,680 p=102726 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] ******************************************************
2025-07-07 09:47:10,717 p=102726 u=gpadmin n=ansible | PLAY [Deploy Monitoring Stack] ************************************************************
2025-07-07 09:47:10,726 p=102726 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:47:11,955 p=102726 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:47:12,019 p=102726 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 09:47:12,019 p=102726 u=gpadmin n=ansible | MASTER                     : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 09:47:17,466 p=103303 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **************************************************
2025-07-07 09:47:17,477 p=103303 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:47:18,861 p=103303 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:47:18,932 p=103303 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] ************************************************************
2025-07-07 09:47:18,942 p=103303 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:47:20,140 p=103303 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:47:20,206 p=103303 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] *********************************************************
2025-07-07 09:47:20,206 p=103303 u=gpadmin n=ansible | skipping: no hosts matched
2025-07-07 09:47:20,210 p=103303 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] ******************************************************
2025-07-07 09:47:20,248 p=103303 u=gpadmin n=ansible | PLAY [Deploy Monitoring Stack] ************************************************************
2025-07-07 09:47:20,257 p=103303 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:47:21,433 p=103303 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:47:21,495 p=103303 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 09:47:21,495 p=103303 u=gpadmin n=ansible | MASTER                     : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 09:47:40,653 p=103944 u=gpadmin n=ansible | PLAY [Restart Ray Cluster] ****************************************************************
2025-07-07 09:47:40,677 p=103944 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 09:47:42,214 p=103944 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:47:42,258 p=103944 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:47:42,388 p=103944 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:47:42,418 p=103944 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:47:42,425 p=103944 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:47:42,505 p=103944 u=gpadmin n=ansible | TASK [Stop Ray containers] ****************************************************************
2025-07-07 09:47:43,061 p=103944 u=gpadmin n=ansible | ok: [G-242] => (item=ray_head)
2025-07-07 09:47:43,071 p=103944 u=gpadmin n=ansible | ok: [G-243] => (item=ray_head)
2025-07-07 09:47:43,072 p=103944 u=gpadmin n=ansible | ok: [G-244] => (item=ray_head)
2025-07-07 09:47:43,073 p=103944 u=gpadmin n=ansible | ok: [G-241] => (item=ray_head)
2025-07-07 09:47:53,351 p=103944 u=gpadmin n=ansible | changed: [MASTER] => (item=ray_head)
2025-07-07 09:47:53,446 p=103944 u=gpadmin n=ansible | changed: [G-243] => (item=ray_worker)
2025-07-07 09:47:53,454 p=103944 u=gpadmin n=ansible | changed: [G-242] => (item=ray_worker)
2025-07-07 09:47:53,472 p=103944 u=gpadmin n=ansible | changed: [G-244] => (item=ray_worker)
2025-07-07 09:47:53,480 p=103944 u=gpadmin n=ansible | changed: [G-241] => (item=ray_worker)
2025-07-07 09:47:53,848 p=103944 u=gpadmin n=ansible | ok: [MASTER] => (item=ray_worker)
2025-07-07 09:47:53,862 p=103944 u=gpadmin n=ansible | TASK [Include ray_head role] **************************************************************
2025-07-07 09:47:53,908 p=103944 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 09:47:53,940 p=103944 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 09:47:53,941 p=103944 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 09:47:53,953 p=103944 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 09:47:53,982 p=103944 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] **********************
2025-07-07 09:47:54,389 p=103944 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:47:54,402 p=103944 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ***************
2025-07-07 09:47:54,890 p=103944 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:47:54,903 p=103944 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] **************************
2025-07-07 09:47:55,196 p=103944 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:47:55,210 p=103944 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] *********************************
2025-07-07 09:47:55,934 p=103944 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:47:55,951 p=103944 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] ************************************************
2025-07-07 09:47:56,608 p=103944 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 09:47:56,621 p=103944 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ***************************************
2025-07-07 09:47:56,952 p=103944 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:47:56,965 p=103944 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] *****************************************
2025-07-07 09:47:56,998 p=103944 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n82b0e5d0e68c   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_head"
}
2025-07-07 09:47:57,016 p=103944 u=gpadmin n=ansible | TASK [ray_head : Wait for a moment to let Ray head node initialize] ***********************
2025-07-07 09:47:57,036 p=103944 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 09:47:57,037 p=103944 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 09:48:07,041 p=103944 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:48:07,055 p=103944 u=gpadmin n=ansible | TASK [ray_head : Check Ray head node status] **********************************************
2025-07-07 09:48:09,257 p=103944 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 09:48:09,269 p=103944 u=gpadmin n=ansible | TASK [ray_head : Print Ray head node status] **********************************************
2025-07-07 09:48:09,320 p=103944 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 06:48:04.483340 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_ba06a623ab31fc102bc772dba3216bec4e899553327947f90d90158c",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Usage:",
        " 0.0/32.0 CPU",
        " 0B/100.53KiB memory",
        " 0B/9.31GiB object_store_memory",
        "",
        "Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 09:48:09,350 p=103944 u=gpadmin n=ansible | TASK [Include ray_worker role] ************************************************************
2025-07-07 09:48:09,386 p=103944 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 09:48:09,474 p=103944 u=gpadmin n=ansible | TASK [ray_worker : Ensure Ray temporary directory exists on worker node] ******************
2025-07-07 09:48:09,634 p=103944 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:48:09,649 p=103944 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:48:09,671 p=103944 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:48:09,671 p=103944 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:48:09,730 p=103944 u=gpadmin n=ansible | TASK [ray_worker : Stop and remove existing Ray worker container (idempotency)] ***********
2025-07-07 09:48:10,016 p=103944 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:48:10,038 p=103944 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:48:10,049 p=103944 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:48:10,060 p=103944 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:48:10,081 p=103944 u=gpadmin n=ansible | TASK [ray_worker : Remove old Ray worker start script (idempotency)] **********************
2025-07-07 09:48:10,270 p=103944 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:48:10,272 p=103944 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:48:10,278 p=103944 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:48:10,287 p=103944 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:48:10,308 p=103944 u=gpadmin n=ansible | TASK [ray_worker : Copy Ray worker start script to worker node] ***************************
2025-07-07 09:48:10,777 p=103944 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:48:10,790 p=103944 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:48:10,816 p=103944 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:48:10,819 p=103944 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:48:10,840 p=103944 u=gpadmin n=ansible | TASK [ray_worker : Start Ray worker container] ********************************************
2025-07-07 09:48:11,349 p=103944 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 09:48:11,357 p=103944 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 09:48:11,362 p=103944 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 09:48:11,410 p=103944 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 09:48:11,432 p=103944 u=gpadmin n=ansible | TASK [ray_worker : Display Ray worker container status] ***********************************
2025-07-07 09:48:11,622 p=103944 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:48:11,636 p=103944 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:48:11,652 p=103944 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:48:11,661 p=103944 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:48:11,684 p=103944 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker container status] *************************************
2025-07-07 09:48:11,730 p=103944 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n12edbae5ce9a   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:48:11,744 p=103944 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n6573f9d02cf4   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:48:11,746 p=103944 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n222535faf4c7   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:48:11,757 p=103944 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED                  STATUS                  PORTS     NAMES\naabf8684aa8a   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 09:48:11,771 p=103944 u=gpadmin n=ansible | TASK [ray_worker : Wait for a moment to let Ray worker node initialize] *******************
2025-07-07 09:48:11,791 p=103944 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 09:48:11,792 p=103944 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 09:48:21,797 p=103944 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:48:21,818 p=103944 u=gpadmin n=ansible | TASK [ray_worker : Check Ray worker node status] ******************************************
2025-07-07 09:48:22,894 p=103944 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 09:48:22,916 p=103944 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 09:48:22,935 p=103944 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 09:48:22,943 p=103944 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 09:48:22,956 p=103944 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker node status] ******************************************
2025-07-07 09:48:23,007 p=103944 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 06:48:19.517043 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_9d19e0e61343fc414d34246fec58a21bb22008cd5182dc658b029988",
        " 1 node_ba06a623ab31fc102bc772dba3216bec4e899553327947f90d90158c",
        " 1 node_c8994f0c41dbdb3c6bbaf320ecd33477a65570e55d26251efe122b6b",
        " 1 node_1e0cab87130f010639bb1a4dd812c382477b80e09104400e0ba9bb0f",
        " 1 node_d888f7b4d4836e6d89c49695ea8d48220091308680732d7e24977026",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 09:48:23,010 p=103944 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 06:48:19.517043 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_9d19e0e61343fc414d34246fec58a21bb22008cd5182dc658b029988",
        " 1 node_ba06a623ab31fc102bc772dba3216bec4e899553327947f90d90158c",
        " 1 node_c8994f0c41dbdb3c6bbaf320ecd33477a65570e55d26251efe122b6b",
        " 1 node_1e0cab87130f010639bb1a4dd812c382477b80e09104400e0ba9bb0f",
        " 1 node_d888f7b4d4836e6d89c49695ea8d48220091308680732d7e24977026",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 09:48:23,031 p=103944 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 06:48:19.517043 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_9d19e0e61343fc414d34246fec58a21bb22008cd5182dc658b029988",
        " 1 node_ba06a623ab31fc102bc772dba3216bec4e899553327947f90d90158c",
        " 1 node_c8994f0c41dbdb3c6bbaf320ecd33477a65570e55d26251efe122b6b",
        " 1 node_1e0cab87130f010639bb1a4dd812c382477b80e09104400e0ba9bb0f",
        " 1 node_d888f7b4d4836e6d89c49695ea8d48220091308680732d7e24977026",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 09:48:23,038 p=103944 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 06:48:19.517043 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_9d19e0e61343fc414d34246fec58a21bb22008cd5182dc658b029988",
        " 1 node_ba06a623ab31fc102bc772dba3216bec4e899553327947f90d90158c",
        " 1 node_c8994f0c41dbdb3c6bbaf320ecd33477a65570e55d26251efe122b6b",
        " 1 node_1e0cab87130f010639bb1a4dd812c382477b80e09104400e0ba9bb0f",
        " 1 node_d888f7b4d4836e6d89c49695ea8d48220091308680732d7e24977026",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 09:48:23,161 p=103944 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 09:48:23,161 p=103944 u=gpadmin n=ansible | G-241                      : ok=12   changed=4    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
2025-07-07 09:48:23,161 p=103944 u=gpadmin n=ansible | G-242                      : ok=11   changed=4    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
2025-07-07 09:48:23,161 p=103944 u=gpadmin n=ansible | G-243                      : ok=11   changed=4    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
2025-07-07 09:48:23,161 p=103944 u=gpadmin n=ansible | G-244                      : ok=11   changed=4    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
2025-07-07 09:48:23,161 p=103944 u=gpadmin n=ansible | MASTER                     : ok=12   changed=4    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
2025-07-07 10:02:47,082 p=113107 u=gpadmin n=ansible | PLAY [Checkpoint 1: Prerequisites and System Readiness Check] *****************************
2025-07-07 10:02:47,090 p=113107 u=gpadmin n=ansible | TASK [Gathering Facts] ********************************************************************
2025-07-07 10:02:48,502 p=113107 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:02:48,608 p=113107 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:02:48,615 p=113107 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:02:48,621 p=113107 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:02:48,714 p=113107 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:02:48,785 p=113107 u=gpadmin n=ansible | TASK [Display checkpoint information] *****************************************************
2025-07-07 10:02:48,815 p=113107 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "=== CHECKPOINT 1: PREREQUISITES CHECK ===",
        "Approval Required: False",
        "Destructive Operation: False",
        "Description: Validate system requirements and network connectivity"
    ]
}
2025-07-07 10:02:48,829 p=113107 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "=== CHECKPOINT 1: PREREQUISITES CHECK ===",
        "Approval Required: False",
        "Destructive Operation: False",
        "Description: Validate system requirements and network connectivity"
    ]
}
2025-07-07 10:02:48,843 p=113107 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "=== CHECKPOINT 1: PREREQUISITES CHECK ===",
        "Approval Required: False",
        "Destructive Operation: False",
        "Description: Validate system requirements and network connectivity"
    ]
}
2025-07-07 10:02:48,844 p=113107 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "=== CHECKPOINT 1: PREREQUISITES CHECK ===",
        "Approval Required: False",
        "Destructive Operation: False",
        "Description: Validate system requirements and network connectivity"
    ]
}
2025-07-07 10:02:48,854 p=113107 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "=== CHECKPOINT 1: PREREQUISITES CHECK ===",
        "Approval Required: False",
        "Destructive Operation: False",
        "Description: Validate system requirements and network connectivity"
    ]
}
2025-07-07 10:02:48,862 p=113107 u=gpadmin n=ansible | TASK [Check Python installation] **********************************************************
2025-07-07 10:02:49,121 p=113107 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:02:49,127 p=113107 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:02:49,134 p=113107 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:02:49,134 p=113107 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:02:49,220 p=113107 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:02:49,232 p=113107 u=gpadmin n=ansible | TASK [Check SSH connectivity] *************************************************************
2025-07-07 10:02:49,498 p=113107 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:02:49,501 p=113107 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:02:49,502 p=113107 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:02:49,509 p=113107 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:02:49,585 p=113107 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:02:49,597 p=113107 u=gpadmin n=ansible | TASK [Check available disk space] *********************************************************
2025-07-07 10:02:49,765 p=113107 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:02:49,802 p=113107 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:02:49,803 p=113107 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:02:49,813 p=113107 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:02:49,880 p=113107 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:02:49,892 p=113107 u=gpadmin n=ansible | TASK [Check available memory] *************************************************************
2025-07-07 10:02:50,063 p=113107 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:02:50,083 p=113107 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:02:50,086 p=113107 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:02:50,110 p=113107 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:02:50,167 p=113107 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:02:50,175 p=113107 u=gpadmin n=ansible | TASK [Check CPU cores] ********************************************************************
2025-07-07 10:02:50,339 p=113107 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:02:50,373 p=113107 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:02:50,376 p=113107 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:02:50,382 p=113107 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:02:50,461 p=113107 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:02:50,473 p=113107 u=gpadmin n=ansible | TASK [Check network connectivity to head node] ********************************************
2025-07-07 10:02:50,497 p=113107 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:03:00,762 p=113107 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "elapsed": 10, "msg": "Timeout when waiting for 192.168.40.240:22"}
2025-07-07 10:03:00,764 p=113107 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "elapsed": 10, "msg": "Timeout when waiting for 192.168.40.240:22"}
2025-07-07 10:03:00,775 p=113107 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "elapsed": 10, "msg": "Timeout when waiting for 192.168.40.240:22"}
2025-07-07 10:03:00,780 p=113107 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "elapsed": 10, "msg": "Timeout when waiting for 192.168.40.240:22"}
2025-07-07 10:03:00,797 p=113107 u=gpadmin n=ansible | TASK [Compile system information] *********************************************************
2025-07-07 10:03:00,825 p=113107 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:03:00,837 p=113107 u=gpadmin n=ansible | TASK [Display system readiness summary] ***************************************************
2025-07-07 10:03:00,859 p=113107 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Node: G-K3S-Master (192.168.40.240)",
        "OS: Debian 24.04",
        "Python: Python 3.13.5",
        "Memory: 125Gi",
        "Free Disk: 820G",
        "CPU Cores: 32",
        "SSH Connectivity: True"
    ]
}
2025-07-07 10:03:00,871 p=113107 u=gpadmin n=ansible | TASK [Save checkpoint results] ************************************************************
2025-07-07 10:03:01,548 p=113107 u=gpadmin n=ansible | changed: [MASTER -> localhost]
2025-07-07 10:03:01,590 p=113107 u=gpadmin n=ansible | PLAY [Generate Prerequisites Check Report] ************************************************
2025-07-07 10:03:01,596 p=113107 u=gpadmin n=ansible | TASK [Compile overall status] *************************************************************
2025-07-07 10:03:01,616 p=113107 u=gpadmin n=ansible | ok: [localhost] => {
    "msg": [
        "=== CHECKPOINT 1 COMPLETED ===",
        "All systems validated successfully",
        "Ready to proceed to Checkpoint 2: Docker Installation"
    ]
}
2025-07-07 10:03:01,619 p=113107 u=gpadmin n=ansible | PLAY RECAP ********************************************************************************
2025-07-07 10:03:01,619 p=113107 u=gpadmin n=ansible | G-241                      : ok=7    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-07-07 10:03:01,619 p=113107 u=gpadmin n=ansible | G-242                      : ok=7    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-07-07 10:03:01,619 p=113107 u=gpadmin n=ansible | G-243                      : ok=7    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-07-07 10:03:01,619 p=113107 u=gpadmin n=ansible | G-244                      : ok=7    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-07-07 10:03:01,620 p=113107 u=gpadmin n=ansible | MASTER                     : ok=10   changed=1    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   
2025-07-07 10:03:01,620 p=113107 u=gpadmin n=ansible | localhost                  : ok=1    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 10:23:29,020 p=125136 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] ******************************************************
2025-07-07 10:23:29,030 p=125136 u=gpadmin n=ansible | TASK [Gathering Facts] ************************************************************************
2025-07-07 10:23:30,539 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:23:30,625 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:23:30,714 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:23:30,733 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:23:30,742 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:23:30,816 p=125136 u=gpadmin n=ansible | TASK [common : Update apt cache] **************************************************************
2025-07-07 10:23:33,020 p=125136 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:23:33,126 p=125136 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:23:33,132 p=125136 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:23:33,183 p=125136 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:23:35,986 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:23:35,998 p=125136 u=gpadmin n=ansible | TASK [common : Install common packages] *******************************************************
2025-07-07 10:23:36,453 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:23:36,497 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:23:36,500 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:23:36,505 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:23:37,070 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:23:37,083 p=125136 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] ************************************************
2025-07-07 10:23:37,360 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:23:37,362 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:23:37,368 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:23:37,378 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:23:37,472 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:23:37,485 p=125136 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] *************************************************
2025-07-07 10:23:37,769 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:23:37,770 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:23:37,773 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:23:37,787 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:23:37,886 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:23:37,942 p=125136 u=gpadmin n=ansible | TASK [docker : Check if Docker is available via which command] ********************************
2025-07-07 10:23:38,208 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:23:38,222 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:23:38,222 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:23:38,230 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:23:38,325 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:23:38,338 p=125136 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via apt] ******************************************
2025-07-07 10:23:38,535 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:23:38,562 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:23:38,565 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:23:38,568 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:23:38,659 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:23:38,672 p=125136 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via snap] *****************************************
2025-07-07 10:23:38,881 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:23:38,896 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:23:38,916 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:23:38,919 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:23:39,024 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:23:39,037 p=125136 u=gpadmin n=ansible | TASK [docker : Set Docker installation status facts] ******************************************
2025-07-07 10:23:39,085 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:23:39,099 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:23:39,114 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:23:39,117 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:23:39,134 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:23:39,143 p=125136 u=gpadmin n=ansible | TASK [docker : Display Docker installation status] ********************************************
2025-07-07 10:23:39,176 p=125136 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: True",
        "Docker installed via snap: False"
    ]
}
2025-07-07 10:23:39,195 p=125136 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:23:39,212 p=125136 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:23:39,213 p=125136 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:23:39,230 p=125136 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:23:39,239 p=125136 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] ************************************************************
2025-07-07 10:23:39,258 p=125136 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:23:39,286 p=125136 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:23:39,300 p=125136 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:23:39,302 p=125136 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:23:39,308 p=125136 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:23:39,317 p=125136 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] *****************************************************
2025-07-07 10:23:39,354 p=125136 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:23:39,370 p=125136 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:23:39,384 p=125136 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:23:39,386 p=125136 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:23:39,397 p=125136 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:23:39,408 p=125136 u=gpadmin n=ansible | TASK [docker : Update apt cache after adding Docker repository] *******************************
2025-07-07 10:23:39,429 p=125136 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:23:39,459 p=125136 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:23:39,473 p=125136 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:23:39,474 p=125136 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:23:39,481 p=125136 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:23:39,490 p=125136 u=gpadmin n=ansible | TASK [docker : Install Docker Engine via APT] *************************************************
2025-07-07 10:23:39,525 p=125136 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:23:39,541 p=125136 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:23:39,555 p=125136 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:23:39,557 p=125136 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:23:39,564 p=125136 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:23:39,573 p=125136 u=gpadmin n=ansible | TASK [docker : Check Docker service status (systemd)] *****************************************
2025-07-07 10:23:39,755 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:23:39,762 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:23:39,815 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:23:39,844 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:23:39,872 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:23:39,887 p=125136 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] ************************************************
2025-07-07 10:23:39,947 p=125136 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:23:39,963 p=125136 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:23:39,966 p=125136 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:23:39,977 p=125136 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:23:40,540 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:23:40,553 p=125136 u=gpadmin n=ansible | TASK [docker : Install Docker via snap if not installed via apt] ******************************
2025-07-07 10:23:40,582 p=125136 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:23:40,601 p=125136 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:23:40,620 p=125136 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:23:40,636 p=125136 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:23:40,645 p=125136 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:23:40,655 p=125136 u=gpadmin n=ansible | TASK [docker : Wait for snap Docker installation to complete] *********************************
2025-07-07 10:23:40,682 p=125136 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:23:40,691 p=125136 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] **************************
2025-07-07 10:23:40,724 p=125136 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:23:40,887 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:23:40,899 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:23:40,923 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:23:40,934 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:23:40,948 p=125136 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ***************************************************
2025-07-07 10:23:40,994 p=125136 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:23:41,292 p=125136 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:24:14,392 p=125136 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:24:14,419 p=125136 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:24:14,468 p=125136 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:24:14,482 p=125136 u=gpadmin n=ansible | TASK [docker : Add user to docker group (for apt installation)] *******************************
2025-07-07 10:24:14,541 p=125136 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:24:14,557 p=125136 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:24:14,561 p=125136 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:24:14,577 p=125136 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:24:15,041 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:24:15,055 p=125136 u=gpadmin n=ansible | TASK [docker : Wait for Docker to be ready] ***************************************************
2025-07-07 10:24:15,296 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:24:15,328 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:24:15,334 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:24:15,353 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:24:15,488 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:24:15,506 p=125136 u=gpadmin n=ansible | TASK [docker : Display Docker version] ********************************************************
2025-07-07 10:24:15,724 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:24:15,755 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:24:15,762 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:24:15,774 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:24:15,818 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:24:15,832 p=125136 u=gpadmin n=ansible | TASK [docker : Show Docker version] ***********************************************************
2025-07-07 10:24:15,861 p=125136 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Docker version 28.3.1, build 38b7060"
}
2025-07-07 10:24:15,894 p=125136 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:24:15,910 p=125136 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:24:15,912 p=125136 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:24:15,922 p=125136 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:24:15,932 p=125136 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] *********************************************************
2025-07-07 10:24:16,868 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:24:16,869 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:24:16,906 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:24:16,908 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:24:16,997 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:24:17,010 p=125136 u=gpadmin n=ansible | TASK [docker : Pull monitoring Docker images] *************************************************
2025-07-07 10:24:17,897 p=125136 u=gpadmin n=ansible | ok: [MASTER] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:24:18,904 p=125136 u=gpadmin n=ansible | ok: [MASTER] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:24:21,024 p=125136 u=gpadmin n=ansible | ok: [MASTER] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:24:21,708 p=125136 u=gpadmin n=ansible | changed: [G-243] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:24:22,237 p=125136 u=gpadmin n=ansible | changed: [G-241] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:24:22,411 p=125136 u=gpadmin n=ansible | ok: [G-243] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:24:22,941 p=125136 u=gpadmin n=ansible | ok: [G-241] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:24:23,261 p=125136 u=gpadmin n=ansible | ok: [G-243] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:24:23,364 p=125136 u=gpadmin n=ansible | changed: [G-242] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:24:23,826 p=125136 u=gpadmin n=ansible | changed: [G-244] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:24:23,829 p=125136 u=gpadmin n=ansible | ok: [G-241] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:24:24,045 p=125136 u=gpadmin n=ansible | ok: [G-242] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:24:24,504 p=125136 u=gpadmin n=ansible | ok: [G-244] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:24:24,974 p=125136 u=gpadmin n=ansible | ok: [G-242] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:24:25,404 p=125136 u=gpadmin n=ansible | ok: [G-244] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:24:28,992 p=125136 u=gpadmin n=ansible | changed: [G-241] => (item=grafana/grafana:10.0.3)
2025-07-07 10:24:29,056 p=125136 u=gpadmin n=ansible | changed: [G-243] => (item=grafana/grafana:10.0.3)
2025-07-07 10:24:29,769 p=125136 u=gpadmin n=ansible | changed: [G-242] => (item=grafana/grafana:10.0.3)
2025-07-07 10:24:30,240 p=125136 u=gpadmin n=ansible | changed: [MASTER] => (item=grafana/grafana:10.0.3)
2025-07-07 10:24:30,444 p=125136 u=gpadmin n=ansible | changed: [G-244] => (item=grafana/grafana:10.0.3)
2025-07-07 10:24:30,604 p=125136 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] ****************************************************************
2025-07-07 10:24:30,615 p=125136 u=gpadmin n=ansible | TASK [Gathering Facts] ************************************************************************
2025-07-07 10:24:31,860 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:24:31,899 p=125136 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] **************************
2025-07-07 10:24:32,205 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:24:32,220 p=125136 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] *******************
2025-07-07 10:24:33,403 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:24:33,417 p=125136 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] ******************************
2025-07-07 10:24:33,707 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:24:33,723 p=125136 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] *************************************
2025-07-07 10:24:34,384 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:24:34,399 p=125136 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] ****************************************************
2025-07-07 10:24:34,969 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:24:34,979 p=125136 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] *******************************************
2025-07-07 10:24:35,313 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:24:35,328 p=125136 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] *********************************************
2025-07-07 10:24:35,352 p=125136 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\n2a807dcf7b2f   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_head"
}
2025-07-07 10:24:35,367 p=125136 u=gpadmin n=ansible | TASK [ray_head : Wait for a moment to let Ray head node initialize] ***************************
2025-07-07 10:24:35,382 p=125136 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 10:24:35,383 p=125136 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 10:24:45,387 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:24:45,402 p=125136 u=gpadmin n=ansible | TASK [ray_head : Check Ray head node status] **************************************************
2025-07-07 10:24:47,462 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:24:47,480 p=125136 u=gpadmin n=ansible | TASK [ray_head : Print Ray head node status] **************************************************
2025-07-07 10:24:47,525 p=125136 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:24:42.801430 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_76888ec52ce7273dc3edc165bba59741b7f746fabc6e012937d6070d",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Usage:",
        " 0.0/32.0 CPU",
        " 0B/100.53KiB memory",
        " 0B/9.31GiB object_store_memory",
        "",
        "Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:24:47,579 p=125136 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] *************************************************************
2025-07-07 10:24:47,588 p=125136 u=gpadmin n=ansible | TASK [Gathering Facts] ************************************************************************
2025-07-07 10:24:48,403 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:24:48,445 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:24:48,458 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:24:48,473 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:24:48,575 p=125136 u=gpadmin n=ansible | TASK [ray_worker : Ensure Ray temporary directory exists on worker node] **********************
2025-07-07 10:24:48,756 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:24:48,765 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:24:48,776 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:24:48,777 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:24:48,790 p=125136 u=gpadmin n=ansible | TASK [ray_worker : Stop and remove existing Ray worker container (idempotency)] ***************
2025-07-07 10:24:49,317 p=125136 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:24:49,326 p=125136 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:24:49,358 p=125136 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:24:49,411 p=125136 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:24:49,424 p=125136 u=gpadmin n=ansible | TASK [ray_worker : Remove old Ray worker start script (idempotency)] **************************
2025-07-07 10:24:49,587 p=125136 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:24:49,611 p=125136 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:24:49,618 p=125136 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:24:49,660 p=125136 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:24:49,674 p=125136 u=gpadmin n=ansible | TASK [ray_worker : Copy Ray worker start script to worker node] *******************************
2025-07-07 10:24:50,126 p=125136 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:24:50,150 p=125136 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:24:50,163 p=125136 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:24:50,184 p=125136 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:24:50,196 p=125136 u=gpadmin n=ansible | TASK [ray_worker : Start Ray worker container] ************************************************
2025-07-07 10:24:50,709 p=125136 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:24:50,710 p=125136 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:24:50,759 p=125136 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:24:50,838 p=125136 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:24:50,851 p=125136 u=gpadmin n=ansible | TASK [ray_worker : Display Ray worker container status] ***************************************
2025-07-07 10:24:51,050 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:24:51,058 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:24:51,065 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:24:51,085 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:24:51,098 p=125136 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker container status] *****************************************
2025-07-07 10:24:51,144 p=125136 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\n7eb98e6b7d0e   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:24:51,161 p=125136 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\n3473dd7e6fa7   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:24:51,164 p=125136 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\n45b85f660893   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:24:51,179 p=125136 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "CONTAINER ID   IMAGE                  COMMAND                  CREATED        STATUS                  PORTS     NAMES\nf6a2be217568   rayproject/ray:2.9.0   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:24:51,188 p=125136 u=gpadmin n=ansible | TASK [ray_worker : Wait for a moment to let Ray worker node initialize] ***********************
2025-07-07 10:24:51,203 p=125136 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 10:24:51,204 p=125136 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 10:25:01,208 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:25:01,220 p=125136 u=gpadmin n=ansible | TASK [ray_worker : Check Ray worker node status] **********************************************
2025-07-07 10:25:02,283 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:25:02,319 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:25:02,345 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:25:02,356 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:25:02,369 p=125136 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker node status] **********************************************
2025-07-07 10:25:02,434 p=125136 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:24:57.837883 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_21158c017d9412a789f3241487d1dffd428d800da1978a6809f16cc9",
        " 1 node_e3dad78d8ce7ba949de0ab84afde900ab12c0a68d4c2f8f4be984350",
        " 1 node_76888ec52ce7273dc3edc165bba59741b7f746fabc6e012937d6070d",
        " 1 node_fd9a86bf00183f9457bcc89df5071a04835ac91079d8b2fec9fec7e4",
        " 1 node_ae9588fe2673f6f7095084a6eb3d4f842250e91d0fb478621d53c359",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:25:02,435 p=125136 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:24:57.837883 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_21158c017d9412a789f3241487d1dffd428d800da1978a6809f16cc9",
        " 1 node_e3dad78d8ce7ba949de0ab84afde900ab12c0a68d4c2f8f4be984350",
        " 1 node_76888ec52ce7273dc3edc165bba59741b7f746fabc6e012937d6070d",
        " 1 node_fd9a86bf00183f9457bcc89df5071a04835ac91079d8b2fec9fec7e4",
        " 1 node_ae9588fe2673f6f7095084a6eb3d4f842250e91d0fb478621d53c359",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:25:02,454 p=125136 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:24:57.837883 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_21158c017d9412a789f3241487d1dffd428d800da1978a6809f16cc9",
        " 1 node_e3dad78d8ce7ba949de0ab84afde900ab12c0a68d4c2f8f4be984350",
        " 1 node_76888ec52ce7273dc3edc165bba59741b7f746fabc6e012937d6070d",
        " 1 node_fd9a86bf00183f9457bcc89df5071a04835ac91079d8b2fec9fec7e4",
        " 1 node_ae9588fe2673f6f7095084a6eb3d4f842250e91d0fb478621d53c359",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:25:02,462 p=125136 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:24:57.837883 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_21158c017d9412a789f3241487d1dffd428d800da1978a6809f16cc9",
        " 1 node_e3dad78d8ce7ba949de0ab84afde900ab12c0a68d4c2f8f4be984350",
        " 1 node_76888ec52ce7273dc3edc165bba59741b7f746fabc6e012937d6070d",
        " 1 node_fd9a86bf00183f9457bcc89df5071a04835ac91079d8b2fec9fec7e4",
        " 1 node_ae9588fe2673f6f7095084a6eb3d4f842250e91d0fb478621d53c359",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:25:02,582 p=125136 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] **********************************************************
2025-07-07 10:25:02,603 p=125136 u=gpadmin n=ansible | TASK [Check Ray head node status] *************************************************************
2025-07-07 10:25:04,656 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:25:04,670 p=125136 u=gpadmin n=ansible | TASK [Display Ray cluster status] *************************************************************
2025-07-07 10:25:04,698 p=125136 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:25:02.845943 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_21158c017d9412a789f3241487d1dffd428d800da1978a6809f16cc9",
        " 1 node_e3dad78d8ce7ba949de0ab84afde900ab12c0a68d4c2f8f4be984350",
        " 1 node_76888ec52ce7273dc3edc165bba59741b7f746fabc6e012937d6070d",
        " 1 node_fd9a86bf00183f9457bcc89df5071a04835ac91079d8b2fec9fec7e4",
        " 1 node_ae9588fe2673f6f7095084a6eb3d4f842250e91d0fb478621d53c359",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:25:04,711 p=125136 u=gpadmin n=ansible | TASK [Display Ray cluster error] **************************************************************
2025-07-07 10:25:04,733 p=125136 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:25:04,773 p=125136 u=gpadmin n=ansible | PLAY [Deploy Monitoring Stack] ****************************************************************
2025-07-07 10:25:04,785 p=125136 u=gpadmin n=ansible | TASK [Gathering Facts] ************************************************************************
2025-07-07 10:25:05,610 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:25:05,626 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:25:05,629 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:25:05,655 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:25:06,020 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:25:06,094 p=125136 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus data directory] ******************************************
2025-07-07 10:25:06,275 p=125136 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:25:06,295 p=125136 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:25:06,296 p=125136 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:25:06,312 p=125136 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:25:06,377 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:25:06,393 p=125136 u=gpadmin n=ansible | TASK [monitoring : Create Grafana data directory] *********************************************
2025-07-07 10:25:06,587 p=125136 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:25:06,610 p=125136 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:25:06,646 p=125136 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:25:06,654 p=125136 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:25:06,688 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:25:06,701 p=125136 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Node Exporter container (idempotency)] ************
2025-07-07 10:25:07,090 p=125136 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:25:07,127 p=125136 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:25:07,128 p=125136 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:25:07,167 p=125136 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:25:07,295 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:25:07,303 p=125136 u=gpadmin n=ansible | TASK [monitoring : Start Node Exporter container] *********************************************
2025-07-07 10:25:07,825 p=125136 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:25:07,831 p=125136 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:25:07,832 p=125136 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:25:07,942 p=125136 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:25:07,997 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:25:08,011 p=125136 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing cAdvisor container (idempotency)] *****************
2025-07-07 10:25:08,330 p=125136 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:25:08,339 p=125136 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:25:08,344 p=125136 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:25:08,356 p=125136 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:25:08,635 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:25:08,691 p=125136 u=gpadmin n=ansible | TASK [monitoring : Start cAdvisor container] **************************************************
2025-07-07 10:25:09,154 p=125136 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Error starting container f934930c53a777bce9d333eba4a5ff45cd130bdd645deb758c7159a8431b6710: 500 Server Error for http+docker://localhost/v1.49/containers/f934930c53a777bce9d333eba4a5ff45cd130bdd645deb758c7159a8431b6710/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:25:09,176 p=125136 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Error starting container 5f99defc1284775436703b181fe543952c17b51431d7a7ce611633b2c171d674: 500 Server Error for http+docker://localhost/v1.49/containers/5f99defc1284775436703b181fe543952c17b51431d7a7ce611633b2c171d674/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:25:09,184 p=125136 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Error starting container 7b31500b90ca7f67e12174494f72e0c763fa34d6c7653a29e21f839fd0253b26: 500 Server Error for http+docker://localhost/v1.49/containers/7b31500b90ca7f67e12174494f72e0c763fa34d6c7653a29e21f839fd0253b26/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:25:09,278 p=125136 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Error starting container 34526b55476fd28fa8e22046800609fcb93ec822b01eb0006ac709c678ee0ab2: 500 Server Error for http+docker://localhost/v1.49/containers/34526b55476fd28fa8e22046800609fcb93ec822b01eb0006ac709c678ee0ab2/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:25:09,397 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:25:09,405 p=125136 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Prometheus container (idempotency)] ***************
2025-07-07 10:25:10,012 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:25:10,026 p=125136 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus configuration] *******************************************
2025-07-07 10:25:10,668 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:25:10,682 p=125136 u=gpadmin n=ansible | TASK [monitoring : Start Prometheus container] ************************************************
2025-07-07 10:25:11,356 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:25:11,365 p=125136 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Grafana container (idempotency)] ******************
2025-07-07 10:25:12,019 p=125136 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:25:12,036 p=125136 u=gpadmin n=ansible | TASK [monitoring : Create Grafana provisioning directories] ***********************************
2025-07-07 10:25:12,329 p=125136 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/datasources)
2025-07-07 10:25:12,600 p=125136 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/dashboards)
2025-07-07 10:25:12,867 p=125136 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/dashboards)
2025-07-07 10:25:12,883 p=125136 u=gpadmin n=ansible | TASK [monitoring : Create Grafana datasource configuration] ***********************************
2025-07-07 10:25:13,435 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:25:13,449 p=125136 u=gpadmin n=ansible | TASK [monitoring : Create Grafana dashboard provider configuration] ***************************
2025-07-07 10:25:14,027 p=125136 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:25:14,041 p=125136 u=gpadmin n=ansible | TASK [monitoring : Copy Grafana dashboards] ***************************************************
2025-07-07 10:25:14,574 p=125136 u=gpadmin n=ansible | ok: [MASTER] => (item=node-exporter-dashboard.json)
2025-07-07 10:25:15,097 p=125136 u=gpadmin n=ansible | ok: [MASTER] => (item=docker-container-dashboard.json)
2025-07-07 10:25:15,631 p=125136 u=gpadmin n=ansible | ok: [MASTER] => (item=ray-cluster-dashboard.json)
2025-07-07 10:25:15,646 p=125136 u=gpadmin n=ansible | TASK [monitoring : Start Grafana container] ***************************************************
2025-07-07 10:25:16,143 p=125136 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Non-string value found for env option. Ambiguous env options must be wrapped in quotes to avoid them being interpreted. Key: GF_SERVER_HTTP_PORT"}
2025-07-07 10:25:16,144 p=125136 u=gpadmin n=ansible | PLAY RECAP ************************************************************************************
2025-07-07 10:25:16,145 p=125136 u=gpadmin n=ansible | G-241                      : ok=35   changed=11   unreachable=0    failed=1    skipped=7    rescued=0    ignored=0   
2025-07-07 10:25:16,145 p=125136 u=gpadmin n=ansible | G-242                      : ok=34   changed=11   unreachable=0    failed=1    skipped=7    rescued=0    ignored=0   
2025-07-07 10:25:16,145 p=125136 u=gpadmin n=ansible | G-243                      : ok=34   changed=11   unreachable=0    failed=1    skipped=7    rescued=0    ignored=0   
2025-07-07 10:25:16,145 p=125136 u=gpadmin n=ansible | G-244                      : ok=34   changed=11   unreachable=0    failed=1    skipped=7    rescued=0    ignored=0   
2025-07-07 10:25:16,145 p=125136 u=gpadmin n=ansible | MASTER                     : ok=46   changed=14   unreachable=0    failed=1    skipped=9    rescued=0    ignored=0   
2025-07-07 10:26:04,841 p=131648 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] ******************************************************
2025-07-07 10:26:04,852 p=131648 u=gpadmin n=ansible | TASK [Gathering Facts] ************************************************************************
2025-07-07 10:26:05,905 p=131648 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:05,938 p=131648 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:05,943 p=131648 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:05,974 p=131648 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:06,354 p=131648 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:06,577 p=131648 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] ****************************************************************
2025-07-07 10:26:06,587 p=131648 u=gpadmin n=ansible | TASK [Gathering Facts] ************************************************************************
2025-07-07 10:26:07,817 p=131648 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:07,885 p=131648 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] *************************************************************
2025-07-07 10:26:07,894 p=131648 u=gpadmin n=ansible | TASK [Gathering Facts] ************************************************************************
2025-07-07 10:26:08,670 p=131648 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:08,693 p=131648 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:08,713 p=131648 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:08,842 p=131648 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:09,041 p=131648 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] **********************************************************
2025-07-07 10:26:09,083 p=131648 u=gpadmin n=ansible | PLAY [Deploy Monitoring Stack] ****************************************************************
2025-07-07 10:26:09,093 p=131648 u=gpadmin n=ansible | TASK [Gathering Facts] ************************************************************************
2025-07-07 10:26:09,922 p=131648 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:09,976 p=131648 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:09,985 p=131648 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:10,009 p=131648 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:10,308 p=131648 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:10,501 p=131648 u=gpadmin n=ansible | PLAY RECAP ************************************************************************************
2025-07-07 10:26:10,501 p=131648 u=gpadmin n=ansible | G-241                      : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 10:26:10,501 p=131648 u=gpadmin n=ansible | G-242                      : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 10:26:10,501 p=131648 u=gpadmin n=ansible | G-243                      : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 10:26:10,501 p=131648 u=gpadmin n=ansible | G-244                      : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 10:26:10,501 p=131648 u=gpadmin n=ansible | MASTER                     : ok=3    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 10:26:33,527 p=132627 u=gpadmin n=ansible | ERROR! 'ansible.builtin.file' is not a valid attribute for a Play

The error appears to be in '/home/gpadmin/cursor-projects/02-Ray-Deploy/roles/ray_head/tasks/main.yml': line 2, column 3, but may
be elsewhere in the file depending on the exact syntax problem.

The offending line appears to be:

---
- name: Ensure Ray temporary directory exists on head node
  ^ here

2025-07-07 10:26:38,219 p=132698 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] ***************************************************************************************
2025-07-07 10:26:38,228 p=132698 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************************************************************
2025-07-07 10:26:39,357 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:39,375 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:39,389 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:39,409 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:39,680 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:39,753 p=132698 u=gpadmin n=ansible | TASK [common : Update apt cache] ***********************************************************************************************
2025-07-07 10:26:40,234 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:40,236 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:40,240 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:40,253 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:40,635 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:40,648 p=132698 u=gpadmin n=ansible | TASK [common : Install common packages] ****************************************************************************************
2025-07-07 10:26:41,105 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:41,120 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:41,137 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:41,149 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:41,707 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:41,720 p=132698 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] *********************************************************************************
2025-07-07 10:26:42,012 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:42,014 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:42,018 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:42,024 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:42,119 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:42,132 p=132698 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] **********************************************************************************
2025-07-07 10:26:42,402 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:42,411 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:42,412 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:42,419 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:42,506 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:42,561 p=132698 u=gpadmin n=ansible | TASK [docker : Check if Docker is available via which command] *****************************************************************
2025-07-07 10:26:42,844 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:42,854 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:42,865 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:42,878 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:42,946 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:42,959 p=132698 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via apt] ***************************************************************************
2025-07-07 10:26:43,141 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:43,160 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:43,177 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:43,199 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:43,280 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:43,294 p=132698 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via snap] **************************************************************************
2025-07-07 10:26:43,482 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:43,528 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:43,529 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:43,555 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:43,624 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:43,638 p=132698 u=gpadmin n=ansible | TASK [docker : Set Docker installation status facts] ***************************************************************************
2025-07-07 10:26:43,686 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:43,702 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:43,717 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:43,719 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:43,731 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:43,740 p=132698 u=gpadmin n=ansible | TASK [docker : Display Docker installation status] *****************************************************************************
2025-07-07 10:26:43,775 p=132698 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: True",
        "Docker installed via snap: False"
    ]
}
2025-07-07 10:26:43,795 p=132698 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:26:43,811 p=132698 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:26:43,813 p=132698 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:26:43,831 p=132698 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:26:43,840 p=132698 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] *********************************************************************************************
2025-07-07 10:26:43,876 p=132698 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:26:43,892 p=132698 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:26:43,907 p=132698 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:26:43,907 p=132698 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:26:43,915 p=132698 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:26:43,924 p=132698 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] **************************************************************************************
2025-07-07 10:26:43,960 p=132698 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:26:43,976 p=132698 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:26:43,990 p=132698 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:26:43,992 p=132698 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:26:44,002 p=132698 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:26:44,014 p=132698 u=gpadmin n=ansible | TASK [docker : Update apt cache after adding Docker repository] ****************************************************************
2025-07-07 10:26:44,051 p=132698 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:26:44,066 p=132698 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:26:44,081 p=132698 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:26:44,083 p=132698 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:26:44,094 p=132698 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:26:44,103 p=132698 u=gpadmin n=ansible | TASK [docker : Install Docker Engine via APT] **********************************************************************************
2025-07-07 10:26:44,139 p=132698 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:26:44,153 p=132698 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:26:44,167 p=132698 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:26:44,169 p=132698 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:26:44,176 p=132698 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:26:44,185 p=132698 u=gpadmin n=ansible | TASK [docker : Check Docker service status (systemd)] **************************************************************************
2025-07-07 10:26:44,358 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:44,378 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:44,426 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:44,438 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:44,498 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:44,511 p=132698 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] *********************************************************************************
2025-07-07 10:26:44,570 p=132698 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:26:44,584 p=132698 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:26:44,591 p=132698 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:26:44,604 p=132698 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:26:45,198 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:26:45,211 p=132698 u=gpadmin n=ansible | TASK [docker : Install Docker via snap if not installed via apt] ***************************************************************
2025-07-07 10:26:45,237 p=132698 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:26:45,256 p=132698 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:26:45,275 p=132698 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:26:45,292 p=132698 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:26:45,301 p=132698 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:26:45,310 p=132698 u=gpadmin n=ansible | TASK [docker : Wait for snap Docker installation to complete] ******************************************************************
2025-07-07 10:26:45,336 p=132698 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:26:45,345 p=132698 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] ***********************************************************
2025-07-07 10:26:45,378 p=132698 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:26:45,547 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:26:45,553 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:26:45,561 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:26:45,568 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:26:45,582 p=132698 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ************************************************************************************
2025-07-07 10:26:45,626 p=132698 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:26:45,936 p=132698 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:27:19,052 p=132698 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:27:19,054 p=132698 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:27:19,058 p=132698 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:27:19,071 p=132698 u=gpadmin n=ansible | TASK [docker : Add user to docker group (for apt installation)] ****************************************************************
2025-07-07 10:27:19,130 p=132698 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:27:19,145 p=132698 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:27:19,147 p=132698 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:27:19,159 p=132698 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:27:19,610 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:27:19,624 p=132698 u=gpadmin n=ansible | TASK [docker : Wait for Docker to be ready] ************************************************************************************
2025-07-07 10:27:19,858 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:27:19,882 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:27:19,897 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:27:19,902 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:27:20,040 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:27:20,057 p=132698 u=gpadmin n=ansible | TASK [docker : Display Docker version] *****************************************************************************************
2025-07-07 10:27:20,298 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:27:20,305 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:27:20,314 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:27:20,338 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:27:20,374 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:27:20,388 p=132698 u=gpadmin n=ansible | TASK [docker : Show Docker version] ********************************************************************************************
2025-07-07 10:27:20,435 p=132698 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Docker version 28.3.1, build 38b7060"
}
2025-07-07 10:27:20,451 p=132698 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:27:20,467 p=132698 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:27:20,469 p=132698 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:27:20,485 p=132698 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:27:20,494 p=132698 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] ******************************************************************************************
2025-07-07 10:28:01,971 p=132698 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:28:03,402 p=132698 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:28:04,451 p=132698 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:28:06,090 p=132698 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:28:15,786 p=132698 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:28:15,798 p=132698 u=gpadmin n=ansible | TASK [docker : Pull monitoring Docker images] **********************************************************************************
2025-07-07 10:28:16,576 p=132698 u=gpadmin n=ansible | ok: [G-243] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:28:16,592 p=132698 u=gpadmin n=ansible | ok: [G-241] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:28:16,606 p=132698 u=gpadmin n=ansible | ok: [G-242] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:28:16,614 p=132698 u=gpadmin n=ansible | ok: [G-244] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:28:16,727 p=132698 u=gpadmin n=ansible | ok: [MASTER] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:28:17,274 p=132698 u=gpadmin n=ansible | ok: [G-242] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:28:17,276 p=132698 u=gpadmin n=ansible | ok: [G-243] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:28:17,301 p=132698 u=gpadmin n=ansible | ok: [G-241] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:28:17,305 p=132698 u=gpadmin n=ansible | ok: [G-244] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:28:17,622 p=132698 u=gpadmin n=ansible | ok: [MASTER] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:28:18,114 p=132698 u=gpadmin n=ansible | ok: [G-243] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:28:18,177 p=132698 u=gpadmin n=ansible | ok: [G-244] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:28:18,179 p=132698 u=gpadmin n=ansible | ok: [G-241] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:28:18,186 p=132698 u=gpadmin n=ansible | ok: [G-242] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:28:18,696 p=132698 u=gpadmin n=ansible | ok: [MASTER] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:28:18,785 p=132698 u=gpadmin n=ansible | ok: [G-243] => (item=grafana/grafana:10.0.3)
2025-07-07 10:28:18,851 p=132698 u=gpadmin n=ansible | ok: [G-242] => (item=grafana/grafana:10.0.3)
2025-07-07 10:28:18,860 p=132698 u=gpadmin n=ansible | ok: [G-244] => (item=grafana/grafana:10.0.3)
2025-07-07 10:28:18,871 p=132698 u=gpadmin n=ansible | ok: [G-241] => (item=grafana/grafana:10.0.3)
2025-07-07 10:28:19,596 p=132698 u=gpadmin n=ansible | ok: [MASTER] => (item=grafana/grafana:10.0.3)
2025-07-07 10:28:19,744 p=132698 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] *************************************************************************************************
2025-07-07 10:28:19,754 p=132698 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************************************************************
2025-07-07 10:28:20,944 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:28:20,993 p=132698 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] ***********************************************************
2025-07-07 10:28:21,293 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:28:21,310 p=132698 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ****************************************************
2025-07-07 10:28:23,826 p=132698 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:28:23,840 p=132698 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] ***************************************************************
2025-07-07 10:28:24,144 p=132698 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:28:24,158 p=132698 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] **********************************************************************
2025-07-07 10:28:24,815 p=132698 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:28:24,829 p=132698 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] *************************************************************************************
2025-07-07 10:28:25,355 p=132698 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:28:25,369 p=132698 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ****************************************************************************
2025-07-07 10:28:25,704 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:28:25,719 p=132698 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] ******************************************************************************
2025-07-07 10:28:25,740 p=132698 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n829f0b991643   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_head"
}
2025-07-07 10:28:25,755 p=132698 u=gpadmin n=ansible | TASK [ray_head : Wait for a moment to let Ray head node initialize] ************************************************************
2025-07-07 10:28:25,775 p=132698 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 10:28:25,776 p=132698 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 10:28:35,780 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:28:35,794 p=132698 u=gpadmin n=ansible | TASK [ray_head : Check Ray head node status] ***********************************************************************************
2025-07-07 10:28:37,893 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:28:37,907 p=132698 u=gpadmin n=ansible | TASK [ray_head : Print Ray head node status] ***********************************************************************************
2025-07-07 10:28:37,949 p=132698 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:28:32.781662 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_bfa36bf8f9dc26b9db809a97dbc99d6b5f33f9e9169d5b0642d4eed9",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/32.0 CPU",
        " 0B/100.53KiB memory",
        " 0B/9.31GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:28:38,006 p=132698 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] **********************************************************************************************
2025-07-07 10:28:38,016 p=132698 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************************************************************
2025-07-07 10:28:38,811 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:28:38,906 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:28:38,911 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:28:38,923 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:28:39,025 p=132698 u=gpadmin n=ansible | TASK [ray_worker : Ensure Ray temporary directory exists on worker node] *******************************************************
2025-07-07 10:28:39,190 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:28:39,229 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:28:39,233 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:28:39,238 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:28:39,250 p=132698 u=gpadmin n=ansible | TASK [ray_worker : Stop and remove existing Ray worker container (idempotency)] ************************************************
2025-07-07 10:28:39,652 p=132698 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:28:39,658 p=132698 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:28:39,684 p=132698 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:28:39,701 p=132698 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:28:39,713 p=132698 u=gpadmin n=ansible | TASK [ray_worker : Remove old Ray worker start script (idempotency)] ***********************************************************
2025-07-07 10:28:39,881 p=132698 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:28:39,900 p=132698 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:28:39,910 p=132698 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:28:39,922 p=132698 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:28:39,935 p=132698 u=gpadmin n=ansible | TASK [ray_worker : Copy Ray worker start script to worker node] ****************************************************************
2025-07-07 10:28:40,395 p=132698 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:28:40,416 p=132698 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:28:40,438 p=132698 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:28:40,492 p=132698 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:28:40,505 p=132698 u=gpadmin n=ansible | TASK [ray_worker : Start Ray worker container] *********************************************************************************
2025-07-07 10:28:41,035 p=132698 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:28:41,064 p=132698 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:28:41,065 p=132698 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:28:41,069 p=132698 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:28:41,083 p=132698 u=gpadmin n=ansible | TASK [ray_worker : Display Ray worker container status] ************************************************************************
2025-07-07 10:28:41,282 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:28:41,311 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:28:41,311 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:28:41,319 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:28:41,332 p=132698 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker container status] **************************************************************************
2025-07-07 10:28:41,382 p=132698 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED        STATUS                  PORTS     NAMES\nfb0de1d06803   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:28:41,398 p=132698 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED        STATUS                  PORTS     NAMES\n9fa67a5d616c   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:28:41,400 p=132698 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED        STATUS                  PORTS     NAMES\n05c79b4fdbca   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:28:41,416 p=132698 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED        STATUS                  PORTS     NAMES\n252201ac72cd   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:28:41,424 p=132698 u=gpadmin n=ansible | TASK [ray_worker : Wait for a moment to let Ray worker node initialize] ********************************************************
2025-07-07 10:28:41,443 p=132698 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 10:28:41,444 p=132698 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 10:28:51,448 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:28:51,460 p=132698 u=gpadmin n=ansible | TASK [ray_worker : Check Ray worker node status] *******************************************************************************
2025-07-07 10:28:52,530 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:28:52,579 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:28:52,579 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:28:52,582 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:28:52,595 p=132698 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker node status] *******************************************************************************
2025-07-07 10:28:52,660 p=132698 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:28:47.813774 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_c43e3cbff485241389a8742db1b6a181199988b03201056be374ba3b",
        " 1 node_d79d7da5be899e464899d48c51187e36a6c36dbb2a5504f688ea46cd",
        " 1 node_bfa36bf8f9dc26b9db809a97dbc99d6b5f33f9e9169d5b0642d4eed9",
        " 1 node_affc43d67c6d4cef37d9fc3a83fcdee943307fa7c29deb5e01c1d00b",
        " 1 node_e5b379a412973a1e474eade552955f9defee3d5ba3bed7ced6b8d593",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:28:52,663 p=132698 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:28:47.813774 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_c43e3cbff485241389a8742db1b6a181199988b03201056be374ba3b",
        " 1 node_d79d7da5be899e464899d48c51187e36a6c36dbb2a5504f688ea46cd",
        " 1 node_bfa36bf8f9dc26b9db809a97dbc99d6b5f33f9e9169d5b0642d4eed9",
        " 1 node_affc43d67c6d4cef37d9fc3a83fcdee943307fa7c29deb5e01c1d00b",
        " 1 node_e5b379a412973a1e474eade552955f9defee3d5ba3bed7ced6b8d593",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:28:52,679 p=132698 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:28:47.813774 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_c43e3cbff485241389a8742db1b6a181199988b03201056be374ba3b",
        " 1 node_d79d7da5be899e464899d48c51187e36a6c36dbb2a5504f688ea46cd",
        " 1 node_bfa36bf8f9dc26b9db809a97dbc99d6b5f33f9e9169d5b0642d4eed9",
        " 1 node_affc43d67c6d4cef37d9fc3a83fcdee943307fa7c29deb5e01c1d00b",
        " 1 node_e5b379a412973a1e474eade552955f9defee3d5ba3bed7ced6b8d593",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:28:52,692 p=132698 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:28:47.813774 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_c43e3cbff485241389a8742db1b6a181199988b03201056be374ba3b",
        " 1 node_d79d7da5be899e464899d48c51187e36a6c36dbb2a5504f688ea46cd",
        " 1 node_bfa36bf8f9dc26b9db809a97dbc99d6b5f33f9e9169d5b0642d4eed9",
        " 1 node_affc43d67c6d4cef37d9fc3a83fcdee943307fa7c29deb5e01c1d00b",
        " 1 node_e5b379a412973a1e474eade552955f9defee3d5ba3bed7ced6b8d593",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:28:52,813 p=132698 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] *******************************************************************************************
2025-07-07 10:28:52,835 p=132698 u=gpadmin n=ansible | TASK [Check Ray head node status] **********************************************************************************************
2025-07-07 10:28:54,944 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:28:54,959 p=132698 u=gpadmin n=ansible | TASK [Display Ray cluster status] **********************************************************************************************
2025-07-07 10:28:54,987 p=132698 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:28:52.825872 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_c43e3cbff485241389a8742db1b6a181199988b03201056be374ba3b",
        " 1 node_d79d7da5be899e464899d48c51187e36a6c36dbb2a5504f688ea46cd",
        " 1 node_bfa36bf8f9dc26b9db809a97dbc99d6b5f33f9e9169d5b0642d4eed9",
        " 1 node_affc43d67c6d4cef37d9fc3a83fcdee943307fa7c29deb5e01c1d00b",
        " 1 node_e5b379a412973a1e474eade552955f9defee3d5ba3bed7ced6b8d593",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:28:55,001 p=132698 u=gpadmin n=ansible | TASK [Display Ray cluster error] ***********************************************************************************************
2025-07-07 10:28:55,023 p=132698 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:28:55,065 p=132698 u=gpadmin n=ansible | PLAY [Deploy Monitoring Stack] *************************************************************************************************
2025-07-07 10:28:55,078 p=132698 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************************************************************
2025-07-07 10:28:55,898 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:28:55,966 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:28:55,971 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:28:55,973 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:28:56,286 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:28:56,365 p=132698 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus data directory] ***************************************************************************
2025-07-07 10:28:56,562 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:28:56,566 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:28:56,577 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:28:56,595 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:28:56,664 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:28:56,680 p=132698 u=gpadmin n=ansible | TASK [monitoring : Create Grafana data directory] ******************************************************************************
2025-07-07 10:28:56,888 p=132698 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:28:56,913 p=132698 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:28:56,915 p=132698 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:28:56,931 p=132698 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:28:56,992 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:28:57,005 p=132698 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Node Exporter container (idempotency)] *********************************************
2025-07-07 10:28:57,415 p=132698 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:28:57,416 p=132698 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:28:57,431 p=132698 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:28:57,448 p=132698 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:28:57,580 p=132698 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:28:57,594 p=132698 u=gpadmin n=ansible | TASK [monitoring : Start Node Exporter container] ******************************************************************************
2025-07-07 10:28:58,116 p=132698 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:28:58,132 p=132698 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:28:58,139 p=132698 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:28:58,157 p=132698 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:28:58,298 p=132698 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:28:58,307 p=132698 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing cAdvisor container (idempotency)] **************************************************
2025-07-07 10:28:58,602 p=132698 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:28:58,636 p=132698 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:28:58,637 p=132698 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:28:58,667 p=132698 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:28:58,907 p=132698 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:28:58,964 p=132698 u=gpadmin n=ansible | TASK [monitoring : Start cAdvisor container] ***********************************************************************************
2025-07-07 10:28:59,423 p=132698 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Error starting container c8e43ce82739ad71d33116c49a0942426ec02aea45bb3f31827472f500fa115c: 500 Server Error for http+docker://localhost/v1.49/containers/c8e43ce82739ad71d33116c49a0942426ec02aea45bb3f31827472f500fa115c/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:28:59,436 p=132698 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Error starting container 91accd8c9faa22c493ab3351b5da68f11be5e2202535b1638f744cf34022504f: 500 Server Error for http+docker://localhost/v1.49/containers/91accd8c9faa22c493ab3351b5da68f11be5e2202535b1638f744cf34022504f/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:28:59,450 p=132698 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Error starting container b28fe8c90a80668941e1d164e4990b586a7f37ad40e796752f0273d9e8e48bc2: 500 Server Error for http+docker://localhost/v1.49/containers/b28fe8c90a80668941e1d164e4990b586a7f37ad40e796752f0273d9e8e48bc2/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:28:59,456 p=132698 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Error starting container f9d3961f4d75106b954a04306822aa688b3ca8c2ed4527647b9be0dcc3ae1e08: 500 Server Error for http+docker://localhost/v1.49/containers/f9d3961f4d75106b954a04306822aa688b3ca8c2ed4527647b9be0dcc3ae1e08/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:28:59,663 p=132698 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:28:59,671 p=132698 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Prometheus container (idempotency)] ************************************************
2025-07-07 10:29:00,265 p=132698 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:29:00,284 p=132698 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus configuration] ****************************************************************************
2025-07-07 10:29:00,936 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:29:00,950 p=132698 u=gpadmin n=ansible | TASK [monitoring : Start Prometheus container] *********************************************************************************
2025-07-07 10:29:01,660 p=132698 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:29:01,672 p=132698 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Grafana container (idempotency)] ***************************************************
2025-07-07 10:29:02,222 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:29:02,240 p=132698 u=gpadmin n=ansible | TASK [monitoring : Create Grafana provisioning directories] ********************************************************************
2025-07-07 10:29:02,538 p=132698 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/datasources)
2025-07-07 10:29:02,804 p=132698 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/dashboards)
2025-07-07 10:29:03,078 p=132698 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/dashboards)
2025-07-07 10:29:03,099 p=132698 u=gpadmin n=ansible | TASK [monitoring : Create Grafana datasource configuration] ********************************************************************
2025-07-07 10:29:03,667 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:29:03,683 p=132698 u=gpadmin n=ansible | TASK [monitoring : Create Grafana dashboard provider configuration] ************************************************************
2025-07-07 10:29:04,259 p=132698 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:29:04,274 p=132698 u=gpadmin n=ansible | TASK [monitoring : Copy Grafana dashboards] ************************************************************************************
2025-07-07 10:29:04,830 p=132698 u=gpadmin n=ansible | ok: [MASTER] => (item=node-exporter-dashboard.json)
2025-07-07 10:29:05,373 p=132698 u=gpadmin n=ansible | ok: [MASTER] => (item=docker-container-dashboard.json)
2025-07-07 10:29:05,901 p=132698 u=gpadmin n=ansible | ok: [MASTER] => (item=ray-cluster-dashboard.json)
2025-07-07 10:29:05,917 p=132698 u=gpadmin n=ansible | TASK [monitoring : Start Grafana container] ************************************************************************************
2025-07-07 10:29:06,416 p=132698 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Non-string value found for env option. Ambiguous env options must be wrapped in quotes to avoid them being interpreted. Key: GF_SERVER_HTTP_PORT"}
2025-07-07 10:29:06,417 p=132698 u=gpadmin n=ansible | PLAY RECAP *********************************************************************************************************************
2025-07-07 10:29:06,418 p=132698 u=gpadmin n=ansible | G-241                      : ok=35   changed=9    unreachable=0    failed=1    skipped=7    rescued=0    ignored=0   
2025-07-07 10:29:06,418 p=132698 u=gpadmin n=ansible | G-242                      : ok=34   changed=9    unreachable=0    failed=1    skipped=7    rescued=0    ignored=0   
2025-07-07 10:29:06,418 p=132698 u=gpadmin n=ansible | G-243                      : ok=34   changed=9    unreachable=0    failed=1    skipped=7    rescued=0    ignored=0   
2025-07-07 10:29:06,418 p=132698 u=gpadmin n=ansible | G-244                      : ok=34   changed=9    unreachable=0    failed=1    skipped=7    rescued=0    ignored=0   
2025-07-07 10:29:06,418 p=132698 u=gpadmin n=ansible | MASTER                     : ok=46   changed=11   unreachable=0    failed=1    skipped=9    rescued=0    ignored=0   
2025-07-07 10:33:38,103 p=141529 u=gpadmin n=ansible | PLAY [Synchronize Ray and Python Versions Across Cluster] **********************************************************************
2025-07-07 10:33:38,114 p=141529 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************************************************************
2025-07-07 10:33:39,702 p=141529 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:33:39,745 p=141529 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:33:39,751 p=141529 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:33:39,867 p=141529 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:33:40,596 p=141529 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:33:40,609 p=141529 u=gpadmin n=ansible | TASK [Display cluster synchronization info] ************************************************************************************
2025-07-07 10:33:40,653 p=141529 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "==========================================",
        "RAY CLUSTER VERSION SYNCHRONIZATION",
        "==========================================",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1",
        "Enforcement enabled: True",
        "Cleanup conflicts: True",
        "=========================================="
    ]
}
2025-07-07 10:33:40,667 p=141529 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "==========================================",
        "RAY CLUSTER VERSION SYNCHRONIZATION",
        "==========================================",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1",
        "Enforcement enabled: True",
        "Cleanup conflicts: True",
        "=========================================="
    ]
}
2025-07-07 10:33:40,683 p=141529 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "==========================================",
        "RAY CLUSTER VERSION SYNCHRONIZATION",
        "==========================================",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1",
        "Enforcement enabled: True",
        "Cleanup conflicts: True",
        "=========================================="
    ]
}
2025-07-07 10:33:40,685 p=141529 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "==========================================",
        "RAY CLUSTER VERSION SYNCHRONIZATION",
        "==========================================",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1",
        "Enforcement enabled: True",
        "Cleanup conflicts: True",
        "=========================================="
    ]
}
2025-07-07 10:33:40,702 p=141529 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "==========================================",
        "RAY CLUSTER VERSION SYNCHRONIZATION",
        "==========================================",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1",
        "Enforcement enabled: True",
        "Cleanup conflicts: True",
        "=========================================="
    ]
}
2025-07-07 10:33:40,766 p=141529 u=gpadmin n=ansible | TASK [version_control : Check current Ray container status] ********************************************************************
2025-07-07 10:33:41,191 p=141529 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:33:41,194 p=141529 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:33:41,197 p=141529 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:33:41,199 p=141529 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:33:41,390 p=141529 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:33:41,406 p=141529 u=gpadmin n=ansible | TASK [version_control : Get current Ray container image if exists] *************************************************************
2025-07-07 10:33:41,449 p=141529 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:33:41,464 p=141529 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:33:41,479 p=141529 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:33:41,481 p=141529 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:33:41,503 p=141529 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:33:41,512 p=141529 u=gpadmin n=ansible | TASK [version_control : Display current Ray image] *****************************************************************************
2025-07-07 10:33:41,548 p=141529 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:33:41,581 p=141529 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:33:41,582 p=141529 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:33:41,586 p=141529 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:33:41,598 p=141529 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:33:41,609 p=141529 u=gpadmin n=ansible | TASK [version_control : Check if Ray version matches target] *******************************************************************
2025-07-07 10:33:41,646 p=141529 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:33:41,665 p=141529 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:33:41,684 p=141529 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:33:41,688 p=141529 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:33:41,706 p=141529 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:33:41,716 p=141529 u=gpadmin n=ansible | TASK [version_control : Pull target Ray Docker image] **************************************************************************
2025-07-07 10:33:41,752 p=141529 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:33:41,773 p=141529 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:33:41,789 p=141529 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:33:41,793 p=141529 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:33:41,810 p=141529 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:33:41,819 p=141529 u=gpadmin n=ansible | TASK [version_control : Stop Ray head container if version mismatch] ***********************************************************
2025-07-07 10:33:41,854 p=141529 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:33:41,869 p=141529 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:33:41,883 p=141529 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:33:41,885 p=141529 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:33:41,904 p=141529 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:33:41,914 p=141529 u=gpadmin n=ansible | TASK [version_control : Stop Ray worker container if version mismatch] *********************************************************
2025-07-07 10:33:41,948 p=141529 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:33:41,963 p=141529 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:33:41,978 p=141529 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:33:41,984 p=141529 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:33:42,002 p=141529 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:33:42,015 p=141529 u=gpadmin n=ansible | TASK [version_control : Remove old Ray images to free space] *******************************************************************
2025-07-07 10:33:42,050 p=141529 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:33:42,067 p=141529 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:33:42,088 p=141529 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:33:42,089 p=141529 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:33:42,102 p=141529 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:33:42,112 p=141529 u=gpadmin n=ansible | TASK [version_control : Start temporary Ray container to check Python version] *************************************************
2025-07-07 10:33:43,063 p=141529 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:33:43,067 p=141529 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:33:43,072 p=141529 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:33:43,100 p=141529 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:33:43,200 p=141529 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:33:43,217 p=141529 u=gpadmin n=ansible | TASK [version_control : Extract Python version from Ray container] *************************************************************
2025-07-07 10:33:43,279 p=141529 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:33:43,295 p=141529 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:33:43,299 p=141529 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:33:43,318 p=141529 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:33:43,335 p=141529 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:33:43,345 p=141529 u=gpadmin n=ansible | TASK [version_control : Display Python version compatibility] ******************************************************************
2025-07-07 10:33:43,407 p=141529 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Ray container Python version: Cannot retrieve result as auto_remove is enabled",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:33:43,409 p=141529 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Ray container Python version: Cannot retrieve result as auto_remove is enabled",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:33:43,422 p=141529 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Ray container Python version: Cannot retrieve result as auto_remove is enabled",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:33:43,438 p=141529 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Ray container Python version: Cannot retrieve result as auto_remove is enabled",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:33:43,462 p=141529 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Ray container Python version: Cannot retrieve result as auto_remove is enabled",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:33:43,472 p=141529 u=gpadmin n=ansible | TASK [version_control : Force pull latest Ray image if enforcing consistency] **************************************************
2025-07-07 10:33:44,818 p=141529 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:33:44,821 p=141529 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:33:44,823 p=141529 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:33:44,825 p=141529 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:33:44,855 p=141529 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:33:44,870 p=141529 u=gpadmin n=ansible | TASK [version_control : Create version info file] ******************************************************************************
2025-07-07 10:33:45,607 p=141529 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:33:45,608 p=141529 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:33:45,623 p=141529 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:33:45,634 p=141529 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:33:46,122 p=141529 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:33:46,132 p=141529 u=gpadmin n=ansible | TASK [version_control : Display version enforcement status] ********************************************************************
2025-07-07 10:33:46,176 p=141529 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:33:46,192 p=141529 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:33:46,215 p=141529 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:33:46,216 p=141529 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:33:46,234 p=141529 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:33:46,359 p=141529 u=gpadmin n=ansible | TASK [Check Ray container version] *********************************************************************************************
2025-07-07 10:33:46,611 p=141529 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:33:46,629 p=141529 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:33:46,644 p=141529 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:33:46,683 p=141529 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:33:46,795 p=141529 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:33:46,811 p=141529 u=gpadmin n=ansible | TASK [Get final Ray image version] *********************************************************************************************
2025-07-07 10:33:46,845 p=141529 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:33:46,875 p=141529 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:33:46,891 p=141529 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:33:46,892 p=141529 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:33:46,903 p=141529 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:33:46,912 p=141529 u=gpadmin n=ansible | TASK [Display final version status] ********************************************************************************************
2025-07-07 10:33:46,945 p=141529 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Node: MASTER",
        "Type: Head",
        "Ray Image: rayproject/ray:2.47.1",
        "Status: SYNCHRONIZED"
    ]
}
2025-07-07 10:33:46,960 p=141529 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Node: G-241",
        "Type: Worker",
        "Ray Image: rayproject/ray:2.47.1",
        "Status: SYNCHRONIZED"
    ]
}
2025-07-07 10:33:46,975 p=141529 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Node: G-242",
        "Type: Worker",
        "Ray Image: rayproject/ray:2.47.1",
        "Status: SYNCHRONIZED"
    ]
}
2025-07-07 10:33:46,980 p=141529 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Node: G-243",
        "Type: Worker",
        "Ray Image: rayproject/ray:2.47.1",
        "Status: SYNCHRONIZED"
    ]
}
2025-07-07 10:33:46,995 p=141529 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Node: G-244",
        "Type: Worker",
        "Ray Image: rayproject/ray:2.47.1",
        "Status: SYNCHRONIZED"
    ]
}
2025-07-07 10:33:47,004 p=141529 u=gpadmin n=ansible | TASK [Read version info file] **************************************************************************************************
2025-07-07 10:33:47,267 p=141529 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:33:47,272 p=141529 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:33:47,277 p=141529 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:33:47,278 p=141529 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:33:47,378 p=141529 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:33:47,391 p=141529 u=gpadmin n=ansible | TASK [Display version info] ****************************************************************************************************
2025-07-07 10:33:47,436 p=141529 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "# Ray Cluster Version Information",
        "# Generated on: 2025-07-07T14:33:38Z",
        "",
        "RAY_VERSION=2.47.1",
        "RAY_DOCKER_IMAGE=rayproject/ray:2.47.1",
        "PYTHON_VERSION=3.11",
        "NODE_TYPE=head",
        "HOSTNAME=MASTER",
        "LAST_UPDATE=2025-07-07T14:33:38Z",
        ""
    ]
}
2025-07-07 10:33:47,465 p=141529 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "# Ray Cluster Version Information",
        "# Generated on: 2025-07-07T14:33:39Z",
        "",
        "RAY_VERSION=2.47.1",
        "RAY_DOCKER_IMAGE=rayproject/ray:2.47.1",
        "PYTHON_VERSION=3.11",
        "NODE_TYPE=worker",
        "HOSTNAME=G-241",
        "LAST_UPDATE=2025-07-07T14:33:39Z",
        ""
    ]
}
2025-07-07 10:33:47,475 p=141529 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "# Ray Cluster Version Information",
        "# Generated on: 2025-07-07T14:33:39Z",
        "",
        "RAY_VERSION=2.47.1",
        "RAY_DOCKER_IMAGE=rayproject/ray:2.47.1",
        "PYTHON_VERSION=3.11",
        "NODE_TYPE=worker",
        "HOSTNAME=G-242",
        "LAST_UPDATE=2025-07-07T14:33:39Z",
        ""
    ]
}
2025-07-07 10:33:47,489 p=141529 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "# Ray Cluster Version Information",
        "# Generated on: 2025-07-07T14:33:39Z",
        "",
        "RAY_VERSION=2.47.1",
        "RAY_DOCKER_IMAGE=rayproject/ray:2.47.1",
        "PYTHON_VERSION=3.11",
        "NODE_TYPE=worker",
        "HOSTNAME=G-243",
        "LAST_UPDATE=2025-07-07T14:33:39Z",
        ""
    ]
}
2025-07-07 10:33:47,504 p=141529 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "# Ray Cluster Version Information",
        "# Generated on: 2025-07-07T14:33:39Z",
        "",
        "RAY_VERSION=2.47.1",
        "RAY_DOCKER_IMAGE=rayproject/ray:2.47.1",
        "PYTHON_VERSION=3.11",
        "NODE_TYPE=worker",
        "HOSTNAME=G-244",
        "LAST_UPDATE=2025-07-07T14:33:39Z",
        ""
    ]
}
2025-07-07 10:33:47,569 p=141529 u=gpadmin n=ansible | PLAY [Generate Version Synchronization Report] *********************************************************************************
2025-07-07 10:33:47,573 p=141529 u=gpadmin n=ansible | TASK [Create synchronization report] *******************************************************************************************
2025-07-07 10:33:47,621 p=141529 u=gpadmin n=ansible | fatal: [localhost]: FAILED! => {"msg": "The task includes an option with an undefined variable. The error was: 'ansible_date_time' is undefined. 'ansible_date_time' is undefined\n\nThe error appears to be in '/home/gpadmin/cursor-projects/02-Ray-Deploy/sync-versions.yml': line 63, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n  tasks:\n    - name: Create synchronization report\n      ^ here\n"}
2025-07-07 10:33:47,621 p=141529 u=gpadmin n=ansible | PLAY RECAP *********************************************************************************************************************
2025-07-07 10:33:47,622 p=141529 u=gpadmin n=ansible | G-241                      : ok=17   changed=2    unreachable=0    failed=0    skipped=4    rescued=0    ignored=0   
2025-07-07 10:33:47,622 p=141529 u=gpadmin n=ansible | G-242                      : ok=17   changed=2    unreachable=0    failed=0    skipped=4    rescued=0    ignored=0   
2025-07-07 10:33:47,622 p=141529 u=gpadmin n=ansible | G-243                      : ok=17   changed=2    unreachable=0    failed=0    skipped=4    rescued=0    ignored=0   
2025-07-07 10:33:47,622 p=141529 u=gpadmin n=ansible | G-244                      : ok=17   changed=2    unreachable=0    failed=0    skipped=4    rescued=0    ignored=0   
2025-07-07 10:33:47,622 p=141529 u=gpadmin n=ansible | MASTER                     : ok=17   changed=2    unreachable=0    failed=0    skipped=4    rescued=0    ignored=0   
2025-07-07 10:33:47,622 p=141529 u=gpadmin n=ansible | localhost                  : ok=0    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   
2025-07-07 10:34:05,689 p=142833 u=gpadmin n=ansible | MASTER | FAILED | rc=-1 >>
template error while templating string: unexpected '.'. String: docker inspect ray_head --format="{{.Config.Image}}". unexpected '.'

2025-07-07 10:34:06,260 p=142846 u=gpadmin n=ansible | G-241 | FAILED | rc=-1 >>
template error while templating string: unexpected '.'. String: docker inspect ray_worker --format="{{.Config.Image}}". unexpected '.'

2025-07-07 10:34:06,260 p=142846 u=gpadmin n=ansible | G-242 | FAILED | rc=-1 >>
template error while templating string: unexpected '.'. String: docker inspect ray_worker --format="{{.Config.Image}}". unexpected '.'

2025-07-07 10:34:06,262 p=142846 u=gpadmin n=ansible | G-243 | FAILED | rc=-1 >>
template error while templating string: unexpected '.'. String: docker inspect ray_worker --format="{{.Config.Image}}". unexpected '.'

2025-07-07 10:34:06,270 p=142846 u=gpadmin n=ansible | G-244 | FAILED | rc=-1 >>
template error while templating string: unexpected '.'. String: docker inspect ray_worker --format="{{.Config.Image}}". unexpected '.'

2025-07-07 10:34:07,804 p=142873 u=gpadmin n=ansible | G-241 | SUCCESS => {
    "ansible_facts": {
        "ansible_all_ipv4_addresses": [
            "172.17.0.1",
            "192.168.40.241"
        ],
        "ansible_all_ipv6_addresses": [
            "fe80::1c43:a9ff:fe61:378e",
            "2001:1970:5641:d600:4836:7493:513e:4e2d",
            "2001:1970:5641:d600:642:1aff:fe03:b000",
            "fe80::642:1aff:fe03:b000"
        ],
        "ansible_apparmor": {
            "status": "enabled"
        },
        "ansible_architecture": "x86_64",
        "ansible_bios_date": "04/27/2021",
        "ansible_bios_vendor": "American Megatrends Inc.",
        "ansible_bios_version": "0820",
        "ansible_board_asset_tag": "Default string",
        "ansible_board_name": "TUF GAMING B560M-PLUS WIFI",
        "ansible_board_serial": "NA",
        "ansible_board_vendor": "ASUSTeK COMPUTER INC.",
        "ansible_board_version": "Rev 1.xx",
        "ansible_chassis_asset_tag": "Default string",
        "ansible_chassis_serial": "NA",
        "ansible_chassis_vendor": "Default string",
        "ansible_chassis_version": "Default string",
        "ansible_cmdline": {
            "BOOT_IMAGE": "/vmlinuz-6.8.0-63-generic",
            "ro": true,
            "root": "/dev/mapper/ubuntu--vg-ubuntu--lv"
        },
        "ansible_date_time": {
            "date": "2025-07-07",
            "day": "07",
            "epoch": "1751898847",
            "epoch_int": "1751898847",
            "hour": "14",
            "iso8601": "2025-07-07T14:34:07Z",
            "iso8601_basic": "20250707T143407341822",
            "iso8601_basic_short": "20250707T143407",
            "iso8601_micro": "2025-07-07T14:34:07.341822Z",
            "minute": "34",
            "month": "07",
            "second": "07",
            "time": "14:34:07",
            "tz": "UTC",
            "tz_dst": "UTC",
            "tz_offset": "+0000",
            "weekday": "Monday",
            "weekday_number": "1",
            "weeknumber": "27",
            "year": "2025"
        },
        "ansible_default_ipv4": {
            "address": "192.168.40.241",
            "alias": "enp4s0",
            "broadcast": "192.168.40.255",
            "gateway": "192.168.40.1",
            "interface": "enp4s0",
            "macaddress": "04:42:1a:03:b0:00",
            "mtu": 1500,
            "netmask": "255.255.255.0",
            "network": "192.168.40.0",
            "prefix": "24",
            "type": "ether"
        },
        "ansible_default_ipv6": {
            "address": "2001:1970:5641:d600:4836:7493:513e:4e2d",
            "gateway": "fe80::a622:49ff:feeb:de0c",
            "interface": "enp4s0",
            "macaddress": "04:42:1a:03:b0:00",
            "mtu": 1500,
            "prefix": "128",
            "scope": "global",
            "type": "ether"
        },
        "ansible_device_links": {
            "ids": {
                "dm-0": [
                    "dm-name-ubuntu--vg-ubuntu--lv",
                    "dm-uuid-LVM-T83N1AFy9h5FBwRtKIbBLQFkEG9PnHP0TLrAXfRA0KunPAXMMGRqanByeYQ99l96"
                ],
                "nvme0n1": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B_1",
                    "nvme-eui.002538ba115191f3"
                ],
                "nvme0n1p1": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B-part1",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B_1-part1",
                    "nvme-eui.002538ba115191f3-part1"
                ],
                "nvme0n1p2": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B-part2",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B_1-part2",
                    "nvme-eui.002538ba115191f3-part2"
                ],
                "nvme0n1p3": [
                    "lvm-pv-uuid-UGtxc2-yT5w-r7la-gbRz-KT83-e20B-6Y0nxw",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B-part3",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B_1-part3",
                    "nvme-eui.002538ba115191f3-part3"
                ]
            },
            "labels": {},
            "masters": {
                "nvme0n1p3": [
                    "dm-0"
                ]
            },
            "uuids": {
                "dm-0": [
                    "ea0fb1e1-b3b4-4251-a5dd-9c742e00937b"
                ],
                "nvme0n1p1": [
                    "D929-BB3C"
                ],
                "nvme0n1p2": [
                    "406133b8-e7b7-495f-91e3-cbf209ff3cd7"
                ]
            }
        },
        "ansible_devices": {
            "dm-0": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [
                        "dm-name-ubuntu--vg-ubuntu--lv",
                        "dm-uuid-LVM-T83N1AFy9h5FBwRtKIbBLQFkEG9PnHP0TLrAXfRA0KunPAXMMGRqanByeYQ99l96"
                    ],
                    "labels": [],
                    "masters": [],
                    "uuids": [
                        "ea0fb1e1-b3b4-4251-a5dd-9c742e00937b"
                    ]
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "",
                "sectors": "3865477120",
                "sectorsize": "512",
                "size": "1.80 TB",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop0": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "151352",
                "sectorsize": "512",
                "size": "73.90 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop1": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "287864",
                "sectorsize": "512",
                "size": "140.56 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop2": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "156840",
                "sectorsize": "512",
                "size": "76.58 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop3": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "104240",
                "sectorsize": "512",
                "size": "50.90 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop4": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop5": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop6": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop7": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "nvme0n1": {
                "holders": [],
                "host": "Non-Volatile memory controller: Samsung Electronics Co Ltd NVMe SSD Controller PM9A1/PM9A3/980PRO",
                "links": {
                    "ids": [
                        "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B",
                        "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B_1",
                        "nvme-eui.002538ba115191f3"
                    ],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": "Samsung SSD 980 PRO 2TB",
                "partitions": {
                    "nvme0n1p1": {
                        "holders": [],
                        "links": {
                            "ids": [
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B-part1",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B_1-part1",
                                "nvme-eui.002538ba115191f3-part1"
                            ],
                            "labels": [],
                            "masters": [],
                            "uuids": [
                                "D929-BB3C"
                            ]
                        },
                        "sectors": "2201600",
                        "sectorsize": 512,
                        "size": "1.05 GB",
                        "start": "2048",
                        "uuid": "D929-BB3C"
                    },
                    "nvme0n1p2": {
                        "holders": [],
                        "links": {
                            "ids": [
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B-part2",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B_1-part2",
                                "nvme-eui.002538ba115191f3-part2"
                            ],
                            "labels": [],
                            "masters": [],
                            "uuids": [
                                "406133b8-e7b7-495f-91e3-cbf209ff3cd7"
                            ]
                        },
                        "sectors": "4194304",
                        "sectorsize": 512,
                        "size": "2.00 GB",
                        "start": "2203648",
                        "uuid": "406133b8-e7b7-495f-91e3-cbf209ff3cd7"
                    },
                    "nvme0n1p3": {
                        "holders": [
                            "ubuntu--vg-ubuntu--lv"
                        ],
                        "links": {
                            "ids": [
                                "lvm-pv-uuid-UGtxc2-yT5w-r7la-gbRz-KT83-e20B-6Y0nxw",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B-part3",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA43856B_1-part3",
                                "nvme-eui.002538ba115191f3-part3"
                            ],
                            "labels": [],
                            "masters": [
                                "dm-0"
                            ],
                            "uuids": []
                        },
                        "sectors": "3900628992",
                        "sectorsize": 512,
                        "size": "1.82 TB",
                        "start": "6397952",
                        "uuid": null
                    }
                },
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "3907029168",
                "sectorsize": "512",
                "size": "1.82 TB",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            }
        },
        "ansible_distribution": "Ubuntu",
        "ansible_distribution_file_parsed": true,
        "ansible_distribution_file_path": "/etc/os-release",
        "ansible_distribution_file_variety": "Debian",
        "ansible_distribution_major_version": "24",
        "ansible_distribution_release": "noble",
        "ansible_distribution_version": "24.04",
        "ansible_dns": {
            "nameservers": [
                "127.0.0.53"
            ],
            "options": {
                "edns0": true,
                "trust-ad": true
            },
            "search": [
                "cgocable.net"
            ]
        },
        "ansible_docker0": {
            "active": false,
            "device": "docker0",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "off [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "on",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "on",
                "tx_fcoe_segmentation": "on",
                "tx_gre_csum_segmentation": "on",
                "tx_gre_segmentation": "on",
                "tx_gso_list": "on",
                "tx_gso_partial": "on",
                "tx_gso_robust": "on",
                "tx_ipxip4_segmentation": "on",
                "tx_ipxip6_segmentation": "on",
                "tx_lockless": "on [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "on",
                "tx_scatter_gather_fraglist": "on",
                "tx_sctp_segmentation": "on",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "on",
                "tx_tcp_mangleid_segmentation": "on",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "on",
                "tx_udp_segmentation": "on",
                "tx_udp_tnl_csum_segmentation": "on",
                "tx_udp_tnl_segmentation": "on",
                "tx_vlan_offload": "on",
                "tx_vlan_stag_hw_insert": "on",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "id": "8000.1e43a961378e",
            "interfaces": [],
            "ipv4": {
                "address": "172.17.0.1",
                "broadcast": "172.17.255.255",
                "netmask": "255.255.0.0",
                "network": "172.17.0.0",
                "prefix": "16"
            },
            "ipv6": [
                {
                    "address": "fe80::1c43:a9ff:fe61:378e",
                    "prefix": "64",
                    "scope": "link"
                }
            ],
            "macaddress": "1e:43:a9:61:37:8e",
            "mtu": 1500,
            "promisc": false,
            "speed": -1,
            "stp": false,
            "timestamping": [],
            "type": "bridge"
        },
        "ansible_domain": "",
        "ansible_effective_group_id": 1000,
        "ansible_effective_user_id": 1000,
        "ansible_enp4s0": {
            "active": true,
            "device": "enp4s0",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "off [requested on]",
                "highdma": "on [fixed]",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "off [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off",
                "rx_checksumming": "on",
                "rx_fcs": "off",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "on",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "off",
                "tcp_segmentation_offload": "off",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "off [fixed]",
                "tx_checksum_ipv4": "on",
                "tx_checksum_ipv6": "on",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "off [fixed]",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "off [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "off",
                "tx_scatter_gather_fraglist": "off [fixed]",
                "tx_sctp_segmentation": "off [fixed]",
                "tx_tcp6_segmentation": "off",
                "tx_tcp_ecn_segmentation": "off [fixed]",
                "tx_tcp_mangleid_segmentation": "off",
                "tx_tcp_segmentation": "off",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "off [fixed]",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "on",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "ipv4": {
                "address": "192.168.40.241",
                "broadcast": "192.168.40.255",
                "netmask": "255.255.255.0",
                "network": "192.168.40.0",
                "prefix": "24"
            },
            "ipv6": [
                {
                    "address": "2001:1970:5641:d600:4836:7493:513e:4e2d",
                    "prefix": "128",
                    "scope": "global"
                },
                {
                    "address": "2001:1970:5641:d600:642:1aff:fe03:b000",
                    "prefix": "64",
                    "scope": "global"
                },
                {
                    "address": "fe80::642:1aff:fe03:b000",
                    "prefix": "64",
                    "scope": "link"
                }
            ],
            "macaddress": "04:42:1a:03:b0:00",
            "module": "r8169",
            "mtu": 1500,
            "pciid": "0000:04:00.0",
            "promisc": false,
            "speed": 1000,
            "timestamping": [],
            "type": "ether"
        },
        "ansible_env": {
            "DBUS_SESSION_BUS_ADDRESS": "unix:path=/run/user/1000/bus",
            "HOME": "/home/gpadmin",
            "LANG": "en_US.UTF-8",
            "LOGNAME": "gpadmin",
            "PATH": "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin",
            "PWD": "/home/gpadmin",
            "SHELL": "/bin/bash",
            "SHLVL": "0",
            "SSH_CLIENT": "192.168.40.240 60790 22",
            "SSH_CONNECTION": "192.168.40.240 60790 192.168.40.241 22",
            "USER": "gpadmin",
            "XDG_RUNTIME_DIR": "/run/user/1000",
            "XDG_SESSION_CLASS": "user",
            "XDG_SESSION_ID": "365",
            "XDG_SESSION_TYPE": "tty",
            "_": "/bin/sh"
        },
        "ansible_fibre_channel_wwn": [],
        "ansible_fips": false,
        "ansible_form_factor": "Desktop",
        "ansible_fqdn": "g-241",
        "ansible_hostname": "g-241",
        "ansible_hostnqn": "",
        "ansible_interfaces": [
            "docker0",
            "lo",
            "enp4s0",
            "wlo1"
        ],
        "ansible_is_chroot": false,
        "ansible_iscsi_iqn": "",
        "ansible_kernel": "6.8.0-63-generic",
        "ansible_kernel_version": "#66-Ubuntu SMP PREEMPT_DYNAMIC Fri Jun 13 20:25:30 UTC 2025",
        "ansible_lo": {
            "active": true,
            "device": "lo",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on [fixed]",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "on [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "on [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "on [fixed]",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "on [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "on",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "on [fixed]",
                "tx_nocache_copy": "off [fixed]",
                "tx_scatter_gather": "on [fixed]",
                "tx_scatter_gather_fraglist": "on [fixed]",
                "tx_sctp_segmentation": "on",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "on",
                "tx_tcp_mangleid_segmentation": "on",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "on",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "off [fixed]",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "on [fixed]"
            },
            "hw_timestamp_filters": [],
            "ipv4": {
                "address": "127.0.0.1",
                "broadcast": "",
                "netmask": "255.0.0.0",
                "network": "127.0.0.0",
                "prefix": "8"
            },
            "ipv6": [
                {
                    "address": "::1",
                    "prefix": "128",
                    "scope": "host"
                }
            ],
            "mtu": 65536,
            "promisc": false,
            "timestamping": [],
            "type": "loopback"
        },
        "ansible_loadavg": {
            "15m": 0.0556640625,
            "1m": 0.0576171875,
            "5m": 0.06103515625
        },
        "ansible_local": {},
        "ansible_locally_reachable_ips": {
            "ipv4": [
                "127.0.0.0/8",
                "127.0.0.1",
                "172.17.0.1",
                "192.168.40.241"
            ],
            "ipv6": [
                "::1",
                "2001:1970:5641:d600:642:1aff:fe03:b000",
                "2001:1970:5641:d600:4836:7493:513e:4e2d",
                "fe80::642:1aff:fe03:b000",
                "fe80::1c43:a9ff:fe61:378e"
            ]
        },
        "ansible_lsb": {
            "codename": "noble",
            "description": "Ubuntu 24.04.2 LTS",
            "id": "Ubuntu",
            "major_release": "24",
            "release": "24.04"
        },
        "ansible_lvm": "N/A",
        "ansible_machine": "x86_64",
        "ansible_machine_id": "18f3e9bf9479424da7af229bf8cd18df",
        "ansible_memfree_mb": 120357,
        "ansible_memory_mb": {
            "nocache": {
                "free": 127336,
                "used": 1311
            },
            "real": {
                "free": 120357,
                "total": 128647,
                "used": 8290
            },
            "swap": {
                "cached": 0,
                "free": 8191,
                "total": 8191,
                "used": 0
            }
        },
        "ansible_memtotal_mb": 128647,
        "ansible_mounts": [
            {
                "block_available": 446298965,
                "block_size": 4096,
                "block_total": 475319188,
                "block_used": 29020223,
                "device": "/dev/mapper/ubuntu--vg-ubuntu--lv",
                "fstype": "ext4",
                "inode_available": 120527914,
                "inode_total": 120799232,
                "inode_used": 271318,
                "mount": "/",
                "options": "rw,relatime",
                "size_available": 1828040560640,
                "size_total": 1946907394048,
                "uuid": "ea0fb1e1-b3b4-4251-a5dd-9c742e00937b"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 592,
                "block_used": 592,
                "device": "/dev/loop0",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 14261,
                "inode_used": 14261,
                "mount": "/snap/core22/2010",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 77594624,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 1125,
                "block_used": 1125,
                "device": "/dev/loop1",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 3532,
                "inode_used": 3532,
                "mount": "/snap/docker/3265",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 147456000,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 613,
                "block_used": 613,
                "device": "/dev/loop2",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 967,
                "inode_used": 967,
                "mount": "/snap/powershell/294",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 80347136,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 408,
                "block_used": 408,
                "device": "/dev/loop3",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 608,
                "inode_used": 608,
                "mount": "/snap/snapd/24718",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 53477376,
                "uuid": "N/A"
            },
            {
                "block_available": 435218,
                "block_size": 4096,
                "block_total": 498138,
                "block_used": 62920,
                "device": "/dev/nvme0n1p2",
                "fstype": "ext4",
                "inode_available": 130762,
                "inode_total": 131072,
                "inode_used": 310,
                "mount": "/boot",
                "options": "rw,relatime",
                "size_available": 1782652928,
                "size_total": 2040373248,
                "uuid": "406133b8-e7b7-495f-91e3-cbf209ff3cd7"
            },
            {
                "block_available": 273086,
                "block_size": 4096,
                "block_total": 274658,
                "block_used": 1572,
                "device": "/dev/nvme0n1p1",
                "fstype": "vfat",
                "inode_available": 0,
                "inode_total": 0,
                "inode_used": 0,
                "mount": "/boot/efi",
                "options": "rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=iso8859-1,shortname=mixed,errors=remount-ro",
                "size_available": 1118560256,
                "size_total": 1124999168,
                "uuid": "D929-BB3C"
            }
        ],
        "ansible_nodename": "g-241",
        "ansible_os_family": "Debian",
        "ansible_pkg_mgr": "apt",
        "ansible_proc_cmdline": {
            "BOOT_IMAGE": "/vmlinuz-6.8.0-63-generic",
            "ro": true,
            "root": "/dev/mapper/ubuntu--vg-ubuntu--lv"
        },
        "ansible_processor": [
            "0",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "1",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "2",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "3",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "4",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "5",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "6",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "7",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "8",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "9",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "10",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "11",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "12",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "13",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "14",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "15",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz"
        ],
        "ansible_processor_cores": 8,
        "ansible_processor_count": 1,
        "ansible_processor_nproc": 16,
        "ansible_processor_threads_per_core": 2,
        "ansible_processor_vcpus": 16,
        "ansible_product_name": "System Product Name",
        "ansible_product_serial": "NA",
        "ansible_product_uuid": "NA",
        "ansible_product_version": "System Version",
        "ansible_python": {
            "executable": "/usr/bin/python3",
            "has_sslcontext": true,
            "type": "cpython",
            "version": {
                "major": 3,
                "micro": 3,
                "minor": 12,
                "releaselevel": "final",
                "serial": 0
            },
            "version_info": [
                3,
                12,
                3,
                "final",
                0
            ]
        },
        "ansible_python_version": "3.12.3",
        "ansible_real_group_id": 1000,
        "ansible_real_user_id": 1000,
        "ansible_selinux": {
            "status": "disabled"
        },
        "ansible_selinux_python_present": true,
        "ansible_service_mgr": "systemd",
        "ansible_ssh_host_key_ecdsa_public": "AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBBuqBsqa3aRbume+nAbBXlU1ajvjRopWxuJyZoRReqvWQpKPjxGmwn1k8/Dl5Q86I369gpQ6umty8UGwNpPp3A8=",
        "ansible_ssh_host_key_ecdsa_public_keytype": "ecdsa-sha2-nistp256",
        "ansible_ssh_host_key_ed25519_public": "AAAAC3NzaC1lZDI1NTE5AAAAILc41i6Mi2mvledIODTYZd2APNOX0WyF4aksqtBgscPh",
        "ansible_ssh_host_key_ed25519_public_keytype": "ssh-ed25519",
        "ansible_ssh_host_key_rsa_public": "AAAAB3NzaC1yc2EAAAADAQABAAABgQCehmbC0O0fS9Bjk8kIG4LrPE6lzkRU9hKDWsmjbA0rfCf38ZiHug+fZNsbGqTwslp9LErcBcPV/wIcTk+RpJP+Zv7pCiysfIYb1k6vykTQ7zOWR+Rmj74Xq9ilDL1qPPJfFZ6iBqGpZC5kRuOlmSNDNnsUuxKYY1dQsflNHyTkSwVigISXvHVJcSuSeJiiEpgm/osEryYh/Arysp1VByQn37cyaGUTUz2nM7k+ubNT3gee3chQ2Z36Q7YPYPMWuwpaYiWqOXKqgvAXoHyoU87t322RXZnVAqcJzGwbcTv1aflA9BGqSxWXNybSRJ6MezMEOKC/k9USZHJwvjvnTluilbnWxCXRDXUROPaga4LHG/UZadnn4nHeukdtbrKoy4h3dvK45zoI6RC0uG6pheHlaAfwAmnNTgmSbQtFnTAh/Je41U17UhNbFoWKtNaPJW2J/mxqtP+IDx1MjIS38YWMtkrdP8lxbRjx6VwM+MbCJBj4V2zdJsh2g6NrInqFaT0=",
        "ansible_ssh_host_key_rsa_public_keytype": "ssh-rsa",
        "ansible_swapfree_mb": 8191,
        "ansible_swaptotal_mb": 8191,
        "ansible_system": "Linux",
        "ansible_system_capabilities": [
            ""
        ],
        "ansible_system_capabilities_enforced": "True",
        "ansible_system_vendor": "ASUS",
        "ansible_uptime_seconds": 167047,
        "ansible_user_dir": "/home/gpadmin",
        "ansible_user_gecos": "gpadmin",
        "ansible_user_gid": 1000,
        "ansible_user_id": "gpadmin",
        "ansible_user_shell": "/bin/bash",
        "ansible_user_uid": 1000,
        "ansible_userspace_architecture": "x86_64",
        "ansible_userspace_bits": "64",
        "ansible_virtualization_role": "NA",
        "ansible_virtualization_tech_guest": [],
        "ansible_virtualization_tech_host": [],
        "ansible_virtualization_type": "NA",
        "ansible_wlo1": {
            "active": false,
            "device": "wlo1",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "on [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "off [fixed]",
                "tx_checksum_ipv4": "on",
                "tx_checksum_ipv6": "on",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "off [fixed]",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "off [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "on",
                "tx_scatter_gather_fraglist": "off [fixed]",
                "tx_sctp_segmentation": "off [fixed]",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "off [fixed]",
                "tx_tcp_mangleid_segmentation": "off",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "off [fixed]",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "off [fixed]",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "macaddress": "ec:63:d7:79:01:76",
            "module": "iwlwifi",
            "mtu": 1500,
            "pciid": "0000:00:14.3",
            "promisc": false,
            "timestamping": [],
            "type": "ether"
        },
        "discovered_interpreter_python": "/usr/bin/python3",
        "gather_subset": [
            "all"
        ],
        "module_setup": true
    },
    "changed": false
}
2025-07-07 10:34:07,813 p=142873 u=gpadmin n=ansible | G-243 | SUCCESS => {
    "ansible_facts": {
        "ansible_all_ipv4_addresses": [
            "172.17.0.1",
            "192.168.40.243"
        ],
        "ansible_all_ipv6_addresses": [
            "fe80::f80e:62ff:fe4d:2c97",
            "2001:1970:5641:d600:3d69:fc23:44b4:3add",
            "2001:1970:5641:d600:642:1aff:fe03:afed",
            "fe80::642:1aff:fe03:afed"
        ],
        "ansible_apparmor": {
            "status": "enabled"
        },
        "ansible_architecture": "x86_64",
        "ansible_bios_date": "04/27/2021",
        "ansible_bios_vendor": "American Megatrends Inc.",
        "ansible_bios_version": "0820",
        "ansible_board_asset_tag": "Default string",
        "ansible_board_name": "TUF GAMING B560M-PLUS WIFI",
        "ansible_board_serial": "NA",
        "ansible_board_vendor": "ASUSTeK COMPUTER INC.",
        "ansible_board_version": "Rev 1.xx",
        "ansible_chassis_asset_tag": "Default string",
        "ansible_chassis_serial": "NA",
        "ansible_chassis_vendor": "Default string",
        "ansible_chassis_version": "Default string",
        "ansible_cmdline": {
            "BOOT_IMAGE": "/vmlinuz-6.8.0-63-generic",
            "ro": true,
            "root": "/dev/mapper/ubuntu--vg-ubuntu--lv"
        },
        "ansible_date_time": {
            "date": "2025-07-07",
            "day": "07",
            "epoch": "1751898847",
            "epoch_int": "1751898847",
            "hour": "14",
            "iso8601": "2025-07-07T14:34:07Z",
            "iso8601_basic": "20250707T143407620938",
            "iso8601_basic_short": "20250707T143407",
            "iso8601_micro": "2025-07-07T14:34:07.620938Z",
            "minute": "34",
            "month": "07",
            "second": "07",
            "time": "14:34:07",
            "tz": "UTC",
            "tz_dst": "UTC",
            "tz_offset": "+0000",
            "weekday": "Monday",
            "weekday_number": "1",
            "weeknumber": "27",
            "year": "2025"
        },
        "ansible_default_ipv4": {
            "address": "192.168.40.243",
            "alias": "enp4s0",
            "broadcast": "192.168.40.255",
            "gateway": "192.168.40.1",
            "interface": "enp4s0",
            "macaddress": "04:42:1a:03:af:ed",
            "mtu": 1500,
            "netmask": "255.255.255.0",
            "network": "192.168.40.0",
            "prefix": "24",
            "type": "ether"
        },
        "ansible_default_ipv6": {
            "address": "2001:1970:5641:d600:3d69:fc23:44b4:3add",
            "gateway": "fe80::a622:49ff:feeb:de0c",
            "interface": "enp4s0",
            "macaddress": "04:42:1a:03:af:ed",
            "mtu": 1500,
            "prefix": "128",
            "scope": "global",
            "type": "ether"
        },
        "ansible_device_links": {
            "ids": {
                "dm-0": [
                    "dm-name-ubuntu--vg-ubuntu--lv",
                    "dm-uuid-LVM-qS3AbUOilNmVQ2J3EHcFZykmaBieZsi1HHQwKnhoMJ1m1OxbHRdNwadUEFPdTOoO"
                ],
                "nvme0n1": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L_1",
                    "nvme-eui.002538ba11522031"
                ],
                "nvme0n1p1": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L-part1",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L_1-part1",
                    "nvme-eui.002538ba11522031-part1"
                ],
                "nvme0n1p2": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L-part2",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L_1-part2",
                    "nvme-eui.002538ba11522031-part2"
                ],
                "nvme0n1p3": [
                    "lvm-pv-uuid-r0jOfT-EUW6-Zj8X-fPYm-7HFv-TWIP-IptT38",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L-part3",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L_1-part3",
                    "nvme-eui.002538ba11522031-part3"
                ]
            },
            "labels": {},
            "masters": {
                "nvme0n1p3": [
                    "dm-0"
                ]
            },
            "uuids": {
                "dm-0": [
                    "53bdb1e3-f590-47db-9933-46606a474418"
                ],
                "nvme0n1p1": [
                    "0F1A-B103"
                ],
                "nvme0n1p2": [
                    "662f2690-2f34-4ca9-bba9-f22e55ed237d"
                ]
            }
        },
        "ansible_devices": {
            "dm-0": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [
                        "dm-name-ubuntu--vg-ubuntu--lv",
                        "dm-uuid-LVM-qS3AbUOilNmVQ2J3EHcFZykmaBieZsi1HHQwKnhoMJ1m1OxbHRdNwadUEFPdTOoO"
                    ],
                    "labels": [],
                    "masters": [],
                    "uuids": [
                        "53bdb1e3-f590-47db-9933-46606a474418"
                    ]
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "",
                "sectors": "3886948352",
                "sectorsize": "512",
                "size": "1.81 TB",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop0": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "104240",
                "sectorsize": "512",
                "size": "50.90 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop1": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "151352",
                "sectorsize": "512",
                "size": "73.90 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop2": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "287864",
                "sectorsize": "512",
                "size": "140.56 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop3": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "156840",
                "sectorsize": "512",
                "size": "76.58 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop4": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop5": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop6": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop7": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "nvme0n1": {
                "holders": [],
                "host": "Non-Volatile memory controller: Samsung Electronics Co Ltd NVMe SSD Controller PM9A1/PM9A3/980PRO",
                "links": {
                    "ids": [
                        "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L",
                        "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L_1",
                        "nvme-eui.002538ba11522031"
                    ],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": "Samsung SSD 980 PRO 2TB",
                "partitions": {
                    "nvme0n1p1": {
                        "holders": [],
                        "links": {
                            "ids": [
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L-part1",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L_1-part1",
                                "nvme-eui.002538ba11522031-part1"
                            ],
                            "labels": [],
                            "masters": [],
                            "uuids": [
                                "0F1A-B103"
                            ]
                        },
                        "sectors": "2201600",
                        "sectorsize": 512,
                        "size": "1.05 GB",
                        "start": "2048",
                        "uuid": "0F1A-B103"
                    },
                    "nvme0n1p2": {
                        "holders": [],
                        "links": {
                            "ids": [
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L-part2",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L_1-part2",
                                "nvme-eui.002538ba11522031-part2"
                            ],
                            "labels": [],
                            "masters": [],
                            "uuids": [
                                "662f2690-2f34-4ca9-bba9-f22e55ed237d"
                            ]
                        },
                        "sectors": "4194304",
                        "sectorsize": 512,
                        "size": "2.00 GB",
                        "start": "2203648",
                        "uuid": "662f2690-2f34-4ca9-bba9-f22e55ed237d"
                    },
                    "nvme0n1p3": {
                        "holders": [
                            "ubuntu--vg-ubuntu--lv"
                        ],
                        "links": {
                            "ids": [
                                "lvm-pv-uuid-r0jOfT-EUW6-Zj8X-fPYm-7HFv-TWIP-IptT38",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L-part3",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NC0RA16437L_1-part3",
                                "nvme-eui.002538ba11522031-part3"
                            ],
                            "labels": [],
                            "masters": [
                                "dm-0"
                            ],
                            "uuids": []
                        },
                        "sectors": "3900628992",
                        "sectorsize": 512,
                        "size": "1.82 TB",
                        "start": "6397952",
                        "uuid": null
                    }
                },
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "3907029168",
                "sectorsize": "512",
                "size": "1.82 TB",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            }
        },
        "ansible_distribution": "Ubuntu",
        "ansible_distribution_file_parsed": true,
        "ansible_distribution_file_path": "/etc/os-release",
        "ansible_distribution_file_variety": "Debian",
        "ansible_distribution_major_version": "24",
        "ansible_distribution_release": "noble",
        "ansible_distribution_version": "24.04",
        "ansible_dns": {
            "nameservers": [
                "127.0.0.53"
            ],
            "options": {
                "edns0": true,
                "trust-ad": true
            },
            "search": [
                "cgocable.net"
            ]
        },
        "ansible_docker0": {
            "active": false,
            "device": "docker0",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "off [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "on",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "on",
                "tx_fcoe_segmentation": "on",
                "tx_gre_csum_segmentation": "on",
                "tx_gre_segmentation": "on",
                "tx_gso_list": "on",
                "tx_gso_partial": "on",
                "tx_gso_robust": "on",
                "tx_ipxip4_segmentation": "on",
                "tx_ipxip6_segmentation": "on",
                "tx_lockless": "on [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "on",
                "tx_scatter_gather_fraglist": "on",
                "tx_sctp_segmentation": "on",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "on",
                "tx_tcp_mangleid_segmentation": "on",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "on",
                "tx_udp_segmentation": "on",
                "tx_udp_tnl_csum_segmentation": "on",
                "tx_udp_tnl_segmentation": "on",
                "tx_vlan_offload": "on",
                "tx_vlan_stag_hw_insert": "on",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "id": "8000.fa0e624d2c97",
            "interfaces": [],
            "ipv4": {
                "address": "172.17.0.1",
                "broadcast": "172.17.255.255",
                "netmask": "255.255.0.0",
                "network": "172.17.0.0",
                "prefix": "16"
            },
            "ipv6": [
                {
                    "address": "fe80::f80e:62ff:fe4d:2c97",
                    "prefix": "64",
                    "scope": "link"
                }
            ],
            "macaddress": "fa:0e:62:4d:2c:97",
            "mtu": 1500,
            "promisc": false,
            "speed": -1,
            "stp": false,
            "timestamping": [],
            "type": "bridge"
        },
        "ansible_domain": "",
        "ansible_effective_group_id": 1000,
        "ansible_effective_user_id": 1000,
        "ansible_enp4s0": {
            "active": true,
            "device": "enp4s0",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "off [requested on]",
                "highdma": "on [fixed]",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "off [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off",
                "rx_checksumming": "on",
                "rx_fcs": "off",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "on",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "off",
                "tcp_segmentation_offload": "off",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "off [fixed]",
                "tx_checksum_ipv4": "on",
                "tx_checksum_ipv6": "on",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "off [fixed]",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "off [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "off",
                "tx_scatter_gather_fraglist": "off [fixed]",
                "tx_sctp_segmentation": "off [fixed]",
                "tx_tcp6_segmentation": "off",
                "tx_tcp_ecn_segmentation": "off [fixed]",
                "tx_tcp_mangleid_segmentation": "off",
                "tx_tcp_segmentation": "off",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "off [fixed]",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "on",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "ipv4": {
                "address": "192.168.40.243",
                "broadcast": "192.168.40.255",
                "netmask": "255.255.255.0",
                "network": "192.168.40.0",
                "prefix": "24"
            },
            "ipv6": [
                {
                    "address": "2001:1970:5641:d600:3d69:fc23:44b4:3add",
                    "prefix": "128",
                    "scope": "global"
                },
                {
                    "address": "2001:1970:5641:d600:642:1aff:fe03:afed",
                    "prefix": "64",
                    "scope": "global"
                },
                {
                    "address": "fe80::642:1aff:fe03:afed",
                    "prefix": "64",
                    "scope": "link"
                }
            ],
            "macaddress": "04:42:1a:03:af:ed",
            "module": "r8169",
            "mtu": 1500,
            "pciid": "0000:04:00.0",
            "promisc": false,
            "speed": 1000,
            "timestamping": [],
            "type": "ether"
        },
        "ansible_env": {
            "DBUS_SESSION_BUS_ADDRESS": "unix:path=/run/user/1000/bus",
            "HOME": "/home/gpadmin",
            "LANG": "en_US.UTF-8",
            "LOGNAME": "gpadmin",
            "PATH": "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin",
            "PWD": "/home/gpadmin",
            "SHELL": "/bin/bash",
            "SHLVL": "0",
            "SSH_CLIENT": "192.168.40.240 55386 22",
            "SSH_CONNECTION": "192.168.40.240 55386 192.168.40.243 22",
            "USER": "gpadmin",
            "XDG_RUNTIME_DIR": "/run/user/1000",
            "XDG_SESSION_CLASS": "user",
            "XDG_SESSION_ID": "364",
            "XDG_SESSION_TYPE": "tty",
            "_": "/bin/sh"
        },
        "ansible_fibre_channel_wwn": [],
        "ansible_fips": false,
        "ansible_form_factor": "Desktop",
        "ansible_fqdn": "g-243",
        "ansible_hostname": "g-243",
        "ansible_hostnqn": "",
        "ansible_interfaces": [
            "enp4s0",
            "wlo1",
            "docker0",
            "lo"
        ],
        "ansible_is_chroot": false,
        "ansible_iscsi_iqn": "",
        "ansible_kernel": "6.8.0-63-generic",
        "ansible_kernel_version": "#66-Ubuntu SMP PREEMPT_DYNAMIC Fri Jun 13 20:25:30 UTC 2025",
        "ansible_lo": {
            "active": true,
            "device": "lo",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on [fixed]",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "on [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "on [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "on [fixed]",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "on [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "on",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "on [fixed]",
                "tx_nocache_copy": "off [fixed]",
                "tx_scatter_gather": "on [fixed]",
                "tx_scatter_gather_fraglist": "on [fixed]",
                "tx_sctp_segmentation": "on",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "on",
                "tx_tcp_mangleid_segmentation": "on",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "on",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "off [fixed]",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "on [fixed]"
            },
            "hw_timestamp_filters": [],
            "ipv4": {
                "address": "127.0.0.1",
                "broadcast": "",
                "netmask": "255.0.0.0",
                "network": "127.0.0.0",
                "prefix": "8"
            },
            "ipv6": [
                {
                    "address": "::1",
                    "prefix": "128",
                    "scope": "host"
                }
            ],
            "mtu": 65536,
            "promisc": false,
            "timestamping": [],
            "type": "loopback"
        },
        "ansible_loadavg": {
            "15m": 0.0390625,
            "1m": 0.00048828125,
            "5m": 0.0537109375
        },
        "ansible_local": {},
        "ansible_locally_reachable_ips": {
            "ipv4": [
                "127.0.0.0/8",
                "127.0.0.1",
                "172.17.0.1",
                "192.168.40.243"
            ],
            "ipv6": [
                "::1",
                "2001:1970:5641:d600:642:1aff:fe03:afed",
                "2001:1970:5641:d600:3d69:fc23:44b4:3add",
                "fe80::642:1aff:fe03:afed",
                "fe80::f80e:62ff:fe4d:2c97"
            ]
        },
        "ansible_lsb": {
            "codename": "noble",
            "description": "Ubuntu 24.04.2 LTS",
            "id": "Ubuntu",
            "major_release": "24",
            "release": "24.04"
        },
        "ansible_lvm": "N/A",
        "ansible_machine": "x86_64",
        "ansible_machine_id": "44ece95c3f5a42beb3ae29b6c2133090",
        "ansible_memfree_mb": 120287,
        "ansible_memory_mb": {
            "nocache": {
                "free": 127399,
                "used": 1250
            },
            "real": {
                "free": 120287,
                "total": 128649,
                "used": 8362
            },
            "swap": {
                "cached": 0,
                "free": 8191,
                "total": 8191,
                "used": 0
            }
        },
        "ansible_memtotal_mb": 128649,
        "ansible_mounts": [
            {
                "block_available": 449270768,
                "block_size": 4096,
                "block_total": 477960925,
                "block_used": 28690157,
                "device": "/dev/mapper/ubuntu--vg-ubuntu--lv",
                "fstype": "ext4",
                "inode_available": 121214972,
                "inode_total": 121470976,
                "inode_used": 256004,
                "mount": "/",
                "options": "rw,relatime",
                "size_available": 1840213065728,
                "size_total": 1957727948800,
                "uuid": "53bdb1e3-f590-47db-9933-46606a474418"
            },
            {
                "block_available": 435221,
                "block_size": 4096,
                "block_total": 498138,
                "block_used": 62917,
                "device": "/dev/nvme0n1p2",
                "fstype": "ext4",
                "inode_available": 130762,
                "inode_total": 131072,
                "inode_used": 310,
                "mount": "/boot",
                "options": "rw,relatime",
                "size_available": 1782665216,
                "size_total": 2040373248,
                "uuid": "662f2690-2f34-4ca9-bba9-f22e55ed237d"
            },
            {
                "block_available": 273086,
                "block_size": 4096,
                "block_total": 274658,
                "block_used": 1572,
                "device": "/dev/nvme0n1p1",
                "fstype": "vfat",
                "inode_available": 0,
                "inode_total": 0,
                "inode_used": 0,
                "mount": "/boot/efi",
                "options": "rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=iso8859-1,shortname=mixed,errors=remount-ro",
                "size_available": 1118560256,
                "size_total": 1124999168,
                "uuid": "0F1A-B103"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 408,
                "block_used": 408,
                "device": "/dev/loop0",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 608,
                "inode_used": 608,
                "mount": "/snap/snapd/24718",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 53477376,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 592,
                "block_used": 592,
                "device": "/dev/loop1",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 14261,
                "inode_used": 14261,
                "mount": "/snap/core22/2010",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 77594624,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 1125,
                "block_used": 1125,
                "device": "/dev/loop2",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 3532,
                "inode_used": 3532,
                "mount": "/snap/docker/3265",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 147456000,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 613,
                "block_used": 613,
                "device": "/dev/loop3",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 967,
                "inode_used": 967,
                "mount": "/snap/powershell/294",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 80347136,
                "uuid": "N/A"
            }
        ],
        "ansible_nodename": "g-243",
        "ansible_os_family": "Debian",
        "ansible_pkg_mgr": "apt",
        "ansible_proc_cmdline": {
            "BOOT_IMAGE": "/vmlinuz-6.8.0-63-generic",
            "ro": true,
            "root": "/dev/mapper/ubuntu--vg-ubuntu--lv"
        },
        "ansible_processor": [
            "0",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "1",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "2",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "3",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "4",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "5",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "6",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "7",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "8",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "9",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "10",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "11",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "12",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "13",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "14",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "15",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz"
        ],
        "ansible_processor_cores": 8,
        "ansible_processor_count": 1,
        "ansible_processor_nproc": 16,
        "ansible_processor_threads_per_core": 2,
        "ansible_processor_vcpus": 16,
        "ansible_product_name": "System Product Name",
        "ansible_product_serial": "NA",
        "ansible_product_uuid": "NA",
        "ansible_product_version": "System Version",
        "ansible_python": {
            "executable": "/usr/bin/python3",
            "has_sslcontext": true,
            "type": "cpython",
            "version": {
                "major": 3,
                "micro": 3,
                "minor": 12,
                "releaselevel": "final",
                "serial": 0
            },
            "version_info": [
                3,
                12,
                3,
                "final",
                0
            ]
        },
        "ansible_python_version": "3.12.3",
        "ansible_real_group_id": 1000,
        "ansible_real_user_id": 1000,
        "ansible_selinux": {
            "status": "disabled"
        },
        "ansible_selinux_python_present": true,
        "ansible_service_mgr": "systemd",
        "ansible_ssh_host_key_ecdsa_public": "AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBNxArKv8nEmHXrZLL0PcBPnBwIEXM/+FimJTWk/WoyfiWePg/60EsBk+FxkAkel+PRpIOfvP766/dCRQSO9XMUo=",
        "ansible_ssh_host_key_ecdsa_public_keytype": "ecdsa-sha2-nistp256",
        "ansible_ssh_host_key_ed25519_public": "AAAAC3NzaC1lZDI1NTE5AAAAINkONRqrEYu7Pz8Up+L8RfNqSzS0cQhgdO8KuIG4G+wS",
        "ansible_ssh_host_key_ed25519_public_keytype": "ssh-ed25519",
        "ansible_ssh_host_key_rsa_public": "AAAAB3NzaC1yc2EAAAADAQABAAABgQDwsZ8foYneOHsHUyH28Yx2zlF4IpohG+IG8dRqho6qvQ06EwQmjl8wFr9F9/Yd0ffypAwo/ISfL0YRsvawAdgfw5m1nF+P3gmQXyhiNxmDk4gTGUbagICpTx+ehHS73emPsGczSZs11TFGV+8RAGhyWo3hYGhTzdF8znBX0NqT0k/EmP4WHVsKhq4/Y6Q/Zr5cAQ6zNVw/8voOL3CrmYeNB8PNxYO7jaE2AeTpFpzD97qs5cGYRpsmUDeMhVNlO7zh00PPNAqXxyNDedyDtGfAqSuCdO7q8pTL3cUDAMUBimIhYlS43Akn6CwnMsFloQtYV9gTzn8YvUyPJgWhjIEqYTVM5FXc5cTLLxoyHoeThxkjSLjjYPqTYovj74Fw/EdXCdPie9exF/2LnTaJmIMtPqdosidKFc32gNtJlKVKEtPW1EDbRoJa7Dr7k7U/o4oVxKCgx6K9gvZWa84AIKnrX1Nwnd2AORVI+dyUL7Rv/+JHSpFZR4R+3DAKPGXKNHU=",
        "ansible_ssh_host_key_rsa_public_keytype": "ssh-rsa",
        "ansible_swapfree_mb": 8191,
        "ansible_swaptotal_mb": 8191,
        "ansible_system": "Linux",
        "ansible_system_capabilities": [
            ""
        ],
        "ansible_system_capabilities_enforced": "True",
        "ansible_system_vendor": "ASUS",
        "ansible_uptime_seconds": 166119,
        "ansible_user_dir": "/home/gpadmin",
        "ansible_user_gecos": "gpadmin",
        "ansible_user_gid": 1000,
        "ansible_user_id": "gpadmin",
        "ansible_user_shell": "/bin/bash",
        "ansible_user_uid": 1000,
        "ansible_userspace_architecture": "x86_64",
        "ansible_userspace_bits": "64",
        "ansible_virtualization_role": "NA",
        "ansible_virtualization_tech_guest": [],
        "ansible_virtualization_tech_host": [],
        "ansible_virtualization_type": "NA",
        "ansible_wlo1": {
            "active": false,
            "device": "wlo1",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "on [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "off [fixed]",
                "tx_checksum_ipv4": "on",
                "tx_checksum_ipv6": "on",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "off [fixed]",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "off [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "on",
                "tx_scatter_gather_fraglist": "off [fixed]",
                "tx_sctp_segmentation": "off [fixed]",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "off [fixed]",
                "tx_tcp_mangleid_segmentation": "off",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "off [fixed]",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "off [fixed]",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "macaddress": "ec:63:d7:3d:3c:ae",
            "module": "iwlwifi",
            "mtu": 1500,
            "pciid": "0000:00:14.3",
            "promisc": false,
            "timestamping": [],
            "type": "ether"
        },
        "discovered_interpreter_python": "/usr/bin/python3",
        "gather_subset": [
            "all"
        ],
        "module_setup": true
    },
    "changed": false
}
2025-07-07 10:34:07,843 p=142873 u=gpadmin n=ansible | G-242 | SUCCESS => {
    "ansible_facts": {
        "ansible_all_ipv4_addresses": [
            "172.17.0.1",
            "192.168.40.242"
        ],
        "ansible_all_ipv6_addresses": [
            "fe80::8849:4ff:feea:a252",
            "2001:1970:5641:d600:415f:3010:6d0b:a5d8",
            "2001:1970:5641:d600:642:1aff:fe03:afb5",
            "fe80::642:1aff:fe03:afb5"
        ],
        "ansible_apparmor": {
            "status": "enabled"
        },
        "ansible_architecture": "x86_64",
        "ansible_bios_date": "04/27/2021",
        "ansible_bios_vendor": "American Megatrends Inc.",
        "ansible_bios_version": "0820",
        "ansible_board_asset_tag": "Default string",
        "ansible_board_name": "TUF GAMING B560M-PLUS WIFI",
        "ansible_board_serial": "NA",
        "ansible_board_vendor": "ASUSTeK COMPUTER INC.",
        "ansible_board_version": "Rev 1.xx",
        "ansible_chassis_asset_tag": "Default string",
        "ansible_chassis_serial": "NA",
        "ansible_chassis_vendor": "Default string",
        "ansible_chassis_version": "Default string",
        "ansible_cmdline": {
            "BOOT_IMAGE": "/vmlinuz-6.8.0-63-generic",
            "ro": true,
            "root": "/dev/mapper/ubuntu--vg-ubuntu--lv"
        },
        "ansible_date_time": {
            "date": "2025-07-07",
            "day": "07",
            "epoch": "1751898847",
            "epoch_int": "1751898847",
            "hour": "14",
            "iso8601": "2025-07-07T14:34:07Z",
            "iso8601_basic": "20250707T143407369038",
            "iso8601_basic_short": "20250707T143407",
            "iso8601_micro": "2025-07-07T14:34:07.369038Z",
            "minute": "34",
            "month": "07",
            "second": "07",
            "time": "14:34:07",
            "tz": "UTC",
            "tz_dst": "UTC",
            "tz_offset": "+0000",
            "weekday": "Monday",
            "weekday_number": "1",
            "weeknumber": "27",
            "year": "2025"
        },
        "ansible_default_ipv4": {
            "address": "192.168.40.242",
            "alias": "enp4s0",
            "broadcast": "192.168.40.255",
            "gateway": "192.168.40.1",
            "interface": "enp4s0",
            "macaddress": "04:42:1a:03:af:b5",
            "mtu": 1500,
            "netmask": "255.255.255.0",
            "network": "192.168.40.0",
            "prefix": "24",
            "type": "ether"
        },
        "ansible_default_ipv6": {
            "address": "2001:1970:5641:d600:415f:3010:6d0b:a5d8",
            "gateway": "fe80::a622:49ff:feeb:de0c",
            "interface": "enp4s0",
            "macaddress": "04:42:1a:03:af:b5",
            "mtu": 1500,
            "prefix": "128",
            "scope": "global",
            "type": "ether"
        },
        "ansible_device_links": {
            "ids": {
                "dm-0": [
                    "dm-name-ubuntu--vg-ubuntu--lv",
                    "dm-uuid-LVM-mU5ZJgEXHxAEfaalRChL1VbocG0rDBXG2kM3vnDkeOi3x4cn8YFakQrdGeTzRKKj"
                ],
                "nvme0n1": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T_1",
                    "nvme-eui.002538ba115192d9"
                ],
                "nvme0n1p1": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T-part1",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T_1-part1",
                    "nvme-eui.002538ba115192d9-part1"
                ],
                "nvme0n1p2": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T-part2",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T_1-part2",
                    "nvme-eui.002538ba115192d9-part2"
                ],
                "nvme0n1p3": [
                    "lvm-pv-uuid-fjE5xj-R3kI-XF2D-kgT1-PfhS-JOQW-WyPitb",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T-part3",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T_1-part3",
                    "nvme-eui.002538ba115192d9-part3"
                ]
            },
            "labels": {},
            "masters": {
                "nvme0n1p3": [
                    "dm-0"
                ]
            },
            "uuids": {
                "dm-0": [
                    "74bdc1f8-ee74-4a56-aacd-1b12c6cd5fb2"
                ],
                "nvme0n1p1": [
                    "472D-C891"
                ],
                "nvme0n1p2": [
                    "315550c3-1272-41b4-8f66-0bc16f8fba04"
                ]
            }
        },
        "ansible_devices": {
            "dm-0": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [
                        "dm-name-ubuntu--vg-ubuntu--lv",
                        "dm-uuid-LVM-mU5ZJgEXHxAEfaalRChL1VbocG0rDBXG2kM3vnDkeOi3x4cn8YFakQrdGeTzRKKj"
                    ],
                    "labels": [],
                    "masters": [],
                    "uuids": [
                        "74bdc1f8-ee74-4a56-aacd-1b12c6cd5fb2"
                    ]
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "",
                "sectors": "3886948352",
                "sectorsize": "512",
                "size": "1.81 TB",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop0": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "151352",
                "sectorsize": "512",
                "size": "73.90 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop1": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "287864",
                "sectorsize": "512",
                "size": "140.56 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop2": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "156840",
                "sectorsize": "512",
                "size": "76.58 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop3": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "104240",
                "sectorsize": "512",
                "size": "50.90 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop4": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop5": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop6": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop7": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "nvme0n1": {
                "holders": [],
                "host": "Non-Volatile memory controller: Samsung Electronics Co Ltd NVMe SSD Controller PM9A1/PM9A3/980PRO",
                "links": {
                    "ids": [
                        "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T",
                        "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T_1",
                        "nvme-eui.002538ba115192d9"
                    ],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": "Samsung SSD 980 PRO 2TB",
                "partitions": {
                    "nvme0n1p1": {
                        "holders": [],
                        "links": {
                            "ids": [
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T-part1",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T_1-part1",
                                "nvme-eui.002538ba115192d9-part1"
                            ],
                            "labels": [],
                            "masters": [],
                            "uuids": [
                                "472D-C891"
                            ]
                        },
                        "sectors": "2201600",
                        "sectorsize": 512,
                        "size": "1.05 GB",
                        "start": "2048",
                        "uuid": "472D-C891"
                    },
                    "nvme0n1p2": {
                        "holders": [],
                        "links": {
                            "ids": [
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T-part2",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T_1-part2",
                                "nvme-eui.002538ba115192d9-part2"
                            ],
                            "labels": [],
                            "masters": [],
                            "uuids": [
                                "315550c3-1272-41b4-8f66-0bc16f8fba04"
                            ]
                        },
                        "sectors": "4194304",
                        "sectorsize": 512,
                        "size": "2.00 GB",
                        "start": "2203648",
                        "uuid": "315550c3-1272-41b4-8f66-0bc16f8fba04"
                    },
                    "nvme0n1p3": {
                        "holders": [
                            "ubuntu--vg-ubuntu--lv"
                        ],
                        "links": {
                            "ids": [
                                "lvm-pv-uuid-fjE5xj-R3kI-XF2D-kgT1-PfhS-JOQW-WyPitb",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T-part3",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44086T_1-part3",
                                "nvme-eui.002538ba115192d9-part3"
                            ],
                            "labels": [],
                            "masters": [
                                "dm-0"
                            ],
                            "uuids": []
                        },
                        "sectors": "3900628992",
                        "sectorsize": 512,
                        "size": "1.82 TB",
                        "start": "6397952",
                        "uuid": null
                    }
                },
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "3907029168",
                "sectorsize": "512",
                "size": "1.82 TB",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            }
        },
        "ansible_distribution": "Ubuntu",
        "ansible_distribution_file_parsed": true,
        "ansible_distribution_file_path": "/etc/os-release",
        "ansible_distribution_file_variety": "Debian",
        "ansible_distribution_major_version": "24",
        "ansible_distribution_release": "noble",
        "ansible_distribution_version": "24.04",
        "ansible_dns": {
            "nameservers": [
                "127.0.0.53"
            ],
            "options": {
                "edns0": true,
                "trust-ad": true
            },
            "search": [
                "cgocable.net"
            ]
        },
        "ansible_docker0": {
            "active": false,
            "device": "docker0",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "off [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "on",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "on",
                "tx_fcoe_segmentation": "on",
                "tx_gre_csum_segmentation": "on",
                "tx_gre_segmentation": "on",
                "tx_gso_list": "on",
                "tx_gso_partial": "on",
                "tx_gso_robust": "on",
                "tx_ipxip4_segmentation": "on",
                "tx_ipxip6_segmentation": "on",
                "tx_lockless": "on [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "on",
                "tx_scatter_gather_fraglist": "on",
                "tx_sctp_segmentation": "on",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "on",
                "tx_tcp_mangleid_segmentation": "on",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "on",
                "tx_udp_segmentation": "on",
                "tx_udp_tnl_csum_segmentation": "on",
                "tx_udp_tnl_segmentation": "on",
                "tx_vlan_offload": "on",
                "tx_vlan_stag_hw_insert": "on",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "id": "8000.8a4904eaa252",
            "interfaces": [],
            "ipv4": {
                "address": "172.17.0.1",
                "broadcast": "172.17.255.255",
                "netmask": "255.255.0.0",
                "network": "172.17.0.0",
                "prefix": "16"
            },
            "ipv6": [
                {
                    "address": "fe80::8849:4ff:feea:a252",
                    "prefix": "64",
                    "scope": "link"
                }
            ],
            "macaddress": "8a:49:04:ea:a2:52",
            "mtu": 1500,
            "promisc": false,
            "speed": -1,
            "stp": false,
            "timestamping": [],
            "type": "bridge"
        },
        "ansible_domain": "",
        "ansible_effective_group_id": 1000,
        "ansible_effective_user_id": 1000,
        "ansible_enp4s0": {
            "active": true,
            "device": "enp4s0",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "off [requested on]",
                "highdma": "on [fixed]",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "off [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off",
                "rx_checksumming": "on",
                "rx_fcs": "off",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "on",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "off",
                "tcp_segmentation_offload": "off",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "off [fixed]",
                "tx_checksum_ipv4": "on",
                "tx_checksum_ipv6": "on",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "off [fixed]",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "off [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "off",
                "tx_scatter_gather_fraglist": "off [fixed]",
                "tx_sctp_segmentation": "off [fixed]",
                "tx_tcp6_segmentation": "off",
                "tx_tcp_ecn_segmentation": "off [fixed]",
                "tx_tcp_mangleid_segmentation": "off",
                "tx_tcp_segmentation": "off",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "off [fixed]",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "on",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "ipv4": {
                "address": "192.168.40.242",
                "broadcast": "192.168.40.255",
                "netmask": "255.255.255.0",
                "network": "192.168.40.0",
                "prefix": "24"
            },
            "ipv6": [
                {
                    "address": "2001:1970:5641:d600:415f:3010:6d0b:a5d8",
                    "prefix": "128",
                    "scope": "global"
                },
                {
                    "address": "2001:1970:5641:d600:642:1aff:fe03:afb5",
                    "prefix": "64",
                    "scope": "global"
                },
                {
                    "address": "fe80::642:1aff:fe03:afb5",
                    "prefix": "64",
                    "scope": "link"
                }
            ],
            "macaddress": "04:42:1a:03:af:b5",
            "module": "r8169",
            "mtu": 1500,
            "pciid": "0000:04:00.0",
            "promisc": false,
            "speed": 1000,
            "timestamping": [],
            "type": "ether"
        },
        "ansible_env": {
            "DBUS_SESSION_BUS_ADDRESS": "unix:path=/run/user/1000/bus",
            "HOME": "/home/gpadmin",
            "LANG": "en_US.UTF-8",
            "LOGNAME": "gpadmin",
            "PATH": "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin",
            "PWD": "/home/gpadmin",
            "SHELL": "/bin/bash",
            "SHLVL": "0",
            "SSH_CLIENT": "192.168.40.240 45904 22",
            "SSH_CONNECTION": "192.168.40.240 45904 192.168.40.242 22",
            "USER": "gpadmin",
            "XDG_RUNTIME_DIR": "/run/user/1000",
            "XDG_SESSION_CLASS": "user",
            "XDG_SESSION_ID": "366",
            "XDG_SESSION_TYPE": "tty",
            "_": "/bin/sh"
        },
        "ansible_fibre_channel_wwn": [],
        "ansible_fips": false,
        "ansible_form_factor": "Desktop",
        "ansible_fqdn": "g-242",
        "ansible_hostname": "g-242",
        "ansible_hostnqn": "",
        "ansible_interfaces": [
            "enp4s0",
            "docker0",
            "wlo1",
            "lo"
        ],
        "ansible_is_chroot": false,
        "ansible_iscsi_iqn": "",
        "ansible_kernel": "6.8.0-63-generic",
        "ansible_kernel_version": "#66-Ubuntu SMP PREEMPT_DYNAMIC Fri Jun 13 20:25:30 UTC 2025",
        "ansible_lo": {
            "active": true,
            "device": "lo",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on [fixed]",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "on [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "on [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "on [fixed]",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "on [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "on",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "on [fixed]",
                "tx_nocache_copy": "off [fixed]",
                "tx_scatter_gather": "on [fixed]",
                "tx_scatter_gather_fraglist": "on [fixed]",
                "tx_sctp_segmentation": "on",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "on",
                "tx_tcp_mangleid_segmentation": "on",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "on",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "off [fixed]",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "on [fixed]"
            },
            "hw_timestamp_filters": [],
            "ipv4": {
                "address": "127.0.0.1",
                "broadcast": "",
                "netmask": "255.0.0.0",
                "network": "127.0.0.0",
                "prefix": "8"
            },
            "ipv6": [
                {
                    "address": "::1",
                    "prefix": "128",
                    "scope": "host"
                }
            ],
            "mtu": 65536,
            "promisc": false,
            "timestamping": [],
            "type": "loopback"
        },
        "ansible_loadavg": {
            "15m": 0.04833984375,
            "1m": 0.056640625,
            "5m": 0.08056640625
        },
        "ansible_local": {},
        "ansible_locally_reachable_ips": {
            "ipv4": [
                "127.0.0.0/8",
                "127.0.0.1",
                "172.17.0.1",
                "192.168.40.242"
            ],
            "ipv6": [
                "::1",
                "2001:1970:5641:d600:642:1aff:fe03:afb5",
                "2001:1970:5641:d600:415f:3010:6d0b:a5d8",
                "fe80::642:1aff:fe03:afb5",
                "fe80::8849:4ff:feea:a252"
            ]
        },
        "ansible_lsb": {
            "codename": "noble",
            "description": "Ubuntu 24.04.2 LTS",
            "id": "Ubuntu",
            "major_release": "24",
            "release": "24.04"
        },
        "ansible_lvm": "N/A",
        "ansible_machine": "x86_64",
        "ansible_machine_id": "9cc4331a552546e4a898402302fe96b6",
        "ansible_memfree_mb": 120587,
        "ansible_memory_mb": {
            "nocache": {
                "free": 127442,
                "used": 1208
            },
            "real": {
                "free": 120587,
                "total": 128650,
                "used": 8063
            },
            "swap": {
                "cached": 0,
                "free": 8191,
                "total": 8191,
                "used": 0
            }
        },
        "ansible_memtotal_mb": 128650,
        "ansible_mounts": [
            {
                "block_available": 449269615,
                "block_size": 4096,
                "block_total": 477960925,
                "block_used": 28691310,
                "device": "/dev/mapper/ubuntu--vg-ubuntu--lv",
                "fstype": "ext4",
                "inode_available": 121214966,
                "inode_total": 121470976,
                "inode_used": 256010,
                "mount": "/",
                "options": "rw,relatime",
                "size_available": 1840208343040,
                "size_total": 1957727948800,
                "uuid": "74bdc1f8-ee74-4a56-aacd-1b12c6cd5fb2"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 1125,
                "block_used": 1125,
                "device": "/dev/loop1",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 3532,
                "inode_used": 3532,
                "mount": "/snap/docker/3265",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 147456000,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 592,
                "block_used": 592,
                "device": "/dev/loop0",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 14261,
                "inode_used": 14261,
                "mount": "/snap/core22/2010",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 77594624,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 613,
                "block_used": 613,
                "device": "/dev/loop2",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 967,
                "inode_used": 967,
                "mount": "/snap/powershell/294",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 80347136,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 408,
                "block_used": 408,
                "device": "/dev/loop3",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 608,
                "inode_used": 608,
                "mount": "/snap/snapd/24718",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 53477376,
                "uuid": "N/A"
            },
            {
                "block_available": 435221,
                "block_size": 4096,
                "block_total": 498138,
                "block_used": 62917,
                "device": "/dev/nvme0n1p2",
                "fstype": "ext4",
                "inode_available": 130762,
                "inode_total": 131072,
                "inode_used": 310,
                "mount": "/boot",
                "options": "rw,relatime",
                "size_available": 1782665216,
                "size_total": 2040373248,
                "uuid": "315550c3-1272-41b4-8f66-0bc16f8fba04"
            },
            {
                "block_available": 273086,
                "block_size": 4096,
                "block_total": 274658,
                "block_used": 1572,
                "device": "/dev/nvme0n1p1",
                "fstype": "vfat",
                "inode_available": 0,
                "inode_total": 0,
                "inode_used": 0,
                "mount": "/boot/efi",
                "options": "rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=iso8859-1,shortname=mixed,errors=remount-ro",
                "size_available": 1118560256,
                "size_total": 1124999168,
                "uuid": "472D-C891"
            }
        ],
        "ansible_nodename": "g-242",
        "ansible_os_family": "Debian",
        "ansible_pkg_mgr": "apt",
        "ansible_proc_cmdline": {
            "BOOT_IMAGE": "/vmlinuz-6.8.0-63-generic",
            "ro": true,
            "root": "/dev/mapper/ubuntu--vg-ubuntu--lv"
        },
        "ansible_processor": [
            "0",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "1",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "2",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "3",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "4",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "5",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "6",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "7",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "8",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "9",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "10",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "11",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "12",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "13",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "14",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "15",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz"
        ],
        "ansible_processor_cores": 8,
        "ansible_processor_count": 1,
        "ansible_processor_nproc": 16,
        "ansible_processor_threads_per_core": 2,
        "ansible_processor_vcpus": 16,
        "ansible_product_name": "System Product Name",
        "ansible_product_serial": "NA",
        "ansible_product_uuid": "NA",
        "ansible_product_version": "System Version",
        "ansible_python": {
            "executable": "/usr/bin/python3",
            "has_sslcontext": true,
            "type": "cpython",
            "version": {
                "major": 3,
                "micro": 3,
                "minor": 12,
                "releaselevel": "final",
                "serial": 0
            },
            "version_info": [
                3,
                12,
                3,
                "final",
                0
            ]
        },
        "ansible_python_version": "3.12.3",
        "ansible_real_group_id": 1000,
        "ansible_real_user_id": 1000,
        "ansible_selinux": {
            "status": "disabled"
        },
        "ansible_selinux_python_present": true,
        "ansible_service_mgr": "systemd",
        "ansible_ssh_host_key_ecdsa_public": "AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFX9NDkB9LW3y4HR77JXe2aM6nwuC/i1DKzU10xYPCPSNRnjUG8XdW1reg5WH9OXq9OuMmZk2q/YOS40yXl913Y=",
        "ansible_ssh_host_key_ecdsa_public_keytype": "ecdsa-sha2-nistp256",
        "ansible_ssh_host_key_ed25519_public": "AAAAC3NzaC1lZDI1NTE5AAAAIDE8+gd919x8yePR/JH7ANfLqYiG0WucuXsEILObxSNN",
        "ansible_ssh_host_key_ed25519_public_keytype": "ssh-ed25519",
        "ansible_ssh_host_key_rsa_public": "AAAAB3NzaC1yc2EAAAADAQABAAABgQCuttxwapFMSS/9eigBZvP+JyhNNTWhaMF+9QMoTo6L2O7BI6hqDUIYXTFhtcLffRTVG+WqxBBzGp4URMKDqcSeg4BmzbAR3eA3wKLO7SQ77b3rR0qEwrQ7jKFxRoBxdx+5a29PE0PgH/baXcAaFAf0O/g+jqBmWhy8prUE06jMhM513HzIjaXghhL86dMrU6oa7R+IfROHrw32pjnKxYPU+o22tRWz3WFQF8/W//y7ni4rWjA08I3sw6SglZEObj7bwTTHbP7zhi3bf17MCgG6juPkaurxLw4ktIkXNuICkGaNem1I8HEud8UkIjHYbUz4QkpG5uTJsS6noez3iXScgIKc5BmNhF0ORmEYc7SwpJN7rhh0cyGC1SHtDGV+lbp7U95Pcrdk7ltBB+xmRhUVXtS/DpAlIYYKGIE+T71GEhehTz6Bthy574EYzgKCPOAER2fUewg7qHeIMaFqoVpGgS44EJ0YkyIJPsHxiWdCRSJrqYxd5Me99nWB/+Ak5Bs=",
        "ansible_ssh_host_key_rsa_public_keytype": "ssh-rsa",
        "ansible_swapfree_mb": 8191,
        "ansible_swaptotal_mb": 8191,
        "ansible_system": "Linux",
        "ansible_system_capabilities": [
            ""
        ],
        "ansible_system_capabilities_enforced": "True",
        "ansible_system_vendor": "ASUS",
        "ansible_uptime_seconds": 167039,
        "ansible_user_dir": "/home/gpadmin",
        "ansible_user_gecos": "gpadmin",
        "ansible_user_gid": 1000,
        "ansible_user_id": "gpadmin",
        "ansible_user_shell": "/bin/bash",
        "ansible_user_uid": 1000,
        "ansible_userspace_architecture": "x86_64",
        "ansible_userspace_bits": "64",
        "ansible_virtualization_role": "NA",
        "ansible_virtualization_tech_guest": [],
        "ansible_virtualization_tech_host": [],
        "ansible_virtualization_type": "NA",
        "ansible_wlo1": {
            "active": false,
            "device": "wlo1",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "on [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "off [fixed]",
                "tx_checksum_ipv4": "on",
                "tx_checksum_ipv6": "on",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "off [fixed]",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "off [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "on",
                "tx_scatter_gather_fraglist": "off [fixed]",
                "tx_sctp_segmentation": "off [fixed]",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "off [fixed]",
                "tx_tcp_mangleid_segmentation": "off",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "off [fixed]",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "off [fixed]",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "macaddress": "ec:63:d7:37:57:df",
            "module": "iwlwifi",
            "mtu": 1500,
            "pciid": "0000:00:14.3",
            "promisc": false,
            "timestamping": [],
            "type": "ether"
        },
        "discovered_interpreter_python": "/usr/bin/python3",
        "gather_subset": [
            "all"
        ],
        "module_setup": true
    },
    "changed": false
}
2025-07-07 10:34:07,860 p=142873 u=gpadmin n=ansible | G-244 | SUCCESS => {
    "ansible_facts": {
        "ansible_all_ipv4_addresses": [
            "192.168.40.244",
            "172.17.0.1"
        ],
        "ansible_all_ipv6_addresses": [
            "2001:1970:5641:d600:6407:b23:d246:870",
            "2001:1970:5641:d600:642:1aff:fe03:afd5",
            "fe80::642:1aff:fe03:afd5",
            "fe80::c69:5aff:fef0:bab0"
        ],
        "ansible_apparmor": {
            "status": "enabled"
        },
        "ansible_architecture": "x86_64",
        "ansible_bios_date": "04/27/2021",
        "ansible_bios_vendor": "American Megatrends Inc.",
        "ansible_bios_version": "0820",
        "ansible_board_asset_tag": "Default string",
        "ansible_board_name": "TUF GAMING B560M-PLUS WIFI",
        "ansible_board_serial": "NA",
        "ansible_board_vendor": "ASUSTeK COMPUTER INC.",
        "ansible_board_version": "Rev 1.xx",
        "ansible_chassis_asset_tag": "Default string",
        "ansible_chassis_serial": "NA",
        "ansible_chassis_vendor": "Default string",
        "ansible_chassis_version": "Default string",
        "ansible_cmdline": {
            "BOOT_IMAGE": "/vmlinuz-6.8.0-63-generic",
            "ro": true,
            "root": "/dev/mapper/ubuntu--vg-ubuntu--lv"
        },
        "ansible_date_time": {
            "date": "2025-07-07",
            "day": "07",
            "epoch": "1751898847",
            "epoch_int": "1751898847",
            "hour": "14",
            "iso8601": "2025-07-07T14:34:07Z",
            "iso8601_basic": "20250707T143407390489",
            "iso8601_basic_short": "20250707T143407",
            "iso8601_micro": "2025-07-07T14:34:07.390489Z",
            "minute": "34",
            "month": "07",
            "second": "07",
            "time": "14:34:07",
            "tz": "UTC",
            "tz_dst": "UTC",
            "tz_offset": "+0000",
            "weekday": "Monday",
            "weekday_number": "1",
            "weeknumber": "27",
            "year": "2025"
        },
        "ansible_default_ipv4": {
            "address": "192.168.40.244",
            "alias": "enp4s0",
            "broadcast": "192.168.40.255",
            "gateway": "192.168.40.1",
            "interface": "enp4s0",
            "macaddress": "04:42:1a:03:af:d5",
            "mtu": 1500,
            "netmask": "255.255.255.0",
            "network": "192.168.40.0",
            "prefix": "24",
            "type": "ether"
        },
        "ansible_default_ipv6": {
            "address": "2001:1970:5641:d600:6407:b23:d246:870",
            "gateway": "fe80::a622:49ff:feeb:de0c",
            "interface": "enp4s0",
            "macaddress": "04:42:1a:03:af:d5",
            "mtu": 1500,
            "prefix": "128",
            "scope": "global",
            "type": "ether"
        },
        "ansible_device_links": {
            "ids": {
                "dm-0": [
                    "dm-name-ubuntu--vg-ubuntu--lv",
                    "dm-uuid-LVM-xoSQ3hOzUfCYqJu9gYiFrhSfUNcog2HcLc3AIqocnt4puJ2cbyiKKDgLM1jzEUam"
                ],
                "nvme0n1": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R_1",
                    "nvme-eui.002538ba115193d5"
                ],
                "nvme0n1p1": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R-part1",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R_1-part1",
                    "nvme-eui.002538ba115193d5-part1"
                ],
                "nvme0n1p2": [
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R-part2",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R_1-part2",
                    "nvme-eui.002538ba115193d5-part2"
                ],
                "nvme0n1p3": [
                    "lvm-pv-uuid-GA05AK-AP7u-eX0A-0Mjy-Z7HW-yYlE-NTyMz7",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R-part3",
                    "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R_1-part3",
                    "nvme-eui.002538ba115193d5-part3"
                ]
            },
            "labels": {},
            "masters": {
                "nvme0n1p3": [
                    "dm-0"
                ]
            },
            "uuids": {
                "dm-0": [
                    "df38ddac-70ee-4627-b698-f263892300ee"
                ],
                "nvme0n1p1": [
                    "35FE-7E9D"
                ],
                "nvme0n1p2": [
                    "38feb8a5-13dc-4d10-9971-0285f9a9cb05"
                ]
            }
        },
        "ansible_devices": {
            "dm-0": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [
                        "dm-name-ubuntu--vg-ubuntu--lv",
                        "dm-uuid-LVM-xoSQ3hOzUfCYqJu9gYiFrhSfUNcog2HcLc3AIqocnt4puJ2cbyiKKDgLM1jzEUam"
                    ],
                    "labels": [],
                    "masters": [],
                    "uuids": [
                        "df38ddac-70ee-4627-b698-f263892300ee"
                    ]
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "",
                "sectors": "3886948352",
                "sectorsize": "512",
                "size": "1.81 TB",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop0": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "104240",
                "sectorsize": "512",
                "size": "50.90 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop1": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "151352",
                "sectorsize": "512",
                "size": "73.90 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop2": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "287864",
                "sectorsize": "512",
                "size": "140.56 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop3": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "156840",
                "sectorsize": "512",
                "size": "76.58 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop4": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop5": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop6": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop7": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "1",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "0",
                "sectorsize": "512",
                "size": "0.00 Bytes",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "nvme0n1": {
                "holders": [],
                "host": "Non-Volatile memory controller: Samsung Electronics Co Ltd NVMe SSD Controller PM9A1/PM9A3/980PRO",
                "links": {
                    "ids": [
                        "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R",
                        "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R_1",
                        "nvme-eui.002538ba115193d5"
                    ],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": "Samsung SSD 980 PRO 2TB",
                "partitions": {
                    "nvme0n1p1": {
                        "holders": [],
                        "links": {
                            "ids": [
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R-part1",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R_1-part1",
                                "nvme-eui.002538ba115193d5-part1"
                            ],
                            "labels": [],
                            "masters": [],
                            "uuids": [
                                "35FE-7E9D"
                            ]
                        },
                        "sectors": "2201600",
                        "sectorsize": 512,
                        "size": "1.05 GB",
                        "start": "2048",
                        "uuid": "35FE-7E9D"
                    },
                    "nvme0n1p2": {
                        "holders": [],
                        "links": {
                            "ids": [
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R-part2",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R_1-part2",
                                "nvme-eui.002538ba115193d5-part2"
                            ],
                            "labels": [],
                            "masters": [],
                            "uuids": [
                                "38feb8a5-13dc-4d10-9971-0285f9a9cb05"
                            ]
                        },
                        "sectors": "4194304",
                        "sectorsize": 512,
                        "size": "2.00 GB",
                        "start": "2203648",
                        "uuid": "38feb8a5-13dc-4d10-9971-0285f9a9cb05"
                    },
                    "nvme0n1p3": {
                        "holders": [
                            "ubuntu--vg-ubuntu--lv"
                        ],
                        "links": {
                            "ids": [
                                "lvm-pv-uuid-GA05AK-AP7u-eX0A-0Mjy-Z7HW-yYlE-NTyMz7",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R-part3",
                                "nvme-Samsung_SSD_980_PRO_2TB_S6B0NG0RA44338R_1-part3",
                                "nvme-eui.002538ba115193d5-part3"
                            ],
                            "labels": [],
                            "masters": [
                                "dm-0"
                            ],
                            "uuids": []
                        },
                        "sectors": "3900628992",
                        "sectorsize": 512,
                        "size": "1.82 TB",
                        "start": "6397952",
                        "uuid": null
                    }
                },
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "3907029168",
                "sectorsize": "512",
                "size": "1.82 TB",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            }
        },
        "ansible_distribution": "Ubuntu",
        "ansible_distribution_file_parsed": true,
        "ansible_distribution_file_path": "/etc/os-release",
        "ansible_distribution_file_variety": "Debian",
        "ansible_distribution_major_version": "24",
        "ansible_distribution_release": "noble",
        "ansible_distribution_version": "24.04",
        "ansible_dns": {
            "nameservers": [
                "127.0.0.53"
            ],
            "options": {
                "edns0": true,
                "trust-ad": true
            },
            "search": [
                "cgocable.net"
            ]
        },
        "ansible_docker0": {
            "active": false,
            "device": "docker0",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "off [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "on",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "on",
                "tx_fcoe_segmentation": "on",
                "tx_gre_csum_segmentation": "on",
                "tx_gre_segmentation": "on",
                "tx_gso_list": "on",
                "tx_gso_partial": "on",
                "tx_gso_robust": "on",
                "tx_ipxip4_segmentation": "on",
                "tx_ipxip6_segmentation": "on",
                "tx_lockless": "on [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "on",
                "tx_scatter_gather_fraglist": "on",
                "tx_sctp_segmentation": "on",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "on",
                "tx_tcp_mangleid_segmentation": "on",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "on",
                "tx_udp_segmentation": "on",
                "tx_udp_tnl_csum_segmentation": "on",
                "tx_udp_tnl_segmentation": "on",
                "tx_vlan_offload": "on",
                "tx_vlan_stag_hw_insert": "on",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "id": "8000.0e695af0bab0",
            "interfaces": [],
            "ipv4": {
                "address": "172.17.0.1",
                "broadcast": "172.17.255.255",
                "netmask": "255.255.0.0",
                "network": "172.17.0.0",
                "prefix": "16"
            },
            "ipv6": [
                {
                    "address": "fe80::c69:5aff:fef0:bab0",
                    "prefix": "64",
                    "scope": "link"
                }
            ],
            "macaddress": "0e:69:5a:f0:ba:b0",
            "mtu": 1500,
            "promisc": false,
            "speed": -1,
            "stp": false,
            "timestamping": [],
            "type": "bridge"
        },
        "ansible_domain": "",
        "ansible_effective_group_id": 1000,
        "ansible_effective_user_id": 1000,
        "ansible_enp4s0": {
            "active": true,
            "device": "enp4s0",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "off [requested on]",
                "highdma": "on [fixed]",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "off [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off",
                "rx_checksumming": "on",
                "rx_fcs": "off",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "on",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "off",
                "tcp_segmentation_offload": "off",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "off [fixed]",
                "tx_checksum_ipv4": "on",
                "tx_checksum_ipv6": "on",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "off [fixed]",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "off [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "off",
                "tx_scatter_gather_fraglist": "off [fixed]",
                "tx_sctp_segmentation": "off [fixed]",
                "tx_tcp6_segmentation": "off",
                "tx_tcp_ecn_segmentation": "off [fixed]",
                "tx_tcp_mangleid_segmentation": "off",
                "tx_tcp_segmentation": "off",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "off [fixed]",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "on",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "ipv4": {
                "address": "192.168.40.244",
                "broadcast": "192.168.40.255",
                "netmask": "255.255.255.0",
                "network": "192.168.40.0",
                "prefix": "24"
            },
            "ipv6": [
                {
                    "address": "2001:1970:5641:d600:6407:b23:d246:870",
                    "prefix": "128",
                    "scope": "global"
                },
                {
                    "address": "2001:1970:5641:d600:642:1aff:fe03:afd5",
                    "prefix": "64",
                    "scope": "global"
                },
                {
                    "address": "fe80::642:1aff:fe03:afd5",
                    "prefix": "64",
                    "scope": "link"
                }
            ],
            "macaddress": "04:42:1a:03:af:d5",
            "module": "r8169",
            "mtu": 1500,
            "pciid": "0000:04:00.0",
            "promisc": false,
            "speed": 1000,
            "timestamping": [],
            "type": "ether"
        },
        "ansible_env": {
            "DBUS_SESSION_BUS_ADDRESS": "unix:path=/run/user/1000/bus",
            "HOME": "/home/gpadmin",
            "LANG": "en_US.UTF-8",
            "LOGNAME": "gpadmin",
            "PATH": "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin",
            "PWD": "/home/gpadmin",
            "SHELL": "/bin/bash",
            "SHLVL": "0",
            "SSH_CLIENT": "192.168.40.240 43592 22",
            "SSH_CONNECTION": "192.168.40.240 43592 192.168.40.244 22",
            "USER": "gpadmin",
            "XDG_RUNTIME_DIR": "/run/user/1000",
            "XDG_SESSION_CLASS": "user",
            "XDG_SESSION_ID": "361",
            "XDG_SESSION_TYPE": "tty",
            "_": "/bin/sh"
        },
        "ansible_fibre_channel_wwn": [],
        "ansible_fips": false,
        "ansible_form_factor": "Desktop",
        "ansible_fqdn": "g-244",
        "ansible_hostname": "g-244",
        "ansible_hostnqn": "",
        "ansible_interfaces": [
            "wlo1",
            "lo",
            "docker0",
            "enp4s0"
        ],
        "ansible_is_chroot": false,
        "ansible_iscsi_iqn": "",
        "ansible_kernel": "6.8.0-63-generic",
        "ansible_kernel_version": "#66-Ubuntu SMP PREEMPT_DYNAMIC Fri Jun 13 20:25:30 UTC 2025",
        "ansible_lo": {
            "active": true,
            "device": "lo",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on [fixed]",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "on [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "on [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "on [fixed]",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "on [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "on",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "on [fixed]",
                "tx_nocache_copy": "off [fixed]",
                "tx_scatter_gather": "on [fixed]",
                "tx_scatter_gather_fraglist": "on [fixed]",
                "tx_sctp_segmentation": "on",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "on",
                "tx_tcp_mangleid_segmentation": "on",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "on",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "off [fixed]",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "on [fixed]"
            },
            "hw_timestamp_filters": [],
            "ipv4": {
                "address": "127.0.0.1",
                "broadcast": "",
                "netmask": "255.0.0.0",
                "network": "127.0.0.0",
                "prefix": "8"
            },
            "ipv6": [
                {
                    "address": "::1",
                    "prefix": "128",
                    "scope": "host"
                }
            ],
            "mtu": 65536,
            "promisc": false,
            "timestamping": [],
            "type": "loopback"
        },
        "ansible_loadavg": {
            "15m": 0.0693359375,
            "1m": 0.20166015625,
            "5m": 0.1240234375
        },
        "ansible_local": {},
        "ansible_locally_reachable_ips": {
            "ipv4": [
                "127.0.0.0/8",
                "127.0.0.1",
                "172.17.0.1",
                "192.168.40.244"
            ],
            "ipv6": [
                "::1",
                "2001:1970:5641:d600:642:1aff:fe03:afd5",
                "2001:1970:5641:d600:6407:b23:d246:870",
                "fe80::642:1aff:fe03:afd5",
                "fe80::c69:5aff:fef0:bab0"
            ]
        },
        "ansible_lsb": {
            "codename": "noble",
            "description": "Ubuntu 24.04.2 LTS",
            "id": "Ubuntu",
            "major_release": "24",
            "release": "24.04"
        },
        "ansible_lvm": "N/A",
        "ansible_machine": "x86_64",
        "ansible_machine_id": "9d7c0a96c4764cd5892db9b35b753f7e",
        "ansible_memfree_mb": 120272,
        "ansible_memory_mb": {
            "nocache": {
                "free": 127382,
                "used": 1267
            },
            "real": {
                "free": 120272,
                "total": 128649,
                "used": 8377
            },
            "swap": {
                "cached": 0,
                "free": 8191,
                "total": 8191,
                "used": 0
            }
        },
        "ansible_memtotal_mb": 128649,
        "ansible_mounts": [
            {
                "block_available": 449270783,
                "block_size": 4096,
                "block_total": 477960925,
                "block_used": 28690142,
                "device": "/dev/mapper/ubuntu--vg-ubuntu--lv",
                "fstype": "ext4",
                "inode_available": 121214972,
                "inode_total": 121470976,
                "inode_used": 256004,
                "mount": "/",
                "options": "rw,relatime",
                "size_available": 1840213127168,
                "size_total": 1957727948800,
                "uuid": "df38ddac-70ee-4627-b698-f263892300ee"
            },
            {
                "block_available": 435221,
                "block_size": 4096,
                "block_total": 498138,
                "block_used": 62917,
                "device": "/dev/nvme0n1p2",
                "fstype": "ext4",
                "inode_available": 130762,
                "inode_total": 131072,
                "inode_used": 310,
                "mount": "/boot",
                "options": "rw,relatime",
                "size_available": 1782665216,
                "size_total": 2040373248,
                "uuid": "38feb8a5-13dc-4d10-9971-0285f9a9cb05"
            },
            {
                "block_available": 273086,
                "block_size": 4096,
                "block_total": 274658,
                "block_used": 1572,
                "device": "/dev/nvme0n1p1",
                "fstype": "vfat",
                "inode_available": 0,
                "inode_total": 0,
                "inode_used": 0,
                "mount": "/boot/efi",
                "options": "rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=iso8859-1,shortname=mixed,errors=remount-ro",
                "size_available": 1118560256,
                "size_total": 1124999168,
                "uuid": "35FE-7E9D"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 408,
                "block_used": 408,
                "device": "/dev/loop0",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 608,
                "inode_used": 608,
                "mount": "/snap/snapd/24718",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 53477376,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 592,
                "block_used": 592,
                "device": "/dev/loop1",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 14261,
                "inode_used": 14261,
                "mount": "/snap/core22/2010",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 77594624,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 1125,
                "block_used": 1125,
                "device": "/dev/loop2",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 3532,
                "inode_used": 3532,
                "mount": "/snap/docker/3265",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 147456000,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 613,
                "block_used": 613,
                "device": "/dev/loop3",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 967,
                "inode_used": 967,
                "mount": "/snap/powershell/294",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 80347136,
                "uuid": "N/A"
            }
        ],
        "ansible_nodename": "g-244",
        "ansible_os_family": "Debian",
        "ansible_pkg_mgr": "apt",
        "ansible_proc_cmdline": {
            "BOOT_IMAGE": "/vmlinuz-6.8.0-63-generic",
            "ro": true,
            "root": "/dev/mapper/ubuntu--vg-ubuntu--lv"
        },
        "ansible_processor": [
            "0",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "1",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "2",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "3",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "4",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "5",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "6",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "7",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "8",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "9",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "10",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "11",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "12",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "13",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "14",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz",
            "15",
            "GenuineIntel",
            "11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz"
        ],
        "ansible_processor_cores": 8,
        "ansible_processor_count": 1,
        "ansible_processor_nproc": 16,
        "ansible_processor_threads_per_core": 2,
        "ansible_processor_vcpus": 16,
        "ansible_product_name": "System Product Name",
        "ansible_product_serial": "NA",
        "ansible_product_uuid": "NA",
        "ansible_product_version": "System Version",
        "ansible_python": {
            "executable": "/usr/bin/python3",
            "has_sslcontext": true,
            "type": "cpython",
            "version": {
                "major": 3,
                "micro": 3,
                "minor": 12,
                "releaselevel": "final",
                "serial": 0
            },
            "version_info": [
                3,
                12,
                3,
                "final",
                0
            ]
        },
        "ansible_python_version": "3.12.3",
        "ansible_real_group_id": 1000,
        "ansible_real_user_id": 1000,
        "ansible_selinux": {
            "status": "disabled"
        },
        "ansible_selinux_python_present": true,
        "ansible_service_mgr": "systemd",
        "ansible_ssh_host_key_ecdsa_public": "AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBD2/bqOFHJ5l6VSqCDP4WxbX6jPoWG9vfth8ZPei/ZzYv+kUnj0R11/HcREirWSPgHJJkwx3U/37zsGziETcMH4=",
        "ansible_ssh_host_key_ecdsa_public_keytype": "ecdsa-sha2-nistp256",
        "ansible_ssh_host_key_ed25519_public": "AAAAC3NzaC1lZDI1NTE5AAAAIFQspAr7KYg8MUb3xFqGpWJPaj2yIsrlujfYc+ZlYyq8",
        "ansible_ssh_host_key_ed25519_public_keytype": "ssh-ed25519",
        "ansible_ssh_host_key_rsa_public": "AAAAB3NzaC1yc2EAAAADAQABAAABgQDbAvAaiW2yqinh5C4K78ZGxHbjEb0tcGcZ8/m2cXVng5nr/YlnbC8ni36T0gmPyRMGUQoMknpRV3I71AoTGsG68a/1FtTE/2oHf+cQCGDfAbtvic+IO9z3Ktod5dSXTgvFMkIwQ3zYEnt3frdIIIugUHS6auLppl1ONOs/qra3iMatJ0sDWppjjHDSfhFTMt752LhLhDJDFvsFuJownYZruOxQ2V3+Jtv1sgSp8FbgfI2kFv8nIM84RGd2Cl/DL7HGPdnaPyQQwVBK7Lo0IAL/JeADKp1mz2ZyrIVategzcDRTSMm+XdYztJY+PXqtnPv1OX2QnJne8/5aCaJ4DroCqjymePGoly7sMbs3l+4cKvPRhv7ltlpB2vhHUTkgOeLZdx6aPP6Af3nV70LfWFCgD73WykIKvGSkF6kdI1jbnLl7QKoxRWgEPngd0qA3PZCdsbgh99JXK9/UBTgxWivEnDCmdTb0Cm7xqR0FsquUFg7Vmk+bEYNqyB9QXK+ac1U=",
        "ansible_ssh_host_key_rsa_public_keytype": "ssh-rsa",
        "ansible_swapfree_mb": 8191,
        "ansible_swaptotal_mb": 8191,
        "ansible_system": "Linux",
        "ansible_system_capabilities": [
            ""
        ],
        "ansible_system_capabilities_enforced": "True",
        "ansible_system_vendor": "ASUS",
        "ansible_uptime_seconds": 165388,
        "ansible_user_dir": "/home/gpadmin",
        "ansible_user_gecos": "gpadmin",
        "ansible_user_gid": 1000,
        "ansible_user_id": "gpadmin",
        "ansible_user_shell": "/bin/bash",
        "ansible_user_uid": 1000,
        "ansible_userspace_architecture": "x86_64",
        "ansible_userspace_bits": "64",
        "ansible_virtualization_role": "NA",
        "ansible_virtualization_tech_guest": [],
        "ansible_virtualization_tech_host": [],
        "ansible_virtualization_type": "NA",
        "ansible_wlo1": {
            "active": false,
            "device": "wlo1",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "on [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "off [fixed]",
                "tx_checksum_ipv4": "on",
                "tx_checksum_ipv6": "on",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "off [fixed]",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "off [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "on",
                "tx_scatter_gather_fraglist": "off [fixed]",
                "tx_sctp_segmentation": "off [fixed]",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "off [fixed]",
                "tx_tcp_mangleid_segmentation": "off",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "off [fixed]",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "off [fixed]",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "macaddress": "ec:63:d7:36:e4:5d",
            "module": "iwlwifi",
            "mtu": 1500,
            "pciid": "0000:00:14.3",
            "promisc": false,
            "timestamping": [],
            "type": "ether"
        },
        "discovered_interpreter_python": "/usr/bin/python3",
        "gather_subset": [
            "all"
        ],
        "module_setup": true
    },
    "changed": false
}
2025-07-07 10:34:08,317 p=142873 u=gpadmin n=ansible | MASTER | SUCCESS => {
    "ansible_facts": {
        "ansible_all_ipv4_addresses": [
            "192.168.40.240",
            "172.17.0.1"
        ],
        "ansible_all_ipv6_addresses": [
            "2001:1970:5641:d600:7129:2439:d62:5873",
            "2001:1970:5641:d600:692:26ff:fedb:b1e",
            "fe80::692:26ff:fedb:b1e",
            "2001:1970:5641:d600:95b8:65dc:3c14:1e6a",
            "2001:1970:5641:d600:2b38:448d:3fd5:237a",
            "fe80::54cd:94ff:fe2a:74a1"
        ],
        "ansible_apparmor": {
            "status": "enabled"
        },
        "ansible_architecture": "x86_64",
        "ansible_bios_date": "01/30/2024",
        "ansible_bios_vendor": "American Megatrends Inc.",
        "ansible_bios_version": "1602",
        "ansible_board_asset_tag": "Default string",
        "ansible_board_name": "PRIME X399-A",
        "ansible_board_serial": "NA",
        "ansible_board_vendor": "ASUSTeK COMPUTER INC.",
        "ansible_board_version": "Rev 1.xx",
        "ansible_chassis_asset_tag": "Default string",
        "ansible_chassis_serial": "NA",
        "ansible_chassis_vendor": "Default string",
        "ansible_chassis_version": "Default string",
        "ansible_cmdline": {
            "BOOT_IMAGE": "/vmlinuz-6.11.0-29-generic",
            "quiet": true,
            "ro": true,
            "root": "/dev/mapper/ubuntu--vg-ubuntu--lv",
            "splash": true,
            "vt.handoff": "7"
        },
        "ansible_date_time": {
            "date": "2025-07-07",
            "day": "07",
            "epoch": "1751898847",
            "epoch_int": "1751898847",
            "hour": "10",
            "iso8601": "2025-07-07T14:34:07Z",
            "iso8601_basic": "20250707T103407600325",
            "iso8601_basic_short": "20250707T103407",
            "iso8601_micro": "2025-07-07T14:34:07.600325Z",
            "minute": "34",
            "month": "07",
            "second": "07",
            "time": "10:34:07",
            "tz": "EDT",
            "tz_dst": "EDT",
            "tz_offset": "-0400",
            "weekday": "Monday",
            "weekday_number": "1",
            "weeknumber": "27",
            "year": "2025"
        },
        "ansible_default_ipv4": {
            "address": "192.168.40.240",
            "alias": "enp5s0",
            "broadcast": "192.168.40.255",
            "gateway": "192.168.40.1",
            "interface": "enp5s0",
            "macaddress": "04:92:26:db:0b:1e",
            "mtu": 1500,
            "netmask": "255.255.255.0",
            "network": "192.168.40.0",
            "prefix": "24",
            "type": "ether"
        },
        "ansible_default_ipv6": {
            "address": "2001:1970:5641:d600:95b8:65dc:3c14:1e6a",
            "gateway": "fe80::a622:49ff:feeb:de0c",
            "interface": "enp5s0",
            "macaddress": "04:92:26:db:0b:1e",
            "mtu": 1500,
            "prefix": "64",
            "scope": "global",
            "type": "ether"
        },
        "ansible_device_links": {
            "ids": {
                "dm-0": [
                    "dm-name-ubuntu--vg-ubuntu--lv",
                    "dm-uuid-LVM-1vuftc4vVF73KWpm29z8QJ9biB30lcjAre6J1qz0fBh8qrsWhPC4EOa6mg2Pd7Ya"
                ],
                "nvme0n1": [
                    "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B",
                    "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B_1",
                    "nvme-eui.0000000001000000e4d25c1b73965001"
                ],
                "nvme0n1p1": [
                    "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B-part1",
                    "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B_1-part1",
                    "nvme-eui.0000000001000000e4d25c1b73965001-part1"
                ],
                "nvme0n1p2": [
                    "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B-part2",
                    "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B_1-part2",
                    "nvme-eui.0000000001000000e4d25c1b73965001-part2"
                ],
                "nvme0n1p3": [
                    "lvm-pv-uuid-WPs4ql-U05L-l4Jl-6N0t-4MWj-a8PG-PNGFVJ",
                    "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B-part3",
                    "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B_1-part3",
                    "nvme-eui.0000000001000000e4d25c1b73965001-part3"
                ]
            },
            "labels": {},
            "masters": {
                "nvme0n1p3": [
                    "dm-0"
                ]
            },
            "uuids": {
                "dm-0": [
                    "5b861674-54bc-4bd2-97ed-55a081165ad0"
                ],
                "nvme0n1p1": [
                    "1968-9B54"
                ],
                "nvme0n1p2": [
                    "a046030c-1072-4f5a-bcac-b841ef1cd857"
                ]
            }
        },
        "ansible_devices": {
            "dm-0": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [
                        "dm-name-ubuntu--vg-ubuntu--lv",
                        "dm-uuid-LVM-1vuftc4vVF73KWpm29z8QJ9biB30lcjAre6J1qz0fBh8qrsWhPC4EOa6mg2Pd7Ya"
                    ],
                    "labels": [],
                    "masters": [],
                    "uuids": [
                        "5b861674-54bc-4bd2-97ed-55a081165ad0"
                    ]
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "",
                "sectors": "1994006528",
                "sectorsize": "512",
                "size": "950.82 GB",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            },
            "loop0": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "151328",
                "sectorsize": "512",
                "size": "73.89 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop1": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "8",
                "sectorsize": "512",
                "size": "4.00 KB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop10": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "91008",
                "sectorsize": "512",
                "size": "44.44 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop11": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "104240",
                "sectorsize": "512",
                "size": "50.90 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop12": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "1136",
                "sectorsize": "512",
                "size": "568.00 KB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop13": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "1152",
                "sectorsize": "512",
                "size": "576.00 KB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop14": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "22176",
                "sectorsize": "512",
                "size": "10.83 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop2": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "151352",
                "sectorsize": "512",
                "size": "73.90 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop3": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "136736",
                "sectorsize": "512",
                "size": "66.77 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop4": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "1056784",
                "sectorsize": "512",
                "size": "516.01 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop5": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "528456",
                "sectorsize": "512",
                "size": "258.04 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop6": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "22800",
                "sectorsize": "512",
                "size": "11.13 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop7": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "187776",
                "sectorsize": "512",
                "size": "91.69 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop8": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "38944",
                "sectorsize": "512",
                "size": "19.02 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "loop9": {
                "holders": [],
                "host": "",
                "links": {
                    "ids": [],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": null,
                "partitions": {},
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "22056",
                "sectorsize": "512",
                "size": "10.77 MB",
                "support_discard": "4096",
                "vendor": null,
                "virtual": 1
            },
            "nvme0n1": {
                "holders": [],
                "host": "Non-Volatile memory controller: Intel Corporation SSD 660P Series (rev 03)",
                "links": {
                    "ids": [
                        "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B",
                        "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B_1",
                        "nvme-eui.0000000001000000e4d25c1b73965001"
                    ],
                    "labels": [],
                    "masters": [],
                    "uuids": []
                },
                "model": "INTEL SSDPEKNW010T8",
                "partitions": {
                    "nvme0n1p1": {
                        "holders": [],
                        "links": {
                            "ids": [
                                "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B-part1",
                                "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B_1-part1",
                                "nvme-eui.0000000001000000e4d25c1b73965001-part1"
                            ],
                            "labels": [],
                            "masters": [],
                            "uuids": [
                                "1968-9B54"
                            ]
                        },
                        "sectors": "2201600",
                        "sectorsize": 512,
                        "size": "1.05 GB",
                        "start": "2048",
                        "uuid": "1968-9B54"
                    },
                    "nvme0n1p2": {
                        "holders": [],
                        "links": {
                            "ids": [
                                "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B-part2",
                                "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B_1-part2",
                                "nvme-eui.0000000001000000e4d25c1b73965001-part2"
                            ],
                            "labels": [],
                            "masters": [],
                            "uuids": [
                                "a046030c-1072-4f5a-bcac-b841ef1cd857"
                            ]
                        },
                        "sectors": "4194304",
                        "sectorsize": 512,
                        "size": "2.00 GB",
                        "start": "2203648",
                        "uuid": "a046030c-1072-4f5a-bcac-b841ef1cd857"
                    },
                    "nvme0n1p3": {
                        "holders": [
                            "ubuntu--vg-ubuntu--lv"
                        ],
                        "links": {
                            "ids": [
                                "lvm-pv-uuid-WPs4ql-U05L-l4Jl-6N0t-4MWj-a8PG-PNGFVJ",
                                "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B-part3",
                                "nvme-INTEL_SSDPEKNW010T8_BTNH91310CNT1P0B_1-part3",
                                "nvme-eui.0000000001000000e4d25c1b73965001-part3"
                            ],
                            "labels": [],
                            "masters": [
                                "dm-0"
                            ],
                            "uuids": []
                        },
                        "sectors": "1994008576",
                        "sectorsize": 512,
                        "size": "950.82 GB",
                        "start": "6397952",
                        "uuid": null
                    }
                },
                "removable": "0",
                "rotational": "0",
                "sas_address": null,
                "sas_device_handle": null,
                "scheduler_mode": "none",
                "sectors": "2000409264",
                "sectorsize": "512",
                "serial": "BTNH91310CNT1P0B",
                "size": "953.87 GB",
                "support_discard": "512",
                "vendor": null,
                "virtual": 1
            }
        },
        "ansible_distribution": "Ubuntu",
        "ansible_distribution_file_parsed": true,
        "ansible_distribution_file_path": "/etc/os-release",
        "ansible_distribution_file_variety": "Debian",
        "ansible_distribution_major_version": "24",
        "ansible_distribution_release": "noble",
        "ansible_distribution_version": "24.04",
        "ansible_dns": {
            "nameservers": [
                "127.0.0.53"
            ],
            "options": {
                "edns0": true,
                "trust-ad": true
            },
            "search": [
                "cgocable.net"
            ]
        },
        "ansible_docker0": {
            "active": false,
            "device": "docker0",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "off [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "on",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "on",
                "tx_fcoe_segmentation": "on",
                "tx_gre_csum_segmentation": "on",
                "tx_gre_segmentation": "on",
                "tx_gso_list": "on",
                "tx_gso_partial": "on",
                "tx_gso_robust": "on",
                "tx_ipxip4_segmentation": "on",
                "tx_ipxip6_segmentation": "on",
                "tx_lockless": "on [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "on",
                "tx_scatter_gather_fraglist": "on",
                "tx_sctp_segmentation": "on",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "on",
                "tx_tcp_mangleid_segmentation": "on",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "on",
                "tx_udp_segmentation": "on",
                "tx_udp_tnl_csum_segmentation": "on",
                "tx_udp_tnl_segmentation": "on",
                "tx_vlan_offload": "on",
                "tx_vlan_stag_hw_insert": "on",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "id": "8000.56cd942a74a1",
            "interfaces": [],
            "ipv4": {
                "address": "172.17.0.1",
                "broadcast": "172.17.255.255",
                "netmask": "255.255.0.0",
                "network": "172.17.0.0",
                "prefix": "16"
            },
            "ipv6": [
                {
                    "address": "fe80::54cd:94ff:fe2a:74a1",
                    "prefix": "64",
                    "scope": "link"
                }
            ],
            "macaddress": "56:cd:94:2a:74:a1",
            "mtu": 1500,
            "promisc": false,
            "speed": -1,
            "stp": false,
            "timestamping": [],
            "type": "bridge"
        },
        "ansible_domain": "",
        "ansible_effective_group_id": 1000,
        "ansible_effective_user_id": 1000,
        "ansible_enp5s0": {
            "active": true,
            "device": "enp5s0",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on [fixed]",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "on",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "off [fixed]",
                "ntuple_filters": "off",
                "receive_hashing": "on",
                "rx_all": "off",
                "rx_checksumming": "on",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "on [fixed]",
                "rx_vlan_offload": "on",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "on",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "on",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "on",
                "tx_gre_segmentation": "on",
                "tx_gso_list": "off [fixed]",
                "tx_gso_partial": "on",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "on",
                "tx_ipxip6_segmentation": "on",
                "tx_lockless": "off [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "on",
                "tx_scatter_gather_fraglist": "off [fixed]",
                "tx_sctp_segmentation": "off [fixed]",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "off [fixed]",
                "tx_tcp_mangleid_segmentation": "off",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "on",
                "tx_udp_tnl_csum_segmentation": "on",
                "tx_udp_tnl_segmentation": "on",
                "tx_vlan_offload": "on",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "ipv4": {
                "address": "192.168.40.240",
                "broadcast": "192.168.40.255",
                "netmask": "255.255.255.0",
                "network": "192.168.40.0",
                "prefix": "24"
            },
            "ipv6": [
                {
                    "address": "2001:1970:5641:d600:7129:2439:d62:5873",
                    "prefix": "128",
                    "scope": "global"
                },
                {
                    "address": "2001:1970:5641:d600:692:26ff:fedb:b1e",
                    "prefix": "64",
                    "scope": "global"
                },
                {
                    "address": "fe80::692:26ff:fedb:b1e",
                    "prefix": "64",
                    "scope": "link"
                },
                {
                    "address": "2001:1970:5641:d600:95b8:65dc:3c14:1e6a",
                    "prefix": "64",
                    "scope": "global"
                },
                {
                    "address": "2001:1970:5641:d600:2b38:448d:3fd5:237a",
                    "prefix": "64",
                    "scope": "global"
                }
            ],
            "macaddress": "04:92:26:db:0b:1e",
            "module": "igb",
            "mtu": 1500,
            "pciid": "0000:05:00.0",
            "phc_index": 0,
            "promisc": false,
            "speed": 1000,
            "timestamping": [],
            "type": "ether"
        },
        "ansible_env": {
            "APPDIR": "/tmp/.mount_Cursor3M4zBO",
            "APPIMAGE": "/home/gpadmin/Downloads/Cursor-1.2.1-x86_64.AppImage",
            "ARGV0": "./Cursor-1.2.1-x86_64.AppImage",
            "BUNDLED_DEBUGPY_PATH": "/home/gpadmin/.cursor/extensions/ms-python.debugpy-2025.6.0-linux-x64/bundled/libs/debugpy",
            "CHROME_DESKTOP": "cursor.desktop",
            "CLUTTER_DISABLE_MIPMAPPED_TEXT": "1",
            "COLORTERM": "truecolor",
            "COLUMNS": "128",
            "COMPOSER_NO_INTERACTION": "1",
            "CONDA_DEFAULT_ENV": "base",
            "CONDA_EXE": "/home/gpadmin/miniconda3/bin/conda",
            "CONDA_PREFIX": "/home/gpadmin/miniconda3",
            "CONDA_PROMPT_MODIFIER": "(base) ",
            "CONDA_PYTHON_EXE": "/home/gpadmin/miniconda3/bin/python",
            "CONDA_SHLVL": "1",
            "CURSOR_TRACE_ID": "46b794d7dbad434084c156bbfd582466",
            "DBUS_SESSION_BUS_ADDRESS": "unix:path=/run/user/1000/bus",
            "DEBUGINFOD_URLS": "https://debuginfod.ubuntu.com ",
            "DESKTOP_SESSION": "ubuntu",
            "DISABLE_AUTO_UPDATE": "true",
            "DISPLAY": ":1",
            "GDK_BACKEND": "x11",
            "GDMSESSION": "ubuntu",
            "GIT_ASKPASS": "/tmp/.mount_Cursor3M4zBO/usr/share/cursor/resources/app/extensions/git/dist/askpass.sh",
            "GNOME_DESKTOP_SESSION_ID": "this-is-deprecated",
            "GNOME_SHELL_SESSION_MODE": "ubuntu",
            "GNOME_TERMINAL_SCREEN": "/org/gnome/Terminal/screen/294b7b7e_3c2e_44fc_8062_d71761907cbc",
            "GNOME_TERMINAL_SERVICE": ":1.103",
            "GPG_AGENT_INFO": "/run/user/1000/gnupg/S.gpg-agent:0:1",
            "GSETTINGS_SCHEMA_DIR": "/tmp/.mount_Cursor3M4zBO/usr/share/glib-2.0/schemas/:",
            "GSM_SKIP_SSH_AGENT_WORKAROUND": "true",
            "GTK_MODULES": "gail:atk-bridge",
            "HOME": "/home/gpadmin",
            "LANG": "en_US.UTF-8",
            "LD_LIBRARY_PATH": "/tmp/.mount_Cursor3M4zBO/usr/lib/:/tmp/.mount_Cursor3M4zBO/usr/lib32/:/tmp/.mount_Cursor3M4zBO/usr/lib64/:/tmp/.mount_Cursor3M4zBO/lib/:/tmp/.mount_Cursor3M4zBO/lib/i386-linux-gnu/:/tmp/.mount_Cursor3M4zBO/lib/x86_64-linux-gnu/:/tmp/.mount_Cursor3M4zBO/lib/aarch64-linux-gnu/:/tmp/.mount_Cursor3M4zBO/lib32/:/tmp/.mount_Cursor3M4zBO/lib64/:",
            "LESSCLOSE": "/usr/bin/lesspipe %s %s",
            "LESSOPEN": "| /usr/bin/lesspipe %s",
            "LINES": "2",
            "LOGNAME": "gpadmin",
            "LS_COLORS": "rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=00:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.avif=01;35:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:*~=00;90:*#=00;90:*.bak=00;90:*.crdownload=00;90:*.dpkg-dist=00;90:*.dpkg-new=00;90:*.dpkg-old=00;90:*.dpkg-tmp=00;90:*.old=00;90:*.orig=00;90:*.part=00;90:*.rej=00;90:*.rpmnew=00;90:*.rpmorig=00;90:*.rpmsave=00;90:*.swp=00;90:*.tmp=00;90:*.ucf-dist=00;90:*.ucf-new=00;90:*.ucf-old=00;90:",
            "MEMORY_PRESSURE_WATCH": "/sys/fs/cgroup/user.slice/user-1000.slice/user@1000.service/session.slice/org.gnome.Shell@x11.service/memory.pressure",
            "MEMORY_PRESSURE_WRITE": "c29tZSAyMDAwMDAgMjAwMDAwMAA=",
            "ORIGINAL_XDG_CURRENT_DESKTOP": "ubuntu:GNOME",
            "OWD": "/home/gpadmin/Downloads",
            "PAGER": "head -n 10000 | cat",
            "PATH": "/home/gpadmin/.npm-global/bin:/home/gpadmin/.npm-global/bin:/tmp/.mount_Cursor3M4zBO/usr/bin:/tmp/.mount_Cursor3M4zBO/usr/sbin:/tmp/.mount_Cursor3M4zBO/usr/games:/tmp/.mount_Cursor3M4zBO/bin:/tmp/.mount_Cursor3M4zBO/sbin:/home/gpadmin/miniconda3/bin:/home/gpadmin/miniconda3/condabin:/home/gpadmin/.npm-global/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin:/home/gpadmin/.local/share/JetBrains/Toolbox/scripts:/home/gpadmin/.local/share/JetBrains/Toolbox/scripts:/home/gpadmin/.cursor/extensions/ms-python.debugpy-2025.6.0-linux-x64/bundled/scripts/noConfigScripts:/home/gpadmin/.local/share/JetBrains/Toolbox/scripts",
            "PERLLIB": "/tmp/.mount_Cursor3M4zBO/usr/share/perl5/:/tmp/.mount_Cursor3M4zBO/usr/lib/perl5/:",
            "PIP_NO_INPUT": "true",
            "PWD": "/home/gpadmin/cursor-projects/02-Ray-Deploy",
            "PYDEVD_DISABLE_FILE_VALIDATION": "1",
            "QT_ACCESSIBILITY": "1",
            "QT_IM_MODULE": "ibus",
            "QT_PLUGIN_PATH": "/tmp/.mount_Cursor3M4zBO/usr/lib/qt4/plugins/:/tmp/.mount_Cursor3M4zBO/usr/lib/i386-linux-gnu/qt4/plugins/:/tmp/.mount_Cursor3M4zBO/usr/lib/x86_64-linux-gnu/qt4/plugins/:/tmp/.mount_Cursor3M4zBO/usr/lib/aarch64-linux-gnu/qt4/plugins/:/tmp/.mount_Cursor3M4zBO/usr/lib32/qt4/plugins/:/tmp/.mount_Cursor3M4zBO/usr/lib64/qt4/plugins/:/tmp/.mount_Cursor3M4zBO/usr/lib/qt5/plugins/:/tmp/.mount_Cursor3M4zBO/usr/lib/i386-linux-gnu/qt5/plugins/:/tmp/.mount_Cursor3M4zBO/usr/lib/x86_64-linux-gnu/qt5/plugins/:/tmp/.mount_Cursor3M4zBO/usr/lib/aarch64-linux-gnu/qt5/plugins/:/tmp/.mount_Cursor3M4zBO/usr/lib32/qt5/plugins/:/tmp/.mount_Cursor3M4zBO/usr/lib64/qt5/plugins/:",
            "RAY_CLIENT_MODE": "0",
            "SESSION_MANAGER": "local/G-K3S-Master:@/tmp/.ICE-unix/3074,unix/G-K3S-Master:/tmp/.ICE-unix/3074",
            "SHELL": "/bin/bash",
            "SHLVL": "2",
            "SSH_AUTH_SOCK": "/run/user/1000/keyring/ssh",
            "SYSTEMD_EXEC_PID": "3117",
            "TERM": "xterm-256color",
            "TERM_PROGRAM": "vscode",
            "TERM_PROGRAM_VERSION": "1.2.1",
            "USER": "gpadmin",
            "USERNAME": "gpadmin",
            "VSCODE_DEBUGPY_ADAPTER_ENDPOINTS": "/home/gpadmin/.cursor/extensions/ms-python.debugpy-2025.6.0-linux-x64/.noConfigDebugAdapterEndpoints/endpoint-73d9fe7d767df3af.txt",
            "VSCODE_GIT_ASKPASS_EXTRA_ARGS": "",
            "VSCODE_GIT_ASKPASS_MAIN": "/tmp/.mount_Cursor3M4zBO/usr/share/cursor/resources/app/extensions/git/dist/askpass-main.js",
            "VSCODE_GIT_ASKPASS_NODE": "/tmp/.mount_Cursor3M4zBO/usr/share/cursor/cursor",
            "VSCODE_GIT_IPC_HANDLE": "/run/user/1000/vscode-git-424856dfc2.sock",
            "VTE_VERSION": "7600",
            "WINDOWPATH": "2",
            "XAUTHORITY": "/run/user/1000/gdm/Xauthority",
            "XDG_CONFIG_DIRS": "/etc/xdg/xdg-ubuntu:/etc/xdg",
            "XDG_CURRENT_DESKTOP": "Unity",
            "XDG_DATA_DIRS": "/tmp/.mount_Cursor3M4zBO/usr/share/:/usr/local/share:/usr/share:/usr/share/ubuntu:/usr/share/gnome:/usr/local/share/:/usr/share/:/var/lib/snapd/desktop",
            "XDG_MENU_PREFIX": "gnome-",
            "XDG_RUNTIME_DIR": "/run/user/1000",
            "XDG_SESSION_CLASS": "user",
            "XDG_SESSION_DESKTOP": "ubuntu",
            "XDG_SESSION_TYPE": "x11",
            "XMODIFIERS": "@im=ibus",
            "_": "/home/gpadmin/miniconda3/bin/python3",
            "npm_config_yes": "true"
        },
        "ansible_fibre_channel_wwn": [],
        "ansible_fips": false,
        "ansible_form_factor": "Desktop",
        "ansible_fqdn": "G-K3S-Master",
        "ansible_hostname": "G-K3S-Master",
        "ansible_hostnqn": "",
        "ansible_interfaces": [
            "enp5s0",
            "docker0",
            "lo",
            "wlp6s0"
        ],
        "ansible_is_chroot": false,
        "ansible_iscsi_iqn": "",
        "ansible_kernel": "6.11.0-29-generic",
        "ansible_kernel_version": "#29~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jun 26 14:16:59 UTC 2",
        "ansible_lo": {
            "active": true,
            "device": "lo",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "on",
                "highdma": "on [fixed]",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "on [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "on [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "on",
                "tcp_segmentation_offload": "on",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "on [fixed]",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "on [fixed]",
                "tx_checksumming": "on",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "on",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "on [fixed]",
                "tx_nocache_copy": "off [fixed]",
                "tx_scatter_gather": "on [fixed]",
                "tx_scatter_gather_fraglist": "on [fixed]",
                "tx_sctp_segmentation": "on",
                "tx_tcp6_segmentation": "on",
                "tx_tcp_ecn_segmentation": "on",
                "tx_tcp_mangleid_segmentation": "on",
                "tx_tcp_segmentation": "on",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "on",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "off [fixed]",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "on [fixed]"
            },
            "hw_timestamp_filters": [],
            "ipv4": {
                "address": "127.0.0.1",
                "broadcast": "",
                "netmask": "255.0.0.0",
                "network": "127.0.0.0",
                "prefix": "8"
            },
            "ipv6": [
                {
                    "address": "::1",
                    "prefix": "128",
                    "scope": "host"
                }
            ],
            "mtu": 65536,
            "promisc": false,
            "timestamping": [],
            "type": "loopback"
        },
        "ansible_loadavg": {
            "15m": 1.205078125,
            "1m": 1.61669921875,
            "5m": 1.29833984375
        },
        "ansible_local": {},
        "ansible_locally_reachable_ips": {
            "ipv4": [
                "127.0.0.0/8",
                "127.0.0.1",
                "172.17.0.1",
                "192.168.40.240"
            ],
            "ipv6": [
                "::1",
                "2001:1970:5641:d600:692:26ff:fedb:b1e",
                "2001:1970:5641:d600:2b38:448d:3fd5:237a",
                "2001:1970:5641:d600:7129:2439:d62:5873",
                "2001:1970:5641:d600:95b8:65dc:3c14:1e6a",
                "fe80::692:26ff:fedb:b1e",
                "fe80::54cd:94ff:fe2a:74a1"
            ]
        },
        "ansible_lsb": {
            "codename": "noble",
            "description": "Ubuntu 24.04.2 LTS",
            "id": "Ubuntu",
            "major_release": "24",
            "release": "24.04"
        },
        "ansible_lvm": "N/A",
        "ansible_machine": "x86_64",
        "ansible_machine_id": "fc85a790d83543eb8a404ee2ad6d08f2",
        "ansible_memfree_mb": 101843,
        "ansible_memory_mb": {
            "nocache": {
                "free": 117873,
                "used": 10808
            },
            "real": {
                "free": 101843,
                "total": 128681,
                "used": 26838
            },
            "swap": {
                "cached": 0,
                "free": 8191,
                "total": 8191,
                "used": 0
            }
        },
        "ansible_memtotal_mb": 128681,
        "ansible_mounts": [
            {
                "block_available": 214023908,
                "block_size": 4096,
                "block_total": 245056938,
                "block_used": 31033030,
                "device": "/dev/mapper/ubuntu--vg-ubuntu--lv",
                "fstype": "ext4",
                "inode_available": 61702735,
                "inode_total": 62316544,
                "inode_used": 613809,
                "mount": "/",
                "options": "rw,relatime",
                "size_available": 876641927168,
                "size_total": 1003753218048,
                "uuid": "5b861674-54bc-4bd2-97ed-55a081165ad0"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 1,
                "block_used": 1,
                "device": "/dev/loop1",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 29,
                "inode_used": 29,
                "mount": "/snap/bare/5",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 131072,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 592,
                "block_used": 592,
                "device": "/dev/loop2",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 14261,
                "inode_used": 14261,
                "mount": "/snap/core22/2010",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 77594624,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 535,
                "block_used": 535,
                "device": "/dev/loop3",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 10632,
                "inode_used": 10632,
                "mount": "/snap/core24/1006",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 70123520,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 592,
                "block_used": 592,
                "device": "/dev/loop0",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 14262,
                "inode_used": 14262,
                "mount": "/snap/core22/1748",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 77594624,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 4129,
                "block_used": 4129,
                "device": "/dev/loop4",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 21133,
                "inode_used": 21133,
                "mount": "/snap/gnome-42-2204/202",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 541196288,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 2065,
                "block_used": 2065,
                "device": "/dev/loop5",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 820,
                "inode_used": 820,
                "mount": "/snap/firefox/5751",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 270663680,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 90,
                "block_used": 90,
                "device": "/dev/loop6",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 65,
                "inode_used": 65,
                "mount": "/snap/firmware-updater/167",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 11796480,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 734,
                "block_used": 734,
                "device": "/dev/loop7",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 76208,
                "inode_used": 76208,
                "mount": "/snap/gtk-common-themes/1535",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 96206848,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 153,
                "block_used": 153,
                "device": "/dev/loop8",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 10,
                "inode_used": 10,
                "mount": "/snap/helm/461",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 20054016,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 87,
                "block_used": 87,
                "device": "/dev/loop9",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 68,
                "inode_used": 68,
                "mount": "/snap/snap-store/1248",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 11403264,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 356,
                "block_used": 356,
                "device": "/dev/loop10",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 608,
                "inode_used": 608,
                "mount": "/snap/snapd/23545",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 46661632,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 408,
                "block_used": 408,
                "device": "/dev/loop11",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 608,
                "inode_used": 608,
                "mount": "/snap/snapd/24718",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 53477376,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 5,
                "block_used": 5,
                "device": "/dev/loop12",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 61,
                "inode_used": 61,
                "mount": "/snap/snapd-desktop-integration/253",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 655360,
                "uuid": "N/A"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 5,
                "block_used": 5,
                "device": "/dev/loop13",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 61,
                "inode_used": 61,
                "mount": "/snap/snapd-desktop-integration/315",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 655360,
                "uuid": "N/A"
            },
            {
                "block_available": 432829,
                "block_size": 4096,
                "block_total": 498138,
                "block_used": 65309,
                "device": "/dev/nvme0n1p2",
                "fstype": "ext4",
                "inode_available": 130754,
                "inode_total": 131072,
                "inode_used": 318,
                "mount": "/boot",
                "options": "rw,relatime",
                "size_available": 1772867584,
                "size_total": 2040373248,
                "uuid": "a046030c-1072-4f5a-bcac-b841ef1cd857"
            },
            {
                "block_available": 273086,
                "block_size": 4096,
                "block_total": 274658,
                "block_used": 1572,
                "device": "/dev/nvme0n1p1",
                "fstype": "vfat",
                "inode_available": 0,
                "inode_total": 0,
                "inode_used": 0,
                "mount": "/boot/efi",
                "options": "rw,relatime,fmask=0022,dmask=0022,codepage=437,iocharset=iso8859-1,shortname=mixed,errors=remount-ro",
                "size_available": 1118560256,
                "size_total": 1124999168,
                "uuid": "1968-9B54"
            },
            {
                "block_available": 0,
                "block_size": 131072,
                "block_total": 87,
                "block_used": 87,
                "device": "/dev/loop14",
                "fstype": "squashfs",
                "inode_available": 0,
                "inode_total": 68,
                "inode_used": 68,
                "mount": "/snap/snap-store/1270",
                "options": "ro,nodev,relatime,errors=continue,threads=single",
                "size_available": 0,
                "size_total": 11403264,
                "uuid": "N/A"
            }
        ],
        "ansible_nodename": "G-K3S-Master",
        "ansible_os_family": "Debian",
        "ansible_pkg_mgr": "apt",
        "ansible_proc_cmdline": {
            "BOOT_IMAGE": "/vmlinuz-6.11.0-29-generic",
            "quiet": true,
            "ro": true,
            "root": "/dev/mapper/ubuntu--vg-ubuntu--lv",
            "splash": true,
            "vt.handoff": "7"
        },
        "ansible_processor": [
            "0",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "1",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "2",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "3",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "4",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "5",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "6",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "7",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "8",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "9",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "10",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "11",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "12",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "13",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "14",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "15",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "16",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "17",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "18",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "19",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "20",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "21",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "22",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "23",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "24",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "25",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "26",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "27",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "28",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "29",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "30",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor",
            "31",
            "AuthenticAMD",
            "AMD Ryzen Threadripper 1950X 16-Core Processor"
        ],
        "ansible_processor_cores": 16,
        "ansible_processor_count": 1,
        "ansible_processor_nproc": 32,
        "ansible_processor_threads_per_core": 2,
        "ansible_processor_vcpus": 32,
        "ansible_product_name": "System Product Name",
        "ansible_product_serial": "NA",
        "ansible_product_uuid": "NA",
        "ansible_product_version": "System Version",
        "ansible_python": {
            "executable": "/usr/bin/python3",
            "has_sslcontext": true,
            "type": "cpython",
            "version": {
                "major": 3,
                "micro": 3,
                "minor": 12,
                "releaselevel": "final",
                "serial": 0
            },
            "version_info": [
                3,
                12,
                3,
                "final",
                0
            ]
        },
        "ansible_python_version": "3.12.3",
        "ansible_real_group_id": 1000,
        "ansible_real_user_id": 1000,
        "ansible_selinux": {
            "status": "disabled"
        },
        "ansible_selinux_python_present": true,
        "ansible_service_mgr": "systemd",
        "ansible_ssh_host_key_ecdsa_public": "AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBF2rGQakYvm1vhQQNHEHYh6HjXtJ2Xd6qEXx8dTG7+gDs68ks4mKMWsHqCPCqQ1cOyGyX2L6glBWMDDi7vmVoJk=",
        "ansible_ssh_host_key_ecdsa_public_keytype": "ecdsa-sha2-nistp256",
        "ansible_ssh_host_key_ed25519_public": "AAAAC3NzaC1lZDI1NTE5AAAAIOxI7cyoL4+xJbuvhKf9U244h7tw8QXSqNoD3gORLIsZ",
        "ansible_ssh_host_key_ed25519_public_keytype": "ssh-ed25519",
        "ansible_ssh_host_key_rsa_public": "AAAAB3NzaC1yc2EAAAADAQABAAABgQDAdRs+6Qbh1DogHTauR35O2CUkvKconKlEoS/2krExaqpUWRNh9txDqQLyzOBcZE6yr9GQ3ueAhX37yh1dv5sDOEeV2AafDr+ArAdNa/vOKX0jqFICPUPgrRDcPAJNVq/6FlZgwIyiaTnAky8jfJJMfqhOTMSxjd/PSh2x0MJzthdhjl1LNqwacn7+/nSv0MVO0hL/FUU8GLzYi5gxyIxRrWEn6XdpYx3UdNEbTXO351ToK6KetlrqleraEln31WjOPQv/dGrOQKp1K67wa+AFMNwuNrQQ/XZwToJQJQMrNL9zJEHjU/4/gW8wU99Je+qiXxIFU5x5+7yialB91sNuNHjbzdgHpVJfhjQjKUoI+ig3qUVFIp3ZQRyNnbI5LYqFRVcqfItm5Btu14fqJllOBunFtXanwJRT8qwW82esPAlwcSYzuR4GF5RDD5OkaveKlWf0KmPYRMtyUrgXvmQV12U6LNMvq2pvJ3RTnJimkZFcIB5M1J4Y6FW0hQ5HlqM=",
        "ansible_ssh_host_key_rsa_public_keytype": "ssh-rsa",
        "ansible_swapfree_mb": 8191,
        "ansible_swaptotal_mb": 8191,
        "ansible_system": "Linux",
        "ansible_system_capabilities": [
            ""
        ],
        "ansible_system_capabilities_enforced": "True",
        "ansible_system_vendor": "System manufacturer",
        "ansible_uptime_seconds": 167037,
        "ansible_user_dir": "/home/gpadmin",
        "ansible_user_gecos": "gpadmin",
        "ansible_user_gid": 1000,
        "ansible_user_id": "gpadmin",
        "ansible_user_shell": "/bin/bash",
        "ansible_user_uid": 1000,
        "ansible_userspace_architecture": "x86_64",
        "ansible_userspace_bits": "64",
        "ansible_virtualization_role": "host",
        "ansible_virtualization_tech_guest": [],
        "ansible_virtualization_tech_host": [
            "kvm"
        ],
        "ansible_virtualization_type": "kvm",
        "ansible_wlp6s0": {
            "active": false,
            "device": "wlp6s0",
            "features": {
                "esp_hw_offload": "off [fixed]",
                "esp_tx_csum_hw_offload": "off [fixed]",
                "fcoe_mtu": "off [fixed]",
                "generic_receive_offload": "on",
                "generic_segmentation_offload": "off [requested on]",
                "highdma": "off [fixed]",
                "hsr_dup_offload": "off [fixed]",
                "hsr_fwd_offload": "off [fixed]",
                "hsr_tag_ins_offload": "off [fixed]",
                "hsr_tag_rm_offload": "off [fixed]",
                "hw_tc_offload": "off [fixed]",
                "l2_fwd_offload": "off [fixed]",
                "large_receive_offload": "off [fixed]",
                "loopback": "off [fixed]",
                "macsec_hw_offload": "off [fixed]",
                "netns_local": "on [fixed]",
                "ntuple_filters": "off [fixed]",
                "receive_hashing": "off [fixed]",
                "rx_all": "off [fixed]",
                "rx_checksumming": "off [fixed]",
                "rx_fcs": "off [fixed]",
                "rx_gro_hw": "off [fixed]",
                "rx_gro_list": "off",
                "rx_udp_gro_forwarding": "off",
                "rx_udp_tunnel_port_offload": "off [fixed]",
                "rx_vlan_filter": "off [fixed]",
                "rx_vlan_offload": "off [fixed]",
                "rx_vlan_stag_filter": "off [fixed]",
                "rx_vlan_stag_hw_parse": "off [fixed]",
                "scatter_gather": "off",
                "tcp_segmentation_offload": "off",
                "tls_hw_record": "off [fixed]",
                "tls_hw_rx_offload": "off [fixed]",
                "tls_hw_tx_offload": "off [fixed]",
                "tx_checksum_fcoe_crc": "off [fixed]",
                "tx_checksum_ip_generic": "off [fixed]",
                "tx_checksum_ipv4": "off [fixed]",
                "tx_checksum_ipv6": "off [fixed]",
                "tx_checksum_sctp": "off [fixed]",
                "tx_checksumming": "off",
                "tx_esp_segmentation": "off [fixed]",
                "tx_fcoe_segmentation": "off [fixed]",
                "tx_gre_csum_segmentation": "off [fixed]",
                "tx_gre_segmentation": "off [fixed]",
                "tx_gso_list": "off [fixed]",
                "tx_gso_partial": "off [fixed]",
                "tx_gso_robust": "off [fixed]",
                "tx_ipxip4_segmentation": "off [fixed]",
                "tx_ipxip6_segmentation": "off [fixed]",
                "tx_lockless": "off [fixed]",
                "tx_nocache_copy": "off",
                "tx_scatter_gather": "off [fixed]",
                "tx_scatter_gather_fraglist": "off [fixed]",
                "tx_sctp_segmentation": "off [fixed]",
                "tx_tcp6_segmentation": "off [fixed]",
                "tx_tcp_ecn_segmentation": "off [fixed]",
                "tx_tcp_mangleid_segmentation": "off [fixed]",
                "tx_tcp_segmentation": "off [fixed]",
                "tx_tunnel_remcsum_segmentation": "off [fixed]",
                "tx_udp_segmentation": "off [fixed]",
                "tx_udp_tnl_csum_segmentation": "off [fixed]",
                "tx_udp_tnl_segmentation": "off [fixed]",
                "tx_vlan_offload": "off [fixed]",
                "tx_vlan_stag_hw_insert": "off [fixed]",
                "vlan_challenged": "off [fixed]"
            },
            "hw_timestamp_filters": [],
            "macaddress": "d0:37:45:ab:9b:65",
            "module": "rtl8821ae",
            "mtu": 1500,
            "pciid": "0000:06:00.0",
            "promisc": false,
            "timestamping": [],
            "type": "ether"
        },
        "discovered_interpreter_python": "/usr/bin/python3",
        "gather_subset": [
            "all"
        ],
        "module_setup": true
    },
    "changed": false
}
2025-07-07 10:34:09,346 p=143080 u=gpadmin n=ansible | G-243 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "content": "IyBSYXkgQ2x1c3RlciBWZXJzaW9uIEluZm9ybWF0aW9uCiMgR2VuZXJhdGVkIG9uOiAyMDI1LTA3LTA3VDE0OjMzOjM5WgoKUkFZX1ZFUlNJT049Mi40Ny4xClJBWV9ET0NLRVJfSU1BR0U9cmF5cHJvamVjdC9yYXk6Mi40Ny4xClBZVEhPTl9WRVJTSU9OPTMuMTEKTk9ERV9UWVBFPXdvcmtlcgpIT1NUTkFNRT1HLTI0MwpMQVNUX1VQREFURT0yMDI1LTA3LTA3VDE0OjMzOjM5Wgo=",
    "encoding": "base64",
    "source": "/home/gpadmin/ray_temp/version_info.env"
}
2025-07-07 10:34:09,348 p=143080 u=gpadmin n=ansible | G-244 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "content": "IyBSYXkgQ2x1c3RlciBWZXJzaW9uIEluZm9ybWF0aW9uCiMgR2VuZXJhdGVkIG9uOiAyMDI1LTA3LTA3VDE0OjMzOjM5WgoKUkFZX1ZFUlNJT049Mi40Ny4xClJBWV9ET0NLRVJfSU1BR0U9cmF5cHJvamVjdC9yYXk6Mi40Ny4xClBZVEhPTl9WRVJTSU9OPTMuMTEKTk9ERV9UWVBFPXdvcmtlcgpIT1NUTkFNRT1HLTI0NApMQVNUX1VQREFURT0yMDI1LTA3LTA3VDE0OjMzOjM5Wgo=",
    "encoding": "base64",
    "source": "/home/gpadmin/ray_temp/version_info.env"
}
2025-07-07 10:34:09,363 p=143080 u=gpadmin n=ansible | G-242 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "content": "IyBSYXkgQ2x1c3RlciBWZXJzaW9uIEluZm9ybWF0aW9uCiMgR2VuZXJhdGVkIG9uOiAyMDI1LTA3LTA3VDE0OjMzOjM5WgoKUkFZX1ZFUlNJT049Mi40Ny4xClJBWV9ET0NLRVJfSU1BR0U9cmF5cHJvamVjdC9yYXk6Mi40Ny4xClBZVEhPTl9WRVJTSU9OPTMuMTEKTk9ERV9UWVBFPXdvcmtlcgpIT1NUTkFNRT1HLTI0MgpMQVNUX1VQREFURT0yMDI1LTA3LTA3VDE0OjMzOjM5Wgo=",
    "encoding": "base64",
    "source": "/home/gpadmin/ray_temp/version_info.env"
}
2025-07-07 10:34:09,365 p=143080 u=gpadmin n=ansible | G-241 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "content": "IyBSYXkgQ2x1c3RlciBWZXJzaW9uIEluZm9ybWF0aW9uCiMgR2VuZXJhdGVkIG9uOiAyMDI1LTA3LTA3VDE0OjMzOjM5WgoKUkFZX1ZFUlNJT049Mi40Ny4xClJBWV9ET0NLRVJfSU1BR0U9cmF5cHJvamVjdC9yYXk6Mi40Ny4xClBZVEhPTl9WRVJTSU9OPTMuMTEKTk9ERV9UWVBFPXdvcmtlcgpIT1NUTkFNRT1HLTI0MQpMQVNUX1VQREFURT0yMDI1LTA3LTA3VDE0OjMzOjM5Wgo=",
    "encoding": "base64",
    "source": "/home/gpadmin/ray_temp/version_info.env"
}
2025-07-07 10:34:09,445 p=143080 u=gpadmin n=ansible | MASTER | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "content": "IyBSYXkgQ2x1c3RlciBWZXJzaW9uIEluZm9ybWF0aW9uCiMgR2VuZXJhdGVkIG9uOiAyMDI1LTA3LTA3VDE0OjMzOjM4WgoKUkFZX1ZFUlNJT049Mi40Ny4xClJBWV9ET0NLRVJfSU1BR0U9cmF5cHJvamVjdC9yYXk6Mi40Ny4xClBZVEhPTl9WRVJTSU9OPTMuMTEKTk9ERV9UWVBFPWhlYWQKSE9TVE5BTUU9TUFTVEVSCkxBU1RfVVBEQVRFPTIwMjUtMDctMDdUMTQ6MzM6MzhaCg==",
    "encoding": "base64",
    "source": "/home/gpadmin/ray_temp/version_info.env"
}
2025-07-07 10:34:21,856 p=143302 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] ***************************************************************************************
2025-07-07 10:34:21,871 p=143302 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************************************************************
2025-07-07 10:34:22,921 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:34:23,026 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:34:23,042 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:34:23,047 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:34:23,382 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:34:23,464 p=143302 u=gpadmin n=ansible | TASK [common : Update apt cache] ***********************************************************************************************
2025-07-07 10:34:23,964 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:34:23,965 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:34:23,966 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:34:23,969 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:34:24,425 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:34:24,438 p=143302 u=gpadmin n=ansible | TASK [common : Install common packages] ****************************************************************************************
2025-07-07 10:34:24,911 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:34:24,929 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:34:24,938 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:34:24,966 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:34:25,580 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:34:25,594 p=143302 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] *********************************************************************************
2025-07-07 10:34:25,887 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:34:25,888 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:34:25,896 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:34:25,899 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:34:26,025 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:34:26,038 p=143302 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] **********************************************************************************
2025-07-07 10:34:26,328 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:34:26,340 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:34:26,345 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:34:26,352 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:34:26,464 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:34:26,522 p=143302 u=gpadmin n=ansible | TASK [docker : Check if Docker is available via which command] *****************************************************************
2025-07-07 10:34:26,801 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:34:26,807 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:34:26,807 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:34:26,813 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:34:26,909 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:34:26,924 p=143302 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via apt] ***************************************************************************
2025-07-07 10:34:27,120 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:34:27,154 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:34:27,166 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:34:27,172 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:34:27,248 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:34:27,263 p=143302 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via snap] **************************************************************************
2025-07-07 10:34:27,468 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:34:27,481 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:34:27,501 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:34:27,521 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:34:27,608 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:34:27,622 p=143302 u=gpadmin n=ansible | TASK [docker : Set Docker installation status facts] ***************************************************************************
2025-07-07 10:34:27,668 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:34:27,684 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:34:27,705 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:34:27,708 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:34:27,725 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:34:27,735 p=143302 u=gpadmin n=ansible | TASK [docker : Display Docker installation status] *****************************************************************************
2025-07-07 10:34:27,772 p=143302 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: True",
        "Docker installed via snap: False"
    ]
}
2025-07-07 10:34:27,789 p=143302 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:34:27,804 p=143302 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:34:27,807 p=143302 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:34:27,824 p=143302 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:34:27,833 p=143302 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] *********************************************************************************************
2025-07-07 10:34:27,869 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:34:27,885 p=143302 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:34:27,900 p=143302 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:34:27,902 p=143302 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:34:27,913 p=143302 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:34:27,923 p=143302 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] **************************************************************************************
2025-07-07 10:34:27,947 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:34:27,980 p=143302 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:34:27,996 p=143302 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:34:27,996 p=143302 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:34:28,005 p=143302 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:34:28,015 p=143302 u=gpadmin n=ansible | TASK [docker : Update apt cache after adding Docker repository] ****************************************************************
2025-07-07 10:34:28,050 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:34:28,064 p=143302 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:34:28,080 p=143302 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:34:28,082 p=143302 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:34:28,089 p=143302 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:34:28,098 p=143302 u=gpadmin n=ansible | TASK [docker : Install Docker Engine via APT] **********************************************************************************
2025-07-07 10:34:28,119 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:34:28,135 p=143302 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:34:28,156 p=143302 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:34:28,214 p=143302 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:34:28,228 p=143302 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:34:28,238 p=143302 u=gpadmin n=ansible | TASK [docker : Check Docker service status (systemd)] **************************************************************************
2025-07-07 10:34:28,420 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:34:28,433 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:34:28,438 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:34:28,439 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:34:28,551 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:34:28,565 p=143302 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] *********************************************************************************
2025-07-07 10:34:28,624 p=143302 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:34:28,639 p=143302 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:34:28,641 p=143302 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:34:28,664 p=143302 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:34:29,280 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:34:29,293 p=143302 u=gpadmin n=ansible | TASK [docker : Install Docker via snap if not installed via apt] ***************************************************************
2025-07-07 10:34:29,322 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:34:29,340 p=143302 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:34:29,371 p=143302 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:34:29,373 p=143302 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:34:29,383 p=143302 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:34:29,393 p=143302 u=gpadmin n=ansible | TASK [docker : Wait for snap Docker installation to complete] ******************************************************************
2025-07-07 10:34:29,420 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:34:29,429 p=143302 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] ***********************************************************
2025-07-07 10:34:29,465 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:34:29,640 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:34:29,641 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:34:29,664 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:34:29,670 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:34:29,685 p=143302 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ************************************************************************************
2025-07-07 10:34:29,732 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:34:30,072 p=143302 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:35:03,073 p=143302 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:35:03,096 p=143302 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:35:03,232 p=143302 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:35:03,246 p=143302 u=gpadmin n=ansible | TASK [docker : Add user to docker group (for apt installation)] ****************************************************************
2025-07-07 10:35:03,306 p=143302 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:35:03,321 p=143302 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:35:03,323 p=143302 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:35:03,337 p=143302 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:35:03,752 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:03,761 p=143302 u=gpadmin n=ansible | TASK [docker : Wait for Docker to be ready] ************************************************************************************
2025-07-07 10:35:03,995 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:04,015 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:04,028 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:04,034 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:04,176 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:04,189 p=143302 u=gpadmin n=ansible | TASK [docker : Display Docker version] *****************************************************************************************
2025-07-07 10:35:04,431 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:04,439 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:04,454 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:04,462 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:04,526 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:04,540 p=143302 u=gpadmin n=ansible | TASK [docker : Show Docker version] ********************************************************************************************
2025-07-07 10:35:04,588 p=143302 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Docker version 28.3.1, build 38b7060"
}
2025-07-07 10:35:04,604 p=143302 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:35:04,622 p=143302 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:35:04,623 p=143302 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:35:04,633 p=143302 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:35:04,642 p=143302 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] ******************************************************************************************
2025-07-07 10:35:05,575 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:05,583 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:05,587 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:05,590 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:05,776 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:05,790 p=143302 u=gpadmin n=ansible | TASK [docker : Pull monitoring Docker images] **********************************************************************************
2025-07-07 10:35:06,527 p=143302 u=gpadmin n=ansible | ok: [G-242] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:35:06,550 p=143302 u=gpadmin n=ansible | ok: [G-243] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:35:06,581 p=143302 u=gpadmin n=ansible | ok: [G-241] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:35:06,606 p=143302 u=gpadmin n=ansible | ok: [G-244] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:35:06,710 p=143302 u=gpadmin n=ansible | ok: [MASTER] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:35:07,186 p=143302 u=gpadmin n=ansible | ok: [G-242] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:35:07,223 p=143302 u=gpadmin n=ansible | ok: [G-243] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:35:07,248 p=143302 u=gpadmin n=ansible | ok: [G-241] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:35:07,278 p=143302 u=gpadmin n=ansible | ok: [G-244] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:35:07,598 p=143302 u=gpadmin n=ansible | ok: [MASTER] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:35:08,083 p=143302 u=gpadmin n=ansible | ok: [G-242] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:35:08,114 p=143302 u=gpadmin n=ansible | ok: [G-241] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:35:08,121 p=143302 u=gpadmin n=ansible | ok: [G-244] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:35:08,300 p=143302 u=gpadmin n=ansible | ok: [G-243] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:35:08,677 p=143302 u=gpadmin n=ansible | ok: [MASTER] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:35:08,784 p=143302 u=gpadmin n=ansible | ok: [G-244] => (item=grafana/grafana:10.0.3)
2025-07-07 10:35:08,801 p=143302 u=gpadmin n=ansible | ok: [G-241] => (item=grafana/grafana:10.0.3)
2025-07-07 10:35:08,816 p=143302 u=gpadmin n=ansible | ok: [G-242] => (item=grafana/grafana:10.0.3)
2025-07-07 10:35:09,038 p=143302 u=gpadmin n=ansible | ok: [G-243] => (item=grafana/grafana:10.0.3)
2025-07-07 10:35:09,585 p=143302 u=gpadmin n=ansible | ok: [MASTER] => (item=grafana/grafana:10.0.3)
2025-07-07 10:35:09,645 p=143302 u=gpadmin n=ansible | TASK [version_control : Check current Ray container status] ********************************************************************
2025-07-07 10:35:10,058 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:10,062 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:10,071 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:10,075 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:10,252 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:10,265 p=143302 u=gpadmin n=ansible | TASK [version_control : Get current Ray container image if exists] *************************************************************
2025-07-07 10:35:10,311 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:10,326 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:10,342 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:10,346 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:10,365 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:10,377 p=143302 u=gpadmin n=ansible | TASK [version_control : Display current Ray image] *****************************************************************************
2025-07-07 10:35:10,413 p=143302 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:35:10,429 p=143302 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:35:10,446 p=143302 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:35:10,455 p=143302 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:35:10,471 p=143302 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:35:10,480 p=143302 u=gpadmin n=ansible | TASK [version_control : Check if Ray version matches target] *******************************************************************
2025-07-07 10:35:10,514 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:10,530 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:10,545 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:10,549 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:10,572 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:10,581 p=143302 u=gpadmin n=ansible | TASK [version_control : Pull target Ray Docker image] **************************************************************************
2025-07-07 10:35:10,619 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:35:10,636 p=143302 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:35:10,652 p=143302 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:35:10,657 p=143302 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:35:10,667 p=143302 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:35:10,676 p=143302 u=gpadmin n=ansible | TASK [version_control : Stop Ray head container if version mismatch] ***********************************************************
2025-07-07 10:35:10,711 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:35:10,728 p=143302 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:35:10,744 p=143302 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:35:10,749 p=143302 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:35:10,766 p=143302 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:35:10,775 p=143302 u=gpadmin n=ansible | TASK [version_control : Stop Ray worker container if version mismatch] *********************************************************
2025-07-07 10:35:10,810 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:35:10,825 p=143302 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:35:10,846 p=143302 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:35:10,848 p=143302 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:35:10,866 p=143302 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:35:10,875 p=143302 u=gpadmin n=ansible | TASK [version_control : Remove old Ray images to free space] *******************************************************************
2025-07-07 10:35:10,910 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:35:10,925 p=143302 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:35:10,941 p=143302 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:35:10,946 p=143302 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:35:10,962 p=143302 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:35:10,971 p=143302 u=gpadmin n=ansible | TASK [version_control : Start temporary Ray container to check Python version] *************************************************
2025-07-07 10:35:11,897 p=143302 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:35:11,909 p=143302 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:35:11,938 p=143302 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:35:11,942 p=143302 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:35:12,073 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:12,092 p=143302 u=gpadmin n=ansible | TASK [version_control : Extract Python version from Ray container] *************************************************************
2025-07-07 10:35:12,153 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:12,170 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:12,181 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:12,196 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:12,211 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:12,221 p=143302 u=gpadmin n=ansible | TASK [version_control : Display Python version compatibility] ******************************************************************
2025-07-07 10:35:12,288 p=143302 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Ray container Python version: Cannot retrieve result as auto_remove is enabled",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:35:12,293 p=143302 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Ray container Python version: Cannot retrieve result as auto_remove is enabled",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:35:12,307 p=143302 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Ray container Python version: Cannot retrieve result as auto_remove is enabled",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:35:12,325 p=143302 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Ray container Python version: Cannot retrieve result as auto_remove is enabled",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:35:12,376 p=143302 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Ray container Python version: Cannot retrieve result as auto_remove is enabled",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:35:12,391 p=143302 u=gpadmin n=ansible | TASK [version_control : Force pull latest Ray image if enforcing consistency] **************************************************
2025-07-07 10:35:13,138 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:13,187 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:13,193 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:13,195 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:13,331 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:13,345 p=143302 u=gpadmin n=ansible | TASK [version_control : Create version info file] ******************************************************************************
2025-07-07 10:35:13,949 p=143302 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:35:13,954 p=143302 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:35:13,969 p=143302 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:35:13,985 p=143302 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:35:14,055 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:14,064 p=143302 u=gpadmin n=ansible | TASK [version_control : Display version enforcement status] ********************************************************************
2025-07-07 10:35:14,100 p=143302 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:35:14,117 p=143302 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:35:14,137 p=143302 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:35:14,139 p=143302 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:35:14,155 p=143302 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:35:14,294 p=143302 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] *************************************************************************************************
2025-07-07 10:35:14,305 p=143302 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************************************************************
2025-07-07 10:35:15,548 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:15,583 p=143302 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] ***********************************************************
2025-07-07 10:35:15,903 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:15,917 p=143302 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ****************************************************
2025-07-07 10:35:16,573 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:16,588 p=143302 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] ***************************************************************
2025-07-07 10:35:16,882 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:16,898 p=143302 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] **********************************************************************
2025-07-07 10:35:17,460 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:17,475 p=143302 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] *************************************************************************************
2025-07-07 10:35:18,045 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:18,055 p=143302 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ****************************************************************************
2025-07-07 10:35:18,395 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:18,410 p=143302 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] ******************************************************************************
2025-07-07 10:35:18,437 p=143302 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED        STATUS                  PORTS     NAMES\n85cc4c9d4076   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_head"
}
2025-07-07 10:35:18,452 p=143302 u=gpadmin n=ansible | TASK [ray_head : Wait for a moment to let Ray head node initialize] ************************************************************
2025-07-07 10:35:18,472 p=143302 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 10:35:18,473 p=143302 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 10:35:28,477 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:28,492 p=143302 u=gpadmin n=ansible | TASK [ray_head : Check Ray head node status] ***********************************************************************************
2025-07-07 10:35:30,794 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:30,814 p=143302 u=gpadmin n=ansible | TASK [ray_head : Print Ray head node status] ***********************************************************************************
2025-07-07 10:35:30,861 p=143302 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:35:25.478847 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_ba28d9b4d77cc21caf144e2b889be2e8972d28f4bacc27971ea07f50",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/32.0 CPU",
        " 0B/100.53KiB memory",
        " 0B/9.31GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:35:30,918 p=143302 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] **********************************************************************************************
2025-07-07 10:35:30,926 p=143302 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************************************************************
2025-07-07 10:35:31,572 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:31,581 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:31,609 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:31,614 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:31,682 p=143302 u=gpadmin n=ansible | TASK [ray_worker : Ensure Ray temporary directory exists on worker node] *******************************************************
2025-07-07 10:35:31,838 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:31,856 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:31,861 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:31,880 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:31,894 p=143302 u=gpadmin n=ansible | TASK [ray_worker : Stop and remove existing Ray worker container (idempotency)] ************************************************
2025-07-07 10:35:32,264 p=143302 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:35:32,287 p=143302 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:35:32,335 p=143302 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:35:32,337 p=143302 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:35:32,350 p=143302 u=gpadmin n=ansible | TASK [ray_worker : Remove old Ray worker start script (idempotency)] ***********************************************************
2025-07-07 10:35:32,514 p=143302 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:35:32,549 p=143302 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:35:32,557 p=143302 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:35:32,581 p=143302 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:35:32,594 p=143302 u=gpadmin n=ansible | TASK [ray_worker : Copy Ray worker start script to worker node] ****************************************************************
2025-07-07 10:35:33,058 p=143302 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:35:33,081 p=143302 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:35:33,082 p=143302 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:35:33,127 p=143302 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:35:33,140 p=143302 u=gpadmin n=ansible | TASK [ray_worker : Start Ray worker container] *********************************************************************************
2025-07-07 10:35:33,691 p=143302 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:35:33,733 p=143302 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:35:33,740 p=143302 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:35:33,770 p=143302 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:35:33,783 p=143302 u=gpadmin n=ansible | TASK [ray_worker : Display Ray worker container status] ************************************************************************
2025-07-07 10:35:33,965 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:33,989 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:33,996 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:34,026 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:34,039 p=143302 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker container status] **************************************************************************
2025-07-07 10:35:34,087 p=143302 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n061532ff4708   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:35:34,103 p=143302 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED                  STATUS                  PORTS     NAMES\nec51a3981609   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:35:34,106 p=143302 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n6eff19ca2b1d   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:35:34,122 p=143302 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED        STATUS                  PORTS     NAMES\n3fc354e94e64   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:35:34,130 p=143302 u=gpadmin n=ansible | TASK [ray_worker : Wait for a moment to let Ray worker node initialize] ********************************************************
2025-07-07 10:35:34,150 p=143302 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 10:35:34,151 p=143302 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 10:35:44,156 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:44,173 p=143302 u=gpadmin n=ansible | TASK [ray_worker : Check Ray worker node status] *******************************************************************************
2025-07-07 10:35:45,234 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:45,280 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:45,292 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:45,294 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:45,315 p=143302 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker node status] *******************************************************************************
2025-07-07 10:35:45,377 p=143302 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:35:40.511215 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_ba28d9b4d77cc21caf144e2b889be2e8972d28f4bacc27971ea07f50",
        " 1 node_5c57e11cc28a8df1c420a2a8554c5fc4d1da1fb78b82030f1b0166a5",
        " 1 node_45795d7f8a1c2cbaa380b0e857088dab8e4696220e391166f8284f28",
        " 1 node_f19e124d74a1883245aa78b90e938b56cf958c1c4d14ac672874bd5f",
        " 1 node_39df6815b278438ba211ed57a9a00b647b64b522672e8603761a2f7d",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:35:45,381 p=143302 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:35:40.511215 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_ba28d9b4d77cc21caf144e2b889be2e8972d28f4bacc27971ea07f50",
        " 1 node_5c57e11cc28a8df1c420a2a8554c5fc4d1da1fb78b82030f1b0166a5",
        " 1 node_45795d7f8a1c2cbaa380b0e857088dab8e4696220e391166f8284f28",
        " 1 node_f19e124d74a1883245aa78b90e938b56cf958c1c4d14ac672874bd5f",
        " 1 node_39df6815b278438ba211ed57a9a00b647b64b522672e8603761a2f7d",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:35:45,397 p=143302 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:35:40.511215 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_ba28d9b4d77cc21caf144e2b889be2e8972d28f4bacc27971ea07f50",
        " 1 node_5c57e11cc28a8df1c420a2a8554c5fc4d1da1fb78b82030f1b0166a5",
        " 1 node_45795d7f8a1c2cbaa380b0e857088dab8e4696220e391166f8284f28",
        " 1 node_f19e124d74a1883245aa78b90e938b56cf958c1c4d14ac672874bd5f",
        " 1 node_39df6815b278438ba211ed57a9a00b647b64b522672e8603761a2f7d",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:35:45,409 p=143302 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:35:40.511215 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_ba28d9b4d77cc21caf144e2b889be2e8972d28f4bacc27971ea07f50",
        " 1 node_5c57e11cc28a8df1c420a2a8554c5fc4d1da1fb78b82030f1b0166a5",
        " 1 node_45795d7f8a1c2cbaa380b0e857088dab8e4696220e391166f8284f28",
        " 1 node_f19e124d74a1883245aa78b90e938b56cf958c1c4d14ac672874bd5f",
        " 1 node_39df6815b278438ba211ed57a9a00b647b64b522672e8603761a2f7d",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:35:45,563 p=143302 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] *******************************************************************************************
2025-07-07 10:35:45,585 p=143302 u=gpadmin n=ansible | TASK [Check Ray head node status] **********************************************************************************************
2025-07-07 10:35:47,685 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:47,699 p=143302 u=gpadmin n=ansible | TASK [Display Ray cluster status] **********************************************************************************************
2025-07-07 10:35:47,727 p=143302 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:35:45.522551 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_ba28d9b4d77cc21caf144e2b889be2e8972d28f4bacc27971ea07f50",
        " 1 node_5c57e11cc28a8df1c420a2a8554c5fc4d1da1fb78b82030f1b0166a5",
        " 1 node_45795d7f8a1c2cbaa380b0e857088dab8e4696220e391166f8284f28",
        " 1 node_f19e124d74a1883245aa78b90e938b56cf958c1c4d14ac672874bd5f",
        " 1 node_39df6815b278438ba211ed57a9a00b647b64b522672e8603761a2f7d",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:35:47,741 p=143302 u=gpadmin n=ansible | TASK [Display Ray cluster error] ***********************************************************************************************
2025-07-07 10:35:47,758 p=143302 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:35:47,799 p=143302 u=gpadmin n=ansible | PLAY [Deploy Monitoring Stack] *************************************************************************************************
2025-07-07 10:35:47,809 p=143302 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************************************************************
2025-07-07 10:35:48,650 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:48,689 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:48,753 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:48,810 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:49,065 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:49,142 p=143302 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus data directory] ***************************************************************************
2025-07-07 10:35:49,317 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:49,346 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:49,357 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:49,368 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:49,441 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:49,450 p=143302 u=gpadmin n=ansible | TASK [monitoring : Create Grafana data directory] ******************************************************************************
2025-07-07 10:35:49,642 p=143302 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:35:49,644 p=143302 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:35:49,654 p=143302 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:35:49,692 p=143302 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:35:49,742 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:49,755 p=143302 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Node Exporter container (idempotency)] *********************************************
2025-07-07 10:35:50,173 p=143302 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:35:50,175 p=143302 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:35:50,193 p=143302 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:35:50,214 p=143302 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:35:50,322 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:50,336 p=143302 u=gpadmin n=ansible | TASK [monitoring : Start Node Exporter container] ******************************************************************************
2025-07-07 10:35:50,843 p=143302 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:35:50,882 p=143302 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:35:50,883 p=143302 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:35:50,897 p=143302 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:35:51,000 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:51,014 p=143302 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing cAdvisor container (idempotency)] **************************************************
2025-07-07 10:35:51,331 p=143302 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:35:51,356 p=143302 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:35:51,363 p=143302 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:35:51,386 p=143302 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:35:51,617 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:51,631 p=143302 u=gpadmin n=ansible | TASK [monitoring : Start cAdvisor container] ***********************************************************************************
2025-07-07 10:35:52,095 p=143302 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Error starting container ed6fda158da0deb558815746e0a9f6a79dec9a1b18d972937b4bf0874d5f26a9: 500 Server Error for http+docker://localhost/v1.49/containers/ed6fda158da0deb558815746e0a9f6a79dec9a1b18d972937b4bf0874d5f26a9/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:35:52,122 p=143302 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Error starting container ab0b6e29acbc7c8ac6e337630582592c8c1021beb50fac4478b964526c9ccc2c: 500 Server Error for http+docker://localhost/v1.49/containers/ab0b6e29acbc7c8ac6e337630582592c8c1021beb50fac4478b964526c9ccc2c/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:35:52,141 p=143302 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Error starting container f78ba53c97a009779ff5aa5b33546288b1b9fac35d69f36def66851e6eca7afd: 500 Server Error for http+docker://localhost/v1.49/containers/f78ba53c97a009779ff5aa5b33546288b1b9fac35d69f36def66851e6eca7afd/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:35:52,144 p=143302 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Error starting container 69082fd59d8ae53c139bdd728a0ce1bff36909eaf90b376cee6c900b287922eb: 500 Server Error for http+docker://localhost/v1.49/containers/69082fd59d8ae53c139bdd728a0ce1bff36909eaf90b376cee6c900b287922eb/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:35:52,351 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:52,360 p=143302 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Prometheus container (idempotency)] ************************************************
2025-07-07 10:35:52,956 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:52,971 p=143302 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus configuration] ****************************************************************************
2025-07-07 10:35:53,623 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:53,640 p=143302 u=gpadmin n=ansible | TASK [monitoring : Start Prometheus container] *********************************************************************************
2025-07-07 10:35:54,334 p=143302 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:35:54,348 p=143302 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Grafana container (idempotency)] ***************************************************
2025-07-07 10:35:54,865 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:54,879 p=143302 u=gpadmin n=ansible | TASK [monitoring : Create Grafana provisioning directories] ********************************************************************
2025-07-07 10:35:55,180 p=143302 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/datasources)
2025-07-07 10:35:55,485 p=143302 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/dashboards)
2025-07-07 10:35:55,783 p=143302 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/dashboards)
2025-07-07 10:35:55,798 p=143302 u=gpadmin n=ansible | TASK [monitoring : Create Grafana datasource configuration] ********************************************************************
2025-07-07 10:35:56,388 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:56,402 p=143302 u=gpadmin n=ansible | TASK [monitoring : Create Grafana dashboard provider configuration] ************************************************************
2025-07-07 10:35:56,984 p=143302 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:35:56,998 p=143302 u=gpadmin n=ansible | TASK [monitoring : Copy Grafana dashboards] ************************************************************************************
2025-07-07 10:35:57,571 p=143302 u=gpadmin n=ansible | ok: [MASTER] => (item=node-exporter-dashboard.json)
2025-07-07 10:35:58,121 p=143302 u=gpadmin n=ansible | ok: [MASTER] => (item=docker-container-dashboard.json)
2025-07-07 10:35:58,663 p=143302 u=gpadmin n=ansible | ok: [MASTER] => (item=ray-cluster-dashboard.json)
2025-07-07 10:35:58,679 p=143302 u=gpadmin n=ansible | TASK [monitoring : Start Grafana container] ************************************************************************************
2025-07-07 10:35:59,186 p=143302 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Non-string value found for env option. Ambiguous env options must be wrapped in quotes to avoid them being interpreted. Key: GF_SERVER_HTTP_PORT"}
2025-07-07 10:35:59,187 p=143302 u=gpadmin n=ansible | PLAY RECAP *********************************************************************************************************************
2025-07-07 10:35:59,188 p=143302 u=gpadmin n=ansible | G-241                      : ok=45   changed=10   unreachable=0    failed=1    skipped=11   rescued=0    ignored=0   
2025-07-07 10:35:59,188 p=143302 u=gpadmin n=ansible | G-242                      : ok=44   changed=10   unreachable=0    failed=1    skipped=11   rescued=0    ignored=0   
2025-07-07 10:35:59,188 p=143302 u=gpadmin n=ansible | G-243                      : ok=44   changed=10   unreachable=0    failed=1    skipped=11   rescued=0    ignored=0   
2025-07-07 10:35:59,188 p=143302 u=gpadmin n=ansible | G-244                      : ok=44   changed=10   unreachable=0    failed=1    skipped=11   rescued=0    ignored=0   
2025-07-07 10:35:59,188 p=143302 u=gpadmin n=ansible | MASTER                     : ok=56   changed=12   unreachable=0    failed=1    skipped=13   rescued=0    ignored=0   
2025-07-07 10:37:55,542 p=150366 u=gpadmin n=ansible | MASTER | CHANGED | rc=0 >>
Validating Ray Docker image: rayproject/ray:2.47.1
Warning: Required Ray image rayproject/ray:2.47.1 not found locally
Pulling rayproject/ray:2.47.1...
2.47.1: Pulling from rayproject/ray
434c9928633c: Pulling fs layer
330e09f860a1: Pulling fs layer
6d13d74e978a: Pulling fs layer
4180ee28640a: Pulling fs layer
f557aa5ee224: Pulling fs layer
9cd39ae38cf5: Pulling fs layer
4f4fb700ef54: Pulling fs layer
78d3c6f12481: Pulling fs layer
ad32ef251b8c: Pulling fs layer
4f4fb700ef54: Already exists
78d3c6f12481: Download complete
434c9928633c: Download complete
f557aa5ee224: Download complete
f557aa5ee224: Pull complete
6d13d74e978a: Download complete
4180ee28640a: Download complete
330e09f860a1: Download complete
ad32ef251b8c: Download complete
9cd39ae38cf5: Download complete
78d3c6f12481: Pull complete
ad32ef251b8c: Pull complete
4f4fb700ef54: Pull complete
330e09f860a1: Pull complete
6d13d74e978a: Pull complete
4180ee28640a: Pull complete
434c9928633c: Pull complete
9cd39ae38cf5: Pull complete
Digest: sha256:e9950dadc62e8d9a2dd2b9bce8c785ab382af76d7103ad5a8d00efde0cb36c00
Status: Downloaded newer image for rayproject/ray:2.47.1
docker.io/rayproject/ray:2.47.1
51e6fa50953f21dac6251775b51cf2f3b48a6c529ac3ba1297c6ff3475cdee3e
Ray head node started on G-K3S-Master with container name ray_head/usr/local/bin/start_ray_head.sh: line 22: /home/gpadmin/ray_temp/start_ray_head.sh: Permission denied
chmod: changing permissions of '/home/gpadmin/ray_temp/start_ray_head.sh': Operation not permitted

2025-07-07 10:40:46,880 p=153312 u=gpadmin n=ansible | PLAY [Complete Ray Cluster Cleanup] ***************************************************************************
2025-07-07 10:40:46,889 p=153312 u=gpadmin n=ansible | TASK [Gathering Facts] ****************************************************************************************
2025-07-07 10:40:48,413 p=153312 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:40:48,560 p=153312 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:40:48,599 p=153312 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:40:48,602 p=153312 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:40:48,681 p=153312 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:40:48,757 p=153312 u=gpadmin n=ansible | TASK [Display cleanup information] ****************************************************************************
2025-07-07 10:40:48,802 p=153312 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "================================================",
        "COMPLETE RAY CLUSTER CLEANUP",
        "================================================",
        "This will remove ALL Ray containers and images",
        "Target nodes: 5",
        "================================================"
    ]
}
2025-07-07 10:40:48,816 p=153312 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "================================================",
        "COMPLETE RAY CLUSTER CLEANUP",
        "================================================",
        "This will remove ALL Ray containers and images",
        "Target nodes: 5",
        "================================================"
    ]
}
2025-07-07 10:40:48,827 p=153312 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "================================================",
        "COMPLETE RAY CLUSTER CLEANUP",
        "================================================",
        "This will remove ALL Ray containers and images",
        "Target nodes: 5",
        "================================================"
    ]
}
2025-07-07 10:40:48,836 p=153312 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "================================================",
        "COMPLETE RAY CLUSTER CLEANUP",
        "================================================",
        "This will remove ALL Ray containers and images",
        "Target nodes: 5",
        "================================================"
    ]
}
2025-07-07 10:40:48,855 p=153312 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "================================================",
        "COMPLETE RAY CLUSTER CLEANUP",
        "================================================",
        "This will remove ALL Ray containers and images",
        "Target nodes: 5",
        "================================================"
    ]
}
2025-07-07 10:40:48,863 p=153312 u=gpadmin n=ansible | TASK [Stop Ray head container] ********************************************************************************
2025-07-07 10:40:49,356 p=153312 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:49,356 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:49,358 p=153312 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:49,358 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:49,368 p=153312 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:49,368 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:49,378 p=153312 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:49,378 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:49,561 p=153312 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:49,562 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:49,574 p=153312 u=gpadmin n=ansible | TASK [Stop Ray worker container] ******************************************************************************
2025-07-07 10:40:49,857 p=153312 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:49,857 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:49,867 p=153312 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:49,867 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:49,886 p=153312 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:49,886 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:49,887 p=153312 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:49,887 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,116 p=153312 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:50,116 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,129 p=153312 u=gpadmin n=ansible | TASK [Stop any other Ray containers] **************************************************************************
2025-07-07 10:40:50,160 p=153312 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"msg": "template error while templating string: unexpected '.'. String: docker ps -a --filter \"name=ray\" --format \"{{.Names}}\" | xargs -r docker rm -f\n. unexpected '.'"}
2025-07-07 10:40:50,161 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,177 p=153312 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"msg": "template error while templating string: unexpected '.'. String: docker ps -a --filter \"name=ray\" --format \"{{.Names}}\" | xargs -r docker rm -f\n. unexpected '.'"}
2025-07-07 10:40:50,177 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,207 p=153312 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"msg": "template error while templating string: unexpected '.'. String: docker ps -a --filter \"name=ray\" --format \"{{.Names}}\" | xargs -r docker rm -f\n. unexpected '.'"}
2025-07-07 10:40:50,207 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,208 p=153312 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"msg": "template error while templating string: unexpected '.'. String: docker ps -a --filter \"name=ray\" --format \"{{.Names}}\" | xargs -r docker rm -f\n. unexpected '.'"}
2025-07-07 10:40:50,208 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,217 p=153312 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"msg": "template error while templating string: unexpected '.'. String: docker ps -a --filter \"name=ray\" --format \"{{.Names}}\" | xargs -r docker rm -f\n. unexpected '.'"}
2025-07-07 10:40:50,217 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,226 p=153312 u=gpadmin n=ansible | TASK [Stop Node Exporter container] ***************************************************************************
2025-07-07 10:40:50,496 p=153312 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:50,496 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,511 p=153312 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:50,511 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,527 p=153312 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:50,527 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,528 p=153312 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:50,528 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,728 p=153312 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:50,728 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:50,740 p=153312 u=gpadmin n=ansible | TASK [Stop cAdvisor container] ********************************************************************************
2025-07-07 10:40:51,006 p=153312 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:51,007 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:51,044 p=153312 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:51,044 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:51,050 p=153312 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:51,051 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:51,056 p=153312 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:51,056 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:51,241 p=153312 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:51,241 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:51,253 p=153312 u=gpadmin n=ansible | TASK [Stop Prometheus container (head node only)] *************************************************************
2025-07-07 10:40:51,297 p=153312 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:40:51,311 p=153312 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:40:51,327 p=153312 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:40:51,334 p=153312 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:40:51,750 p=153312 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:51,750 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:51,763 p=153312 u=gpadmin n=ansible | TASK [Stop Grafana container (head node only)] ****************************************************************
2025-07-07 10:40:51,810 p=153312 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:40:51,828 p=153312 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:40:51,844 p=153312 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:40:51,853 p=153312 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:40:52,256 p=153312 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Unsupported parameters for (community.docker.docker_container) module: force_delete. Supported parameters include: api_version, auto_remove, blkio_weight, ca_path, cap_drop, capabilities, cgroup_parent, cgroupns_mode, cleanup, client_cert, client_key, command, command_handling, comparisons, container_default_behavior, cpu_period, cpu_quota, cpu_shares, cpus, cpuset_cpus, cpuset_mems, debug, default_host_ip, detach, device_read_bps, device_read_iops, device_requests, device_write_bps, device_write_iops, devices, dns_opts, dns_search_domains, dns_servers, docker_host, domainname, entrypoint, env, env_file, etc_hosts, exposed_ports, force_kill, groups, healthcheck, hostname, ignore_image, image, image_comparison, image_label_mismatch, image_name_mismatch, init, interactive, ipc_mode, keep_volumes, kernel_memory, kill_signal, labels, links, log_driver, log_options, mac_address, memory, memory_reservation, memory_swap, memory_swappiness, mounts, name, network_mode, networks, networks_cli_compatible, oom_killer, oom_score_adj, output_logs, paused, pid_mode, pids_limit, platform, privileged, publish_all_ports, published_ports, pull, purge_networks, read_only, recreate, removal_wait_timeout, restart, restart_policy, restart_retries, runtime, security_opts, shm_size, ssl_version, state, stop_signal, stop_timeout, storage_opts, sysctls, timeout, tls, tls_hostname, tmpfs, tty, ulimits, use_ssh_client, user, userns_mode, uts, validate_certs, volume_driver, volumes, volumes_from, working_dir (ca_cert, cacert_path, cert_path, docker_api_version, docker_url, expose, exposed, forcekill, key_path, log_opt, ports, tls_ca_cert, tls_client_cert, tls_client_key, tls_verify)."}
2025-07-07 10:40:52,257 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:52,270 p=153312 u=gpadmin n=ansible | TASK [List Ray images] ****************************************************************************************
2025-07-07 10:40:52,298 p=153312 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"msg": "template error while templating string: unexpected '.'. String: docker images --filter \"reference=rayproject/ray:*\" --format \"{{.Repository}}:{{.Tag}}\". unexpected '.'"}
2025-07-07 10:40:52,299 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:52,316 p=153312 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"msg": "template error while templating string: unexpected '.'. String: docker images --filter \"reference=rayproject/ray:*\" --format \"{{.Repository}}:{{.Tag}}\". unexpected '.'"}
2025-07-07 10:40:52,316 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:52,330 p=153312 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"msg": "template error while templating string: unexpected '.'. String: docker images --filter \"reference=rayproject/ray:*\" --format \"{{.Repository}}:{{.Tag}}\". unexpected '.'"}
2025-07-07 10:40:52,331 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:52,346 p=153312 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"msg": "template error while templating string: unexpected '.'. String: docker images --filter \"reference=rayproject/ray:*\" --format \"{{.Repository}}:{{.Tag}}\". unexpected '.'"}
2025-07-07 10:40:52,346 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:52,352 p=153312 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"msg": "template error while templating string: unexpected '.'. String: docker images --filter \"reference=rayproject/ray:*\" --format \"{{.Repository}}:{{.Tag}}\". unexpected '.'"}
2025-07-07 10:40:52,353 p=153312 u=gpadmin n=ansible | ...ignoring
2025-07-07 10:40:52,361 p=153312 u=gpadmin n=ansible | TASK [Remove Ray images] **************************************************************************************
2025-07-07 10:40:52,395 p=153312 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:40:52,411 p=153312 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:40:52,427 p=153312 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:40:52,431 p=153312 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:40:52,446 p=153312 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:40:52,457 p=153312 u=gpadmin n=ansible | TASK [Remove monitoring images] *******************************************************************************
2025-07-07 10:40:53,011 p=153312 u=gpadmin n=ansible | changed: [G-241] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:40:53,018 p=153312 u=gpadmin n=ansible | changed: [G-242] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:40:53,022 p=153312 u=gpadmin n=ansible | changed: [G-244] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:40:53,029 p=153312 u=gpadmin n=ansible | changed: [G-243] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:40:53,185 p=153312 u=gpadmin n=ansible | changed: [MASTER] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:40:53,261 p=153312 u=gpadmin n=ansible | changed: [G-241] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:40:53,275 p=153312 u=gpadmin n=ansible | changed: [G-243] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:40:53,276 p=153312 u=gpadmin n=ansible | changed: [G-244] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:40:53,277 p=153312 u=gpadmin n=ansible | changed: [G-242] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:40:53,520 p=153312 u=gpadmin n=ansible | changed: [G-241] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:40:53,533 p=153312 u=gpadmin n=ansible | changed: [G-244] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:40:53,548 p=153312 u=gpadmin n=ansible | changed: [G-242] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:40:53,556 p=153312 u=gpadmin n=ansible | changed: [G-243] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:40:53,658 p=153312 u=gpadmin n=ansible | changed: [MASTER] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:40:53,867 p=153312 u=gpadmin n=ansible | changed: [G-241] => (item=grafana/grafana:10.0.3)
2025-07-07 10:40:53,889 p=153312 u=gpadmin n=ansible | changed: [G-244] => (item=grafana/grafana:10.0.3)
2025-07-07 10:40:53,898 p=153312 u=gpadmin n=ansible | changed: [G-242] => (item=grafana/grafana:10.0.3)
2025-07-07 10:40:53,906 p=153312 u=gpadmin n=ansible | changed: [G-243] => (item=grafana/grafana:10.0.3)
2025-07-07 10:40:54,125 p=153312 u=gpadmin n=ansible | changed: [MASTER] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:40:54,959 p=153312 u=gpadmin n=ansible | changed: [MASTER] => (item=grafana/grafana:10.0.3)
2025-07-07 10:40:54,974 p=153312 u=gpadmin n=ansible | TASK [Remove Ray temporary directory] *************************************************************************
2025-07-07 10:40:55,266 p=153312 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:40:55,276 p=153312 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:40:55,291 p=153312 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:40:55,295 p=153312 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:40:55,437 p=153312 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:40:55,453 p=153312 u=gpadmin n=ansible | TASK [Remove Ray start scripts] *******************************************************************************
2025-07-07 10:40:55,649 p=153312 u=gpadmin n=ansible | ok: [G-241] => (item=/usr/local/bin/start_ray_head.sh)
2025-07-07 10:40:55,665 p=153312 u=gpadmin n=ansible | ok: [G-242] => (item=/usr/local/bin/start_ray_head.sh)
2025-07-07 10:40:55,683 p=153312 u=gpadmin n=ansible | ok: [G-243] => (item=/usr/local/bin/start_ray_head.sh)
2025-07-07 10:40:55,692 p=153312 u=gpadmin n=ansible | ok: [G-244] => (item=/usr/local/bin/start_ray_head.sh)
2025-07-07 10:40:55,760 p=153312 u=gpadmin n=ansible | changed: [MASTER] => (item=/usr/local/bin/start_ray_head.sh)
2025-07-07 10:40:55,787 p=153312 u=gpadmin n=ansible | changed: [G-241] => (item=/usr/local/bin/start_ray_worker.sh)
2025-07-07 10:40:55,815 p=153312 u=gpadmin n=ansible | changed: [G-242] => (item=/usr/local/bin/start_ray_worker.sh)
2025-07-07 10:40:55,833 p=153312 u=gpadmin n=ansible | changed: [G-243] => (item=/usr/local/bin/start_ray_worker.sh)
2025-07-07 10:40:55,848 p=153312 u=gpadmin n=ansible | changed: [G-244] => (item=/usr/local/bin/start_ray_worker.sh)
2025-07-07 10:40:56,048 p=153312 u=gpadmin n=ansible | ok: [MASTER] => (item=/usr/local/bin/start_ray_worker.sh)
2025-07-07 10:40:56,063 p=153312 u=gpadmin n=ansible | TASK [Remove version info files] ******************************************************************************
2025-07-07 10:40:56,244 p=153312 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:40:56,267 p=153312 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:40:56,289 p=153312 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:40:56,295 p=153312 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:40:56,369 p=153312 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:40:56,382 p=153312 u=gpadmin n=ansible | TASK [Remove Prometheus data directory] ***********************************************************************
2025-07-07 10:40:56,471 p=153312 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:40:56,486 p=153312 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:40:56,501 p=153312 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:40:56,511 p=153312 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:40:56,687 p=153312 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:40:56,700 p=153312 u=gpadmin n=ansible | TASK [Remove Grafana data directory] **************************************************************************
2025-07-07 10:40:56,759 p=153312 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:40:56,775 p=153312 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:40:56,775 p=153312 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:40:56,782 p=153312 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:40:57,016 p=153312 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:40:57,029 p=153312 u=gpadmin n=ansible | TASK [Remove unused Docker images] ****************************************************************************
2025-07-07 10:40:57,356 p=153312 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:40:57,372 p=153312 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:40:57,378 p=153312 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:40:57,383 p=153312 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:40:57,497 p=153312 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:40:57,510 p=153312 u=gpadmin n=ansible | TASK [Remove unused Docker containers] ************************************************************************
2025-07-07 10:40:57,784 p=153312 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:40:57,788 p=153312 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:40:57,807 p=153312 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:40:57,837 p=153312 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:40:57,840 p=153312 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:40:57,854 p=153312 u=gpadmin n=ansible | TASK [Remove unused Docker volumes] ***************************************************************************
2025-07-07 10:40:58,093 p=153312 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:40:58,104 p=153312 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:40:58,119 p=153312 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:40:58,123 p=153312 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:40:58,176 p=153312 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:40:58,195 p=153312 u=gpadmin n=ansible | TASK [Remove unused Docker networks] **************************************************************************
2025-07-07 10:40:58,433 p=153312 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:40:58,449 p=153312 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:40:58,461 p=153312 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:40:58,468 p=153312 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:40:58,527 p=153312 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:40:58,542 p=153312 u=gpadmin n=ansible | TASK [Show remaining Docker images] ***************************************************************************
2025-07-07 10:40:58,768 p=153312 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:40:58,801 p=153312 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:40:58,801 p=153312 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:40:58,813 p=153312 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:40:58,893 p=153312 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:40:58,906 p=153312 u=gpadmin n=ansible | TASK [Show remaining Docker containers] ***********************************************************************
2025-07-07 10:40:59,131 p=153312 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:40:59,154 p=153312 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:40:59,168 p=153312 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:40:59,188 p=153312 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:40:59,228 p=153312 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:40:59,239 p=153312 u=gpadmin n=ansible | TASK [Display cleanup summary] ********************************************************************************
2025-07-07 10:40:59,302 p=153312 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "CLEANUP COMPLETE FOR: MASTER",
        "Remaining images: 8",
        "Remaining containers: 4"
    ]
}
2025-07-07 10:40:59,320 p=153312 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "CLEANUP COMPLETE FOR: G-241",
        "Remaining images: 4",
        "Remaining containers: 2"
    ]
}
2025-07-07 10:40:59,328 p=153312 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "CLEANUP COMPLETE FOR: G-242",
        "Remaining images: 4",
        "Remaining containers: 2"
    ]
}
2025-07-07 10:40:59,339 p=153312 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "CLEANUP COMPLETE FOR: G-243",
        "Remaining images: 4",
        "Remaining containers: 2"
    ]
}
2025-07-07 10:40:59,354 p=153312 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "CLEANUP COMPLETE FOR: G-244",
        "Remaining images: 4",
        "Remaining containers: 2"
    ]
}
2025-07-07 10:40:59,470 p=153312 u=gpadmin n=ansible | PLAY [Cleanup Summary Report] *********************************************************************************
2025-07-07 10:40:59,473 p=153312 u=gpadmin n=ansible | TASK [Gathering Facts] ****************************************************************************************
2025-07-07 10:41:00,770 p=153312 u=gpadmin n=ansible | ok: [localhost]
2025-07-07 10:41:00,804 p=153312 u=gpadmin n=ansible | TASK [Create cleanup report] **********************************************************************************
2025-07-07 10:41:01,485 p=153312 u=gpadmin n=ansible | changed: [localhost]
2025-07-07 10:41:01,497 p=153312 u=gpadmin n=ansible | TASK [Display cleanup completion] *****************************************************************************
2025-07-07 10:41:01,518 p=153312 u=gpadmin n=ansible | ok: [localhost] => {
    "msg": [
        "================================================",
        "CLEANUP COMPLETED SUCCESSFULLY",
        "================================================",
        "All Ray containers and images have been removed",
        "All configuration files have been cleaned up",
        "Ready for fresh installation",
        "================================================",
        "Next: Run 'ansible-playbook site.yml' to reinstall",
        "================================================"
    ]
}
2025-07-07 10:41:01,557 p=153312 u=gpadmin n=ansible | PLAY RECAP ****************************************************************************************************
2025-07-07 10:41:01,557 p=153312 u=gpadmin n=ansible | G-241                      : ok=19   changed=9    unreachable=0    failed=0    skipped=5    rescued=0    ignored=6   
2025-07-07 10:41:01,557 p=153312 u=gpadmin n=ansible | G-242                      : ok=19   changed=9    unreachable=0    failed=0    skipped=5    rescued=0    ignored=6   
2025-07-07 10:41:01,558 p=153312 u=gpadmin n=ansible | G-243                      : ok=19   changed=9    unreachable=0    failed=0    skipped=5    rescued=0    ignored=6   
2025-07-07 10:41:01,558 p=153312 u=gpadmin n=ansible | G-244                      : ok=19   changed=9    unreachable=0    failed=0    skipped=5    rescued=0    ignored=6   
2025-07-07 10:41:01,558 p=153312 u=gpadmin n=ansible | MASTER                     : ok=23   changed=11   unreachable=0    failed=0    skipped=1    rescued=0    ignored=8   
2025-07-07 10:41:01,558 p=153312 u=gpadmin n=ansible | localhost                  : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
2025-07-07 10:42:56,128 p=156556 u=gpadmin n=ansible | G-243 | FAILED | rc=1 >>
permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.48/containers/json?all=1": dial unix /var/run/docker.sock: connect: permission deniednon-zero return code

2025-07-07 10:42:56,146 p=156556 u=gpadmin n=ansible | G-242 | FAILED | rc=1 >>
permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.48/containers/json?all=1": dial unix /var/run/docker.sock: connect: permission deniednon-zero return code

2025-07-07 10:42:56,167 p=156556 u=gpadmin n=ansible | G-244 | FAILED | rc=1 >>
permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.48/containers/json?all=1": dial unix /var/run/docker.sock: connect: permission deniednon-zero return code

2025-07-07 10:42:56,208 p=156556 u=gpadmin n=ansible | G-241 | FAILED | rc=1 >>
permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.48/containers/json?all=1": dial unix /var/run/docker.sock: connect: permission deniednon-zero return code

2025-07-07 10:43:01,622 p=156709 u=gpadmin n=ansible | G-243 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                   COMMAND                  CREATED         STATUS         PORTS     NAMES
a47ec76c2d00   458e026e6aa6            "/bin/node_exporter …"   7 minutes ago   Up 7 minutes             node-exporter
6eff19ca2b1d   rayproject/ray:2.47.1   "/home/gpadmin/ray_t…"   7 minutes ago   Up 7 minutes             ray_worker

2025-07-07 10:43:01,629 p=156709 u=gpadmin n=ansible | G-244 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                   COMMAND                  CREATED         STATUS         PORTS     NAMES
d658fb6de1b6   458e026e6aa6            "/bin/node_exporter …"   7 minutes ago   Up 7 minutes             node-exporter
3fc354e94e64   rayproject/ray:2.47.1   "/home/gpadmin/ray_t…"   7 minutes ago   Up 7 minutes             ray_worker

2025-07-07 10:43:01,632 p=156709 u=gpadmin n=ansible | G-241 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                   COMMAND                  CREATED         STATUS         PORTS     NAMES
bfc9d8570842   458e026e6aa6            "/bin/node_exporter …"   7 minutes ago   Up 7 minutes             node-exporter
061532ff4708   rayproject/ray:2.47.1   "/home/gpadmin/ray_t…"   7 minutes ago   Up 7 minutes             ray_worker

2025-07-07 10:43:01,644 p=156709 u=gpadmin n=ansible | G-242 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                   COMMAND                  CREATED         STATUS         PORTS     NAMES
d31583443a30   458e026e6aa6            "/bin/node_exporter …"   7 minutes ago   Up 7 minutes             node-exporter
ec51a3981609   rayproject/ray:2.47.1   "/home/gpadmin/ray_t…"   7 minutes ago   Up 7 minutes             ray_worker

2025-07-07 10:43:08,145 p=156820 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] **********************************************************************
2025-07-07 10:43:08,158 p=156820 u=gpadmin n=ansible | TASK [Gathering Facts] ****************************************************************************************
2025-07-07 10:43:09,287 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:09,292 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:09,314 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:09,337 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:09,611 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:09,687 p=156820 u=gpadmin n=ansible | TASK [common : Update apt cache] ******************************************************************************
2025-07-07 10:43:10,175 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:10,176 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:10,179 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:10,186 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:10,571 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:10,584 p=156820 u=gpadmin n=ansible | TASK [common : Install common packages] ***********************************************************************
2025-07-07 10:43:11,034 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:11,063 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:11,077 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:11,090 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:11,682 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:11,696 p=156820 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] ****************************************************************
2025-07-07 10:43:11,987 p=156820 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:43:11,988 p=156820 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:43:11,991 p=156820 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:43:12,001 p=156820 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:43:12,109 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:43:12,118 p=156820 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] *****************************************************************
2025-07-07 10:43:12,387 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:12,392 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:12,400 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:12,403 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:12,499 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:12,559 p=156820 u=gpadmin n=ansible | TASK [docker : Check if Docker is available via which command] ************************************************
2025-07-07 10:43:12,882 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:12,885 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:12,897 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:12,901 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:12,995 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:13,010 p=156820 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via apt] **********************************************************
2025-07-07 10:43:13,233 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:13,241 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:13,254 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:13,268 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:13,349 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:13,363 p=156820 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via snap] *********************************************************
2025-07-07 10:43:13,575 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:13,598 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:13,615 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:13,618 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:13,711 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:13,725 p=156820 u=gpadmin n=ansible | TASK [docker : Set Docker installation status facts] **********************************************************
2025-07-07 10:43:13,754 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:13,772 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:13,808 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:13,809 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:13,827 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:13,836 p=156820 u=gpadmin n=ansible | TASK [docker : Display Docker installation status] ************************************************************
2025-07-07 10:43:13,876 p=156820 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: True",
        "Docker installed via snap: False"
    ]
}
2025-07-07 10:43:13,893 p=156820 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:43:13,908 p=156820 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:43:13,910 p=156820 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:43:13,927 p=156820 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:43:13,936 p=156820 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] ****************************************************************************
2025-07-07 10:43:13,971 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:43:13,988 p=156820 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:43:14,003 p=156820 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:43:14,003 p=156820 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:43:14,014 p=156820 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:43:14,023 p=156820 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] *********************************************************************
2025-07-07 10:43:14,049 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:43:14,081 p=156820 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:43:14,098 p=156820 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:43:14,099 p=156820 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:43:14,110 p=156820 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:43:14,119 p=156820 u=gpadmin n=ansible | TASK [docker : Update apt cache after adding Docker repository] ***********************************************
2025-07-07 10:43:14,156 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:43:14,171 p=156820 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:43:14,186 p=156820 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:43:14,187 p=156820 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:43:14,199 p=156820 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:43:14,210 p=156820 u=gpadmin n=ansible | TASK [docker : Install Docker Engine via APT] *****************************************************************
2025-07-07 10:43:14,246 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:43:14,266 p=156820 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:43:14,324 p=156820 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:43:14,325 p=156820 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:43:14,337 p=156820 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:43:14,347 p=156820 u=gpadmin n=ansible | TASK [docker : Check Docker service status (systemd)] *********************************************************
2025-07-07 10:43:14,523 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:14,541 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:14,555 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:14,564 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:14,653 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:14,668 p=156820 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] ****************************************************************
2025-07-07 10:43:14,731 p=156820 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:43:14,748 p=156820 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:43:14,750 p=156820 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:43:14,767 p=156820 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:43:15,347 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:15,361 p=156820 u=gpadmin n=ansible | TASK [docker : Install Docker via snap if not installed via apt] **********************************************
2025-07-07 10:43:15,392 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:43:15,426 p=156820 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:43:15,442 p=156820 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:43:15,444 p=156820 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:43:15,451 p=156820 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:43:15,462 p=156820 u=gpadmin n=ansible | TASK [docker : Wait for snap Docker installation to complete] *************************************************
2025-07-07 10:43:15,489 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:43:15,499 p=156820 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] ******************************************
2025-07-07 10:43:15,534 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:43:15,693 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:15,725 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:15,735 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:15,748 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:15,770 p=156820 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] *******************************************************************
2025-07-07 10:43:15,818 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:43:16,140 p=156820 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:43:49,187 p=156820 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:43:49,210 p=156820 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:43:49,278 p=156820 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:43:49,291 p=156820 u=gpadmin n=ansible | TASK [docker : Add user to docker group (for apt installation)] ***********************************************
2025-07-07 10:43:49,351 p=156820 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:43:49,367 p=156820 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:43:49,371 p=156820 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:43:49,383 p=156820 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:43:49,791 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:49,805 p=156820 u=gpadmin n=ansible | TASK [docker : Wait for Docker to be ready] *******************************************************************
2025-07-07 10:43:50,052 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:50,063 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:50,073 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:50,079 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:50,220 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:50,235 p=156820 u=gpadmin n=ansible | TASK [docker : Display Docker version] ************************************************************************
2025-07-07 10:43:50,457 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:50,476 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:50,478 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:50,508 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:50,569 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:50,582 p=156820 u=gpadmin n=ansible | TASK [docker : Show Docker version] ***************************************************************************
2025-07-07 10:43:50,611 p=156820 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Docker version 28.3.1, build 38b7060"
}
2025-07-07 10:43:50,632 p=156820 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:43:50,662 p=156820 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:43:50,664 p=156820 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:43:50,674 p=156820 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:43:50,684 p=156820 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] *************************************************************************
2025-07-07 10:43:51,630 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:43:51,642 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:43:51,656 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:43:51,660 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:43:51,788 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:43:51,802 p=156820 u=gpadmin n=ansible | TASK [docker : Pull monitoring Docker images] *****************************************************************
2025-07-07 10:43:52,720 p=156820 u=gpadmin n=ansible | changed: [MASTER] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:43:53,730 p=156820 u=gpadmin n=ansible | changed: [MASTER] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:43:57,032 p=156820 u=gpadmin n=ansible | changed: [G-244] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:43:57,234 p=156820 u=gpadmin n=ansible | changed: [G-241] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:43:57,709 p=156820 u=gpadmin n=ansible | changed: [G-243] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:43:57,772 p=156820 u=gpadmin n=ansible | changed: [G-244] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:43:57,917 p=156820 u=gpadmin n=ansible | changed: [G-241] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:43:58,039 p=156820 u=gpadmin n=ansible | changed: [G-242] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:43:58,467 p=156820 u=gpadmin n=ansible | changed: [G-243] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:43:58,754 p=156820 u=gpadmin n=ansible | changed: [G-242] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:44:01,485 p=156820 u=gpadmin n=ansible | changed: [G-241] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:44:01,547 p=156820 u=gpadmin n=ansible | changed: [G-244] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:44:01,762 p=156820 u=gpadmin n=ansible | changed: [G-243] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:44:02,066 p=156820 u=gpadmin n=ansible | changed: [G-242] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:44:05,259 p=156820 u=gpadmin n=ansible | changed: [MASTER] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:44:06,628 p=156820 u=gpadmin n=ansible | changed: [G-244] => (item=grafana/grafana:10.0.3)
2025-07-07 10:44:06,784 p=156820 u=gpadmin n=ansible | changed: [G-241] => (item=grafana/grafana:10.0.3)
2025-07-07 10:44:07,594 p=156820 u=gpadmin n=ansible | changed: [G-242] => (item=grafana/grafana:10.0.3)
2025-07-07 10:44:08,000 p=156820 u=gpadmin n=ansible | changed: [G-243] => (item=grafana/grafana:10.0.3)
2025-07-07 10:44:11,563 p=156820 u=gpadmin n=ansible | changed: [MASTER] => (item=grafana/grafana:10.0.3)
2025-07-07 10:44:11,622 p=156820 u=gpadmin n=ansible | TASK [version_control : Check current Ray container status] ***************************************************
2025-07-07 10:44:12,025 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:12,036 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:12,039 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:12,042 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:12,205 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:12,219 p=156820 u=gpadmin n=ansible | TASK [version_control : Get current Ray container image if exists] ********************************************
2025-07-07 10:44:12,263 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:12,278 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:12,295 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:12,304 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:12,322 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:12,333 p=156820 u=gpadmin n=ansible | TASK [version_control : Display current Ray image] ************************************************************
2025-07-07 10:44:12,369 p=156820 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:44:12,385 p=156820 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:44:12,401 p=156820 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:44:12,404 p=156820 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:44:12,420 p=156820 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:44:12,429 p=156820 u=gpadmin n=ansible | TASK [version_control : Check if Ray version matches target] **************************************************
2025-07-07 10:44:12,478 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:12,493 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:12,495 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:12,503 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:12,518 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:12,527 p=156820 u=gpadmin n=ansible | TASK [version_control : Pull target Ray Docker image] *********************************************************
2025-07-07 10:44:12,566 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:44:12,582 p=156820 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:44:12,599 p=156820 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:44:12,602 p=156820 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:44:12,620 p=156820 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:44:12,629 p=156820 u=gpadmin n=ansible | TASK [version_control : Stop Ray head container if version mismatch] ******************************************
2025-07-07 10:44:12,665 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:44:12,682 p=156820 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:44:12,698 p=156820 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:44:12,702 p=156820 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:44:12,713 p=156820 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:44:12,722 p=156820 u=gpadmin n=ansible | TASK [version_control : Stop Ray worker container if version mismatch] ****************************************
2025-07-07 10:44:12,757 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:44:12,773 p=156820 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:44:12,793 p=156820 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:44:12,796 p=156820 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:44:12,814 p=156820 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:44:12,824 p=156820 u=gpadmin n=ansible | TASK [version_control : Remove old Ray images to free space] **************************************************
2025-07-07 10:44:12,861 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:44:12,877 p=156820 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:44:12,894 p=156820 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:44:12,898 p=156820 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:44:12,914 p=156820 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:44:12,924 p=156820 u=gpadmin n=ansible | TASK [version_control : Check Python version in Ray container] ************************************************
2025-07-07 10:44:13,714 p=156820 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:44:13,720 p=156820 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:44:13,729 p=156820 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:44:13,748 p=156820 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:44:13,765 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:13,780 p=156820 u=gpadmin n=ansible | TASK [version_control : Extract Python version from Ray container] ********************************************
2025-07-07 10:44:13,850 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:13,868 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:13,875 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:13,892 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:13,910 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:13,920 p=156820 u=gpadmin n=ansible | TASK [version_control : Display Python version compatibility] *************************************************
2025-07-07 10:44:13,987 p=156820 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Ray container Python version: 3.9",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:44:13,989 p=156820 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Ray container Python version: 3.9",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:44:13,999 p=156820 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Ray container Python version: 3.9",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:44:14,018 p=156820 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Ray container Python version: 3.9",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:44:14,035 p=156820 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Ray container Python version: 3.9",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:44:14,045 p=156820 u=gpadmin n=ansible | TASK [version_control : Warn about Python version mismatch] ***************************************************
2025-07-07 10:44:14,166 p=156820 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "⚠️  WARNING: Python version mismatch detected!",
        "Container has Python 3.9",
        "Target is Python 3.11",
        "This may cause client connection issues"
    ]
}
2025-07-07 10:44:14,167 p=156820 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "⚠️  WARNING: Python version mismatch detected!",
        "Container has Python 3.9",
        "Target is Python 3.11",
        "This may cause client connection issues"
    ]
}
2025-07-07 10:44:14,179 p=156820 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "⚠️  WARNING: Python version mismatch detected!",
        "Container has Python 3.9",
        "Target is Python 3.11",
        "This may cause client connection issues"
    ]
}
2025-07-07 10:44:14,201 p=156820 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "⚠️  WARNING: Python version mismatch detected!",
        "Container has Python 3.9",
        "Target is Python 3.11",
        "This may cause client connection issues"
    ]
}
2025-07-07 10:44:14,211 p=156820 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "⚠️  WARNING: Python version mismatch detected!",
        "Container has Python 3.9",
        "Target is Python 3.11",
        "This may cause client connection issues"
    ]
}
2025-07-07 10:44:14,225 p=156820 u=gpadmin n=ansible | TASK [version_control : Force pull latest Ray image if enforcing consistency] *********************************
2025-07-07 10:44:14,951 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:14,968 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:14,985 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:15,007 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:15,102 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:15,117 p=156820 u=gpadmin n=ansible | TASK [version_control : Create version info file] *************************************************************
2025-07-07 10:44:15,711 p=156820 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:44:15,712 p=156820 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:44:15,722 p=156820 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:44:15,722 p=156820 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:44:15,799 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:15,824 p=156820 u=gpadmin n=ansible | TASK [version_control : Display version enforcement status] ***************************************************
2025-07-07 10:44:15,868 p=156820 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:44:15,886 p=156820 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:44:15,911 p=156820 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:44:15,912 p=156820 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:44:15,934 p=156820 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:44:16,079 p=156820 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] ********************************************************************************
2025-07-07 10:44:16,089 p=156820 u=gpadmin n=ansible | TASK [Gathering Facts] ****************************************************************************************
2025-07-07 10:44:17,344 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:17,387 p=156820 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] ******************************************
2025-07-07 10:44:17,694 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:17,708 p=156820 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ***********************************
2025-07-07 10:44:18,595 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:18,611 p=156820 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] **********************************************
2025-07-07 10:44:18,907 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:18,923 p=156820 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] *****************************************************
2025-07-07 10:44:19,476 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:19,491 p=156820 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] ********************************************************************
2025-07-07 10:44:20,038 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:20,053 p=156820 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ***********************************************************
2025-07-07 10:44:20,367 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:20,386 p=156820 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] *************************************************************
2025-07-07 10:44:20,406 p=156820 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED        STATUS                  PORTS     NAMES\ne6e8ea559ba0   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_head"
}
2025-07-07 10:44:20,422 p=156820 u=gpadmin n=ansible | TASK [ray_head : Wait for a moment to let Ray head node initialize] *******************************************
2025-07-07 10:44:20,437 p=156820 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 10:44:20,438 p=156820 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 10:44:30,442 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:30,459 p=156820 u=gpadmin n=ansible | TASK [ray_head : Check Ray head node status] ******************************************************************
2025-07-07 10:44:32,691 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:32,709 p=156820 u=gpadmin n=ansible | TASK [ray_head : Print Ray head node status] ******************************************************************
2025-07-07 10:44:32,753 p=156820 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:44:27.480744 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_29339dd52e652d203f97244dbe50264a6ed091a323a65dada6278e94",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/32.0 CPU",
        " 0B/100.53KiB memory",
        " 0B/9.31GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:44:32,814 p=156820 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] *****************************************************************************
2025-07-07 10:44:32,823 p=156820 u=gpadmin n=ansible | TASK [Gathering Facts] ****************************************************************************************
2025-07-07 10:44:33,472 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:33,479 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:33,484 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:33,493 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:33,561 p=156820 u=gpadmin n=ansible | TASK [ray_worker : Ensure Ray temporary directory exists on worker node] **************************************
2025-07-07 10:44:33,712 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:33,731 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:33,745 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:33,758 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:33,770 p=156820 u=gpadmin n=ansible | TASK [ray_worker : Stop and remove existing Ray worker container (idempotency)] *******************************
2025-07-07 10:44:34,159 p=156820 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:44:34,177 p=156820 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:44:34,213 p=156820 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:44:34,229 p=156820 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:44:34,242 p=156820 u=gpadmin n=ansible | TASK [ray_worker : Remove old Ray worker start script (idempotency)] ******************************************
2025-07-07 10:44:34,404 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:34,424 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:34,440 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:34,460 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:34,473 p=156820 u=gpadmin n=ansible | TASK [ray_worker : Copy Ray worker start script to worker node] ***********************************************
2025-07-07 10:44:34,925 p=156820 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:44:34,961 p=156820 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:44:34,969 p=156820 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:44:34,973 p=156820 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:44:34,983 p=156820 u=gpadmin n=ansible | TASK [ray_worker : Start Ray worker container] ****************************************************************
2025-07-07 10:44:35,540 p=156820 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:44:35,566 p=156820 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:44:35,578 p=156820 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:44:35,592 p=156820 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:44:35,604 p=156820 u=gpadmin n=ansible | TASK [ray_worker : Display Ray worker container status] *******************************************************
2025-07-07 10:44:35,797 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:35,830 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:35,847 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:35,854 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:35,867 p=156820 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker container status] *********************************************************
2025-07-07 10:44:35,920 p=156820 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n397ff60fffc1   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:44:35,938 p=156820 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n78c7b294f8f6   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:44:35,939 p=156820 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n07902eb5c74e   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:44:35,956 p=156820 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED                  STATUS                  PORTS     NAMES\ne269d0465797   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:44:35,964 p=156820 u=gpadmin n=ansible | TASK [ray_worker : Wait for a moment to let Ray worker node initialize] ***************************************
2025-07-07 10:44:35,985 p=156820 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 10:44:35,986 p=156820 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 10:44:45,991 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:46,003 p=156820 u=gpadmin n=ansible | TASK [ray_worker : Check Ray worker node status] **************************************************************
2025-07-07 10:44:47,072 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:47,095 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:47,101 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:47,126 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:47,141 p=156820 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker node status] **************************************************************
2025-07-07 10:44:47,187 p=156820 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:44:42.513438 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_18a066d7a5faf096cd2c72aff02e459a31399e1a58175885f2b71907",
        " 1 node_07a1bcb232423e241c92bb5a7392226a1a1b896a3c496112b996cc87",
        " 1 node_29339dd52e652d203f97244dbe50264a6ed091a323a65dada6278e94",
        " 1 node_f677e1a571e71ba4cc0364c925c8a118fc87d50fbee7224d9dcd08a6",
        " 1 node_aa32c678e3f88795a7ebbe4bab83811a6c662e4821ccfb196a72e53a",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:44:47,211 p=156820 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:44:42.513438 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_18a066d7a5faf096cd2c72aff02e459a31399e1a58175885f2b71907",
        " 1 node_07a1bcb232423e241c92bb5a7392226a1a1b896a3c496112b996cc87",
        " 1 node_29339dd52e652d203f97244dbe50264a6ed091a323a65dada6278e94",
        " 1 node_f677e1a571e71ba4cc0364c925c8a118fc87d50fbee7224d9dcd08a6",
        " 1 node_aa32c678e3f88795a7ebbe4bab83811a6c662e4821ccfb196a72e53a",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:44:47,280 p=156820 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:44:42.513438 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_18a066d7a5faf096cd2c72aff02e459a31399e1a58175885f2b71907",
        " 1 node_07a1bcb232423e241c92bb5a7392226a1a1b896a3c496112b996cc87",
        " 1 node_29339dd52e652d203f97244dbe50264a6ed091a323a65dada6278e94",
        " 1 node_f677e1a571e71ba4cc0364c925c8a118fc87d50fbee7224d9dcd08a6",
        " 1 node_aa32c678e3f88795a7ebbe4bab83811a6c662e4821ccfb196a72e53a",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:44:47,295 p=156820 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:44:42.513438 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_18a066d7a5faf096cd2c72aff02e459a31399e1a58175885f2b71907",
        " 1 node_07a1bcb232423e241c92bb5a7392226a1a1b896a3c496112b996cc87",
        " 1 node_29339dd52e652d203f97244dbe50264a6ed091a323a65dada6278e94",
        " 1 node_f677e1a571e71ba4cc0364c925c8a118fc87d50fbee7224d9dcd08a6",
        " 1 node_aa32c678e3f88795a7ebbe4bab83811a6c662e4821ccfb196a72e53a",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:44:47,467 p=156820 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] **************************************************************************
2025-07-07 10:44:47,487 p=156820 u=gpadmin n=ansible | TASK [Check Ray head node status] *****************************************************************************
2025-07-07 10:44:49,608 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:49,623 p=156820 u=gpadmin n=ansible | TASK [Display Ray cluster status] *****************************************************************************
2025-07-07 10:44:49,652 p=156820 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:44:47.526307 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_18a066d7a5faf096cd2c72aff02e459a31399e1a58175885f2b71907",
        " 1 node_07a1bcb232423e241c92bb5a7392226a1a1b896a3c496112b996cc87",
        " 1 node_29339dd52e652d203f97244dbe50264a6ed091a323a65dada6278e94",
        " 1 node_f677e1a571e71ba4cc0364c925c8a118fc87d50fbee7224d9dcd08a6",
        " 1 node_aa32c678e3f88795a7ebbe4bab83811a6c662e4821ccfb196a72e53a",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:44:49,669 p=156820 u=gpadmin n=ansible | TASK [Display Ray cluster error] ******************************************************************************
2025-07-07 10:44:49,691 p=156820 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:44:49,732 p=156820 u=gpadmin n=ansible | PLAY [Deploy Monitoring Stack] ********************************************************************************
2025-07-07 10:44:49,743 p=156820 u=gpadmin n=ansible | TASK [Gathering Facts] ****************************************************************************************
2025-07-07 10:44:50,630 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:50,636 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:50,672 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:50,683 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:50,919 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:51,003 p=156820 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus data directory] **********************************************************
2025-07-07 10:44:51,197 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:51,198 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:51,212 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:51,226 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:51,292 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:51,307 p=156820 u=gpadmin n=ansible | TASK [monitoring : Create Grafana data directory] *************************************************************
2025-07-07 10:44:51,496 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:51,516 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:51,551 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:51,557 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:51,604 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:51,618 p=156820 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Node Exporter container (idempotency)] ****************************
2025-07-07 10:44:52,020 p=156820 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:44:52,036 p=156820 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:44:52,041 p=156820 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:44:52,066 p=156820 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:44:52,304 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:52,319 p=156820 u=gpadmin n=ansible | TASK [monitoring : Start Node Exporter container] *************************************************************
2025-07-07 10:44:52,821 p=156820 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:44:52,864 p=156820 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:44:52,865 p=156820 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:44:52,879 p=156820 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:44:52,967 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:52,985 p=156820 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing cAdvisor container (idempotency)] *********************************
2025-07-07 10:44:53,276 p=156820 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:44:53,299 p=156820 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:44:53,303 p=156820 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:44:53,317 p=156820 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:44:53,565 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:53,580 p=156820 u=gpadmin n=ansible | TASK [monitoring : Start cAdvisor container] ******************************************************************
2025-07-07 10:44:54,050 p=156820 u=gpadmin n=ansible | fatal: [G-241]: FAILED! => {"changed": false, "msg": "Error starting container 7b97c073173a3fab9626d2bf1d5c3e4609e7b33200b60acd14ca386de5deb2c9: 500 Server Error for http+docker://localhost/v1.49/containers/7b97c073173a3fab9626d2bf1d5c3e4609e7b33200b60acd14ca386de5deb2c9/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:44:54,068 p=156820 u=gpadmin n=ansible | fatal: [G-243]: FAILED! => {"changed": false, "msg": "Error starting container b948deadb986c0d0c8645da927ffc1d903aba4ab05b387985d2f19336a7ca357: 500 Server Error for http+docker://localhost/v1.49/containers/b948deadb986c0d0c8645da927ffc1d903aba4ab05b387985d2f19336a7ca357/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:44:54,069 p=156820 u=gpadmin n=ansible | fatal: [G-242]: FAILED! => {"changed": false, "msg": "Error starting container 3ed27262d2e7239b5eb338eed1c107fa7c2f17a0b125ff6228be286942411f1c: 500 Server Error for http+docker://localhost/v1.49/containers/3ed27262d2e7239b5eb338eed1c107fa7c2f17a0b125ff6228be286942411f1c/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:44:54,112 p=156820 u=gpadmin n=ansible | fatal: [G-244]: FAILED! => {"changed": false, "msg": "Error starting container 3f3c4563f0a5103c82a4ee7152ee76953455fc01a0ea823f374d737297335085: 500 Server Error for http+docker://localhost/v1.49/containers/3f3c4563f0a5103c82a4ee7152ee76953455fc01a0ea823f374d737297335085/start: Internal Server Error (\"error while creating mount source path '/var/lib/docker': mkdir /var/lib/docker: read-only file system\")"}
2025-07-07 10:44:54,262 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:54,276 p=156820 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Prometheus container (idempotency)] *******************************
2025-07-07 10:44:54,858 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:54,872 p=156820 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus configuration] ***********************************************************
2025-07-07 10:44:55,465 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:55,474 p=156820 u=gpadmin n=ansible | TASK [monitoring : Start Prometheus container] ****************************************************************
2025-07-07 10:44:56,164 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:56,173 p=156820 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Grafana container (idempotency)] **********************************
2025-07-07 10:44:56,660 p=156820 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:44:56,675 p=156820 u=gpadmin n=ansible | TASK [monitoring : Create Grafana provisioning directories] ***************************************************
2025-07-07 10:44:56,987 p=156820 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/datasources)
2025-07-07 10:44:57,264 p=156820 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/dashboards)
2025-07-07 10:44:57,552 p=156820 u=gpadmin n=ansible | changed: [MASTER] => (item=/home/gpadmin/grafana_data/dashboards)
2025-07-07 10:44:57,567 p=156820 u=gpadmin n=ansible | TASK [monitoring : Create Grafana datasource configuration] ***************************************************
2025-07-07 10:44:58,105 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:58,119 p=156820 u=gpadmin n=ansible | TASK [monitoring : Create Grafana dashboard provider configuration] *******************************************
2025-07-07 10:44:58,648 p=156820 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:44:58,663 p=156820 u=gpadmin n=ansible | TASK [monitoring : Copy Grafana dashboards] *******************************************************************
2025-07-07 10:44:59,204 p=156820 u=gpadmin n=ansible | changed: [MASTER] => (item=node-exporter-dashboard.json)
2025-07-07 10:44:59,740 p=156820 u=gpadmin n=ansible | changed: [MASTER] => (item=docker-container-dashboard.json)
2025-07-07 10:45:00,272 p=156820 u=gpadmin n=ansible | changed: [MASTER] => (item=ray-cluster-dashboard.json)
2025-07-07 10:45:00,288 p=156820 u=gpadmin n=ansible | TASK [monitoring : Start Grafana container] *******************************************************************
2025-07-07 10:45:00,792 p=156820 u=gpadmin n=ansible | fatal: [MASTER]: FAILED! => {"changed": false, "msg": "Non-string value found for env option. Ambiguous env options must be wrapped in quotes to avoid them being interpreted. Key: GF_SERVER_HTTP_PORT"}
2025-07-07 10:45:00,794 p=156820 u=gpadmin n=ansible | PLAY RECAP ****************************************************************************************************
2025-07-07 10:45:00,794 p=156820 u=gpadmin n=ansible | G-241                      : ok=46   changed=10   unreachable=0    failed=1    skipped=11   rescued=0    ignored=0   
2025-07-07 10:45:00,794 p=156820 u=gpadmin n=ansible | G-242                      : ok=45   changed=10   unreachable=0    failed=1    skipped=11   rescued=0    ignored=0   
2025-07-07 10:45:00,794 p=156820 u=gpadmin n=ansible | G-243                      : ok=45   changed=10   unreachable=0    failed=1    skipped=11   rescued=0    ignored=0   
2025-07-07 10:45:00,794 p=156820 u=gpadmin n=ansible | G-244                      : ok=45   changed=10   unreachable=0    failed=1    skipped=11   rescued=0    ignored=0   
2025-07-07 10:45:00,795 p=156820 u=gpadmin n=ansible | MASTER                     : ok=57   changed=20   unreachable=0    failed=1    skipped=13   rescued=0    ignored=0   
2025-07-07 10:46:53,474 p=164479 u=gpadmin n=ansible | PLAY [Deploy Ray Cluster - Common Setup] ***************************************
2025-07-07 10:46:53,487 p=164479 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************
2025-07-07 10:46:54,984 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:46:55,126 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:46:55,135 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:46:55,181 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:46:55,203 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:46:55,284 p=164479 u=gpadmin n=ansible | TASK [common : Update apt cache] ***********************************************
2025-07-07 10:46:55,771 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:46:55,771 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:46:55,775 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:46:55,783 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:46:56,197 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:46:56,211 p=164479 u=gpadmin n=ansible | TASK [common : Install common packages] ****************************************
2025-07-07 10:46:56,687 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:46:56,693 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:46:56,703 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:46:56,715 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:46:57,326 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:46:57,339 p=164479 u=gpadmin n=ansible | TASK [common : Create Ray temporary directory] *********************************
2025-07-07 10:46:57,645 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:46:57,647 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:46:57,647 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:46:57,651 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:46:57,753 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:46:57,766 p=164479 u=gpadmin n=ansible | TASK [common : Check if Ray directory exists] **********************************
2025-07-07 10:46:58,049 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:46:58,059 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:46:58,061 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:46:58,088 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:46:58,161 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:46:58,218 p=164479 u=gpadmin n=ansible | TASK [docker : Check if Docker is available via which command] *****************
2025-07-07 10:46:58,475 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:46:58,502 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:46:58,503 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:46:58,524 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:46:58,607 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:46:58,621 p=164479 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via apt] ***************************
2025-07-07 10:46:58,806 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:46:58,827 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:46:58,835 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:46:58,853 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:46:58,955 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:46:58,968 p=164479 u=gpadmin n=ansible | TASK [docker : Check if Docker is installed via snap] **************************
2025-07-07 10:46:59,178 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:46:59,198 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:46:59,211 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:46:59,217 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:46:59,305 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:46:59,319 p=164479 u=gpadmin n=ansible | TASK [docker : Set Docker installation status facts] ***************************
2025-07-07 10:46:59,365 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:46:59,380 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:46:59,401 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:46:59,404 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:46:59,421 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:46:59,430 p=164479 u=gpadmin n=ansible | TASK [docker : Display Docker installation status] *****************************
2025-07-07 10:46:59,467 p=164479 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: True",
        "Docker installed via snap: False"
    ]
}
2025-07-07 10:46:59,483 p=164479 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:46:59,498 p=164479 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:46:59,502 p=164479 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:46:59,517 p=164479 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Docker available via which: True",
        "Docker installed via apt: False",
        "Docker installed via snap: True"
    ]
}
2025-07-07 10:46:59,526 p=164479 u=gpadmin n=ansible | TASK [docker : Add Docker GPG key] *********************************************
2025-07-07 10:46:59,562 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:46:59,577 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:46:59,593 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:46:59,593 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:46:59,606 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:46:59,616 p=164479 u=gpadmin n=ansible | TASK [docker : Add Docker APT repository] **************************************
2025-07-07 10:46:59,642 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:46:59,659 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:46:59,675 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:46:59,692 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:46:59,699 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:46:59,710 p=164479 u=gpadmin n=ansible | TASK [docker : Update apt cache after adding Docker repository] ****************
2025-07-07 10:46:59,745 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:46:59,761 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:46:59,777 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:46:59,777 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:46:59,789 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:46:59,798 p=164479 u=gpadmin n=ansible | TASK [docker : Install Docker Engine via APT] **********************************
2025-07-07 10:46:59,833 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:46:59,854 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:46:59,908 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:46:59,908 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:46:59,921 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:46:59,930 p=164479 u=gpadmin n=ansible | TASK [docker : Check Docker service status (systemd)] **************************
2025-07-07 10:47:00,125 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:47:00,127 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:47:00,135 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:47:00,157 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:47:00,255 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:00,269 p=164479 u=gpadmin n=ansible | TASK [docker : Start Docker service (systemd)] *********************************
2025-07-07 10:47:00,328 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:47:00,344 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:47:00,344 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:47:00,364 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:47:00,960 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:00,974 p=164479 u=gpadmin n=ansible | TASK [docker : Install Docker via snap if not installed via apt] ***************
2025-07-07 10:47:01,003 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:47:01,039 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:47:01,059 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:47:01,060 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:47:01,073 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:47:01,086 p=164479 u=gpadmin n=ansible | TASK [docker : Wait for snap Docker installation to complete] ******************
2025-07-07 10:47:01,113 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:47:01,124 p=164479 u=gpadmin n=ansible | TASK [docker : Wait for any in-progress snap operations to complete] ***********
2025-07-07 10:47:01,166 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:47:01,324 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:47:01,344 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:47:01,362 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:47:01,379 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:47:01,394 p=164479 u=gpadmin n=ansible | TASK [docker : Start Docker service (snap)] ************************************
2025-07-07 10:47:01,441 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:47:01,740 p=164479 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:47:34,817 p=164479 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:47:34,826 p=164479 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:47:34,846 p=164479 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:47:34,859 p=164479 u=gpadmin n=ansible | TASK [docker : Add user to docker group (for apt installation)] ****************
2025-07-07 10:47:34,920 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:47:34,935 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:47:34,939 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:47:34,950 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:47:35,365 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:35,379 p=164479 u=gpadmin n=ansible | TASK [docker : Wait for Docker to be ready] ************************************
2025-07-07 10:47:35,630 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:47:35,643 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:47:35,656 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:47:35,657 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:47:35,806 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:35,821 p=164479 u=gpadmin n=ansible | TASK [docker : Display Docker version] *****************************************
2025-07-07 10:47:36,053 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:47:36,058 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:47:36,084 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:47:36,093 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:47:36,145 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:36,160 p=164479 u=gpadmin n=ansible | TASK [docker : Show Docker version] ********************************************
2025-07-07 10:47:36,203 p=164479 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Docker version 28.3.1, build 38b7060"
}
2025-07-07 10:47:36,220 p=164479 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:47:36,238 p=164479 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:47:36,238 p=164479 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:47:36,256 p=164479 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "Docker version 28.1.1+1, build 068a01e"
}
2025-07-07 10:47:36,266 p=164479 u=gpadmin n=ansible | TASK [docker : Pull Ray Docker image] ******************************************
2025-07-07 10:47:37,194 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:47:37,202 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:47:37,217 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:47:37,225 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:47:37,367 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:37,381 p=164479 u=gpadmin n=ansible | TASK [docker : Pull monitoring Docker images] **********************************
2025-07-07 10:47:38,088 p=164479 u=gpadmin n=ansible | ok: [G-241] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:47:38,141 p=164479 u=gpadmin n=ansible | ok: [G-242] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:47:38,155 p=164479 u=gpadmin n=ansible | ok: [G-243] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:47:38,166 p=164479 u=gpadmin n=ansible | ok: [G-244] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:47:38,262 p=164479 u=gpadmin n=ansible | ok: [MASTER] => (item=prom/prometheus:v2.45.0)
2025-07-07 10:47:38,788 p=164479 u=gpadmin n=ansible | ok: [G-241] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:47:38,814 p=164479 u=gpadmin n=ansible | ok: [G-242] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:47:38,838 p=164479 u=gpadmin n=ansible | ok: [G-243] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:47:38,842 p=164479 u=gpadmin n=ansible | ok: [G-244] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:47:39,131 p=164479 u=gpadmin n=ansible | ok: [MASTER] => (item=prom/node-exporter:v1.6.1)
2025-07-07 10:47:39,663 p=164479 u=gpadmin n=ansible | ok: [G-241] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:47:39,713 p=164479 u=gpadmin n=ansible | ok: [G-243] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:47:39,719 p=164479 u=gpadmin n=ansible | ok: [G-242] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:47:39,741 p=164479 u=gpadmin n=ansible | ok: [G-244] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:47:40,206 p=164479 u=gpadmin n=ansible | ok: [MASTER] => (item=gcr.io/cadvisor/cadvisor:v0.47.2)
2025-07-07 10:47:40,365 p=164479 u=gpadmin n=ansible | ok: [G-241] => (item=grafana/grafana:10.0.3)
2025-07-07 10:47:40,382 p=164479 u=gpadmin n=ansible | ok: [G-242] => (item=grafana/grafana:10.0.3)
2025-07-07 10:47:40,390 p=164479 u=gpadmin n=ansible | ok: [G-243] => (item=grafana/grafana:10.0.3)
2025-07-07 10:47:40,412 p=164479 u=gpadmin n=ansible | ok: [G-244] => (item=grafana/grafana:10.0.3)
2025-07-07 10:47:41,112 p=164479 u=gpadmin n=ansible | ok: [MASTER] => (item=grafana/grafana:10.0.3)
2025-07-07 10:47:41,170 p=164479 u=gpadmin n=ansible | TASK [version_control : Check current Ray container status] ********************
2025-07-07 10:47:41,585 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:47:41,589 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:47:41,591 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:47:41,593 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:47:41,776 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:41,785 p=164479 u=gpadmin n=ansible | TASK [version_control : Get current Ray container image if exists] *************
2025-07-07 10:47:41,820 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:41,838 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:47:41,853 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:47:41,857 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:47:41,877 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:47:41,888 p=164479 u=gpadmin n=ansible | TASK [version_control : Display current Ray image] *****************************
2025-07-07 10:47:41,924 p=164479 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:47:41,939 p=164479 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:47:41,955 p=164479 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:47:41,958 p=164479 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:47:41,981 p=164479 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "Current Ray image: rayproject/ray:2.47.1, Target image: rayproject/ray:2.47.1"
}
2025-07-07 10:47:41,990 p=164479 u=gpadmin n=ansible | TASK [version_control : Check if Ray version matches target] *******************
2025-07-07 10:47:42,025 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:42,040 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:47:42,055 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:47:42,066 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:47:42,074 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:47:42,083 p=164479 u=gpadmin n=ansible | TASK [version_control : Pull target Ray Docker image] **************************
2025-07-07 10:47:42,121 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:47:42,138 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:47:42,155 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:47:42,159 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:47:42,170 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:47:42,179 p=164479 u=gpadmin n=ansible | TASK [version_control : Stop Ray head container if version mismatch] ***********
2025-07-07 10:47:42,214 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:47:42,230 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:47:42,246 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:47:42,251 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:47:42,268 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:47:42,277 p=164479 u=gpadmin n=ansible | TASK [version_control : Stop Ray worker container if version mismatch] *********
2025-07-07 10:47:42,312 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:47:42,327 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:47:42,348 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:47:42,352 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:47:42,366 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:47:42,375 p=164479 u=gpadmin n=ansible | TASK [version_control : Remove old Ray images to free space] *******************
2025-07-07 10:47:42,412 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:47:42,427 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:47:42,444 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:47:42,451 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:47:42,459 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:47:42,468 p=164479 u=gpadmin n=ansible | TASK [version_control : Check Python version in Ray container] *****************
2025-07-07 10:47:43,078 p=164479 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:47:43,080 p=164479 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:47:43,086 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:47:43,090 p=164479 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:47:43,115 p=164479 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:47:43,129 p=164479 u=gpadmin n=ansible | TASK [version_control : Extract Python version from Ray container] *************
2025-07-07 10:47:43,194 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:43,210 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:47:43,220 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:47:43,235 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:47:43,252 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:47:43,262 p=164479 u=gpadmin n=ansible | TASK [version_control : Display Python version compatibility] ******************
2025-07-07 10:47:43,313 p=164479 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Ray container Python version: 3.9",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:47:43,333 p=164479 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Ray container Python version: 3.9",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:47:43,341 p=164479 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Ray container Python version: 3.9",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:47:43,357 p=164479 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Ray container Python version: 3.9",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:47:43,382 p=164479 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Ray container Python version: 3.9",
        "Target Python version: 3.11",
        "Compatible: False"
    ]
}
2025-07-07 10:47:43,390 p=164479 u=gpadmin n=ansible | TASK [version_control : Warn about Python version mismatch] ********************
2025-07-07 10:47:43,510 p=164479 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "⚠️  WARNING: Python version mismatch detected!",
        "Container has Python 3.9",
        "Target is Python 3.11",
        "This may cause client connection issues"
    ]
}
2025-07-07 10:47:43,512 p=164479 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "⚠️  WARNING: Python version mismatch detected!",
        "Container has Python 3.9",
        "Target is Python 3.11",
        "This may cause client connection issues"
    ]
}
2025-07-07 10:47:43,524 p=164479 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "⚠️  WARNING: Python version mismatch detected!",
        "Container has Python 3.9",
        "Target is Python 3.11",
        "This may cause client connection issues"
    ]
}
2025-07-07 10:47:43,533 p=164479 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "⚠️  WARNING: Python version mismatch detected!",
        "Container has Python 3.9",
        "Target is Python 3.11",
        "This may cause client connection issues"
    ]
}
2025-07-07 10:47:43,557 p=164479 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "⚠️  WARNING: Python version mismatch detected!",
        "Container has Python 3.9",
        "Target is Python 3.11",
        "This may cause client connection issues"
    ]
}
2025-07-07 10:47:43,565 p=164479 u=gpadmin n=ansible | TASK [version_control : Force pull latest Ray image if enforcing consistency] ***
2025-07-07 10:47:44,304 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:47:44,310 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:47:44,331 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:47:44,353 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:47:44,500 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:44,513 p=164479 u=gpadmin n=ansible | TASK [version_control : Create version info file] ******************************
2025-07-07 10:47:45,109 p=164479 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:47:45,114 p=164479 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:47:45,114 p=164479 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:47:45,115 p=164479 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:47:45,230 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:47:45,248 p=164479 u=gpadmin n=ansible | TASK [version_control : Display version enforcement status] ********************
2025-07-07 10:47:45,277 p=164479 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:47:45,310 p=164479 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:47:45,326 p=164479 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:47:45,328 p=164479 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:47:45,343 p=164479 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "Version enforcement: ENABLED",
        "Cleanup conflicting versions: ENABLED",
        "Target Ray version: 2.47.1",
        "Target Python version: 3.11",
        "Ray Docker image: rayproject/ray:2.47.1"
    ]
}
2025-07-07 10:47:45,478 p=164479 u=gpadmin n=ansible | PLAY [Configure Ray Head Node] *************************************************
2025-07-07 10:47:45,488 p=164479 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************
2025-07-07 10:47:46,700 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:46,734 p=164479 u=gpadmin n=ansible | TASK [ray_head : Ensure Ray temporary directory exists on head node] ***********
2025-07-07 10:47:47,038 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:47,052 p=164479 u=gpadmin n=ansible | TASK [ray_head : Stop and remove existing Ray head container (idempotency)] ****
2025-07-07 10:47:47,949 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:47:47,964 p=164479 u=gpadmin n=ansible | TASK [ray_head : Remove old Ray head start script (idempotency)] ***************
2025-07-07 10:47:48,261 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:47:48,276 p=164479 u=gpadmin n=ansible | TASK [ray_head : Copy Ray head start script to head node] **********************
2025-07-07 10:47:48,801 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:47:48,815 p=164479 u=gpadmin n=ansible | TASK [ray_head : Start Ray head container] *************************************
2025-07-07 10:47:49,379 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:47:49,394 p=164479 u=gpadmin n=ansible | TASK [ray_head : Display Ray head container status] ****************************
2025-07-07 10:47:49,728 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:49,746 p=164479 u=gpadmin n=ansible | TASK [ray_head : Print Ray head container status] ******************************
2025-07-07 10:47:49,773 p=164479 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED                  STATUS                  PORTS     NAMES\n19f18374a124   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   Less than a second ago   Up Less than a second             ray_head"
}
2025-07-07 10:47:49,787 p=164479 u=gpadmin n=ansible | TASK [ray_head : Wait for a moment to let Ray head node initialize] ************
2025-07-07 10:47:49,807 p=164479 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 10:47:49,808 p=164479 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 10:47:59,812 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:47:59,827 p=164479 u=gpadmin n=ansible | TASK [ray_head : Check Ray head node status] ***********************************
2025-07-07 10:48:02,196 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:48:02,209 p=164479 u=gpadmin n=ansible | TASK [ray_head : Print Ray head node status] ***********************************
2025-07-07 10:48:02,255 p=164479 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:47:56.847606 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_269faec8759fc0c9602580d1f96735fc0100eb97ada378e3bfb9ded8",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/32.0 CPU",
        " 0B/100.53KiB memory",
        " 0B/9.31GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:48:02,312 p=164479 u=gpadmin n=ansible | PLAY [Configure Ray Worker Nodes] **********************************************
2025-07-07 10:48:02,321 p=164479 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************
2025-07-07 10:48:02,970 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:48:02,974 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:48:02,979 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:48:02,988 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:48:03,061 p=164479 u=gpadmin n=ansible | TASK [ray_worker : Ensure Ray temporary directory exists on worker node] *******
2025-07-07 10:48:03,212 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:48:03,230 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:48:03,253 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:48:03,264 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:48:03,278 p=164479 u=gpadmin n=ansible | TASK [ray_worker : Stop and remove existing Ray worker container (idempotency)] ***
2025-07-07 10:48:03,661 p=164479 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:48:03,692 p=164479 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:48:03,701 p=164479 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:48:03,707 p=164479 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:48:03,719 p=164479 u=gpadmin n=ansible | TASK [ray_worker : Remove old Ray worker start script (idempotency)] ***********
2025-07-07 10:48:03,890 p=164479 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:48:03,904 p=164479 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:48:03,928 p=164479 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:48:03,961 p=164479 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:48:03,973 p=164479 u=gpadmin n=ansible | TASK [ray_worker : Copy Ray worker start script to worker node] ****************
2025-07-07 10:48:04,454 p=164479 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:48:04,463 p=164479 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:48:04,469 p=164479 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:48:04,480 p=164479 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:48:04,497 p=164479 u=gpadmin n=ansible | TASK [ray_worker : Start Ray worker container] *********************************
2025-07-07 10:48:05,031 p=164479 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:48:05,067 p=164479 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:48:05,085 p=164479 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:48:05,102 p=164479 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:48:05,115 p=164479 u=gpadmin n=ansible | TASK [ray_worker : Display Ray worker container status] ************************
2025-07-07 10:48:05,307 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:48:05,324 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:48:05,337 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:48:05,351 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:48:05,365 p=164479 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker container status] **************************
2025-07-07 10:48:05,420 p=164479 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED        STATUS                  PORTS     NAMES\n1e12e6ffad48   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:48:05,438 p=164479 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED        STATUS                  PORTS     NAMES\n020bffbbd7f4   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:48:05,440 p=164479 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED        STATUS                  PORTS     NAMES\n089b862d6831   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:48:05,457 p=164479 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": "CONTAINER ID   IMAGE                   COMMAND                  CREATED        STATUS                  PORTS     NAMES\n071212ca026b   rayproject/ray:2.47.1   \"/home/gpadmin/ray_t…\"   1 second ago   Up Less than a second             ray_worker"
}
2025-07-07 10:48:05,466 p=164479 u=gpadmin n=ansible | TASK [ray_worker : Wait for a moment to let Ray worker node initialize] ********
2025-07-07 10:48:05,486 p=164479 u=gpadmin n=ansible | Pausing for 10 seconds
2025-07-07 10:48:05,487 p=164479 u=gpadmin n=ansible | (ctrl+C then 'C' = continue early, ctrl+C then 'A' = abort)
2025-07-07 10:48:15,491 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:48:15,504 p=164479 u=gpadmin n=ansible | TASK [ray_worker : Check Ray worker node status] *******************************
2025-07-07 10:48:16,568 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:48:16,605 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:48:16,606 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:48:16,638 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:48:16,654 p=164479 u=gpadmin n=ansible | TASK [ray_worker : Print Ray worker node status] *******************************
2025-07-07 10:48:16,706 p=164479 u=gpadmin n=ansible | ok: [G-241] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:48:11.881793 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_d6dfaf59d90a0eb9972e3615af72d74f6abf268e0b5c4b6e44b71d99",
        " 1 node_554ca2cd7b23ae4d17cbeca475395a1696f76485359805d792d46560",
        " 1 node_4ced295b8de33c96246c56fab281cdcdc149ea2015c0e8a2394a7c8d",
        " 1 node_f9d373fc821be167feef9fd9d2a1b215e9c834d11e2f73e0ac99b211",
        " 1 node_269faec8759fc0c9602580d1f96735fc0100eb97ada378e3bfb9ded8",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:48:16,729 p=164479 u=gpadmin n=ansible | ok: [G-242] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:48:11.881793 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_d6dfaf59d90a0eb9972e3615af72d74f6abf268e0b5c4b6e44b71d99",
        " 1 node_554ca2cd7b23ae4d17cbeca475395a1696f76485359805d792d46560",
        " 1 node_4ced295b8de33c96246c56fab281cdcdc149ea2015c0e8a2394a7c8d",
        " 1 node_f9d373fc821be167feef9fd9d2a1b215e9c834d11e2f73e0ac99b211",
        " 1 node_269faec8759fc0c9602580d1f96735fc0100eb97ada378e3bfb9ded8",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:48:16,789 p=164479 u=gpadmin n=ansible | ok: [G-243] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:48:11.881793 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_d6dfaf59d90a0eb9972e3615af72d74f6abf268e0b5c4b6e44b71d99",
        " 1 node_554ca2cd7b23ae4d17cbeca475395a1696f76485359805d792d46560",
        " 1 node_4ced295b8de33c96246c56fab281cdcdc149ea2015c0e8a2394a7c8d",
        " 1 node_f9d373fc821be167feef9fd9d2a1b215e9c834d11e2f73e0ac99b211",
        " 1 node_269faec8759fc0c9602580d1f96735fc0100eb97ada378e3bfb9ded8",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:48:16,816 p=164479 u=gpadmin n=ansible | ok: [G-244] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:48:11.881793 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_d6dfaf59d90a0eb9972e3615af72d74f6abf268e0b5c4b6e44b71d99",
        " 1 node_554ca2cd7b23ae4d17cbeca475395a1696f76485359805d792d46560",
        " 1 node_4ced295b8de33c96246c56fab281cdcdc149ea2015c0e8a2394a7c8d",
        " 1 node_f9d373fc821be167feef9fd9d2a1b215e9c834d11e2f73e0ac99b211",
        " 1 node_269faec8759fc0c9602580d1f96735fc0100eb97ada378e3bfb9ded8",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:48:16,986 p=164479 u=gpadmin n=ansible | PLAY [Verify Ray Cluster Deployment] *******************************************
2025-07-07 10:48:17,006 p=164479 u=gpadmin n=ansible | TASK [Check Ray head node status] **********************************************
2025-07-07 10:48:19,102 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:48:19,117 p=164479 u=gpadmin n=ansible | TASK [Display Ray cluster status] **********************************************
2025-07-07 10:48:19,146 p=164479 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "======== Autoscaler status: 2025-07-07 07:48:16.892713 ========",
        "Node status",
        "---------------------------------------------------------------",
        "Active:",
        " 1 node_d6dfaf59d90a0eb9972e3615af72d74f6abf268e0b5c4b6e44b71d99",
        " 1 node_554ca2cd7b23ae4d17cbeca475395a1696f76485359805d792d46560",
        " 1 node_4ced295b8de33c96246c56fab281cdcdc149ea2015c0e8a2394a7c8d",
        " 1 node_f9d373fc821be167feef9fd9d2a1b215e9c834d11e2f73e0ac99b211",
        " 1 node_269faec8759fc0c9602580d1f96735fc0100eb97ada378e3bfb9ded8",
        "Pending:",
        " (no pending nodes)",
        "Recent failures:",
        " (no failures)",
        "",
        "Resources",
        "---------------------------------------------------------------",
        "Total Usage:",
        " 0.0/96.0 CPU",
        " 0B/502.56KiB memory",
        " 0B/46.57GiB object_store_memory",
        "",
        "Total Constraints:",
        " (no request_resources() constraints)",
        "Total Demands:",
        " (no resource demands)"
    ]
}
2025-07-07 10:48:19,162 p=164479 u=gpadmin n=ansible | TASK [Display Ray cluster error] ***********************************************
2025-07-07 10:48:19,179 p=164479 u=gpadmin n=ansible | skipping: [MASTER]
2025-07-07 10:48:19,219 p=164479 u=gpadmin n=ansible | PLAY [Deploy Monitoring Stack] *************************************************
2025-07-07 10:48:19,229 p=164479 u=gpadmin n=ansible | TASK [Gathering Facts] *********************************************************
2025-07-07 10:48:20,069 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:48:20,105 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:48:20,125 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:48:20,140 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:48:20,456 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:48:20,533 p=164479 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus data directory] ***************************
2025-07-07 10:48:20,720 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:48:20,729 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:48:20,748 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:48:20,757 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:48:20,840 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:48:20,854 p=164479 u=gpadmin n=ansible | TASK [monitoring : Create Grafana data directory] ******************************
2025-07-07 10:48:21,036 p=164479 u=gpadmin n=ansible | ok: [G-242]
2025-07-07 10:48:21,066 p=164479 u=gpadmin n=ansible | ok: [G-241]
2025-07-07 10:48:21,081 p=164479 u=gpadmin n=ansible | ok: [G-243]
2025-07-07 10:48:21,113 p=164479 u=gpadmin n=ansible | ok: [G-244]
2025-07-07 10:48:21,159 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:48:21,172 p=164479 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Node Exporter container (idempotency)] ***
2025-07-07 10:48:21,565 p=164479 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:48:21,589 p=164479 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:48:21,605 p=164479 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:48:21,628 p=164479 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:48:21,743 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:48:21,757 p=164479 u=gpadmin n=ansible | TASK [monitoring : Start Node Exporter container] ******************************
2025-07-07 10:48:22,275 p=164479 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:48:22,296 p=164479 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:48:22,302 p=164479 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:48:22,340 p=164479 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:48:22,433 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:48:22,446 p=164479 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing cAdvisor container (idempotency)] ***
2025-07-07 10:48:22,766 p=164479 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:48:22,783 p=164479 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:48:22,792 p=164479 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:48:22,796 p=164479 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:48:23,042 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:48:23,057 p=164479 u=gpadmin n=ansible | TASK [monitoring : Start cAdvisor container] ***********************************
2025-07-07 10:48:23,600 p=164479 u=gpadmin n=ansible | changed: [G-241]
2025-07-07 10:48:23,628 p=164479 u=gpadmin n=ansible | changed: [G-242]
2025-07-07 10:48:23,629 p=164479 u=gpadmin n=ansible | changed: [G-244]
2025-07-07 10:48:23,630 p=164479 u=gpadmin n=ansible | changed: [G-243]
2025-07-07 10:48:23,770 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:48:23,780 p=164479 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Prometheus container (idempotency)] ***
2025-07-07 10:48:23,833 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:48:23,850 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:48:23,850 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:48:23,859 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:48:24,376 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:48:24,394 p=164479 u=gpadmin n=ansible | TASK [monitoring : Create Prometheus configuration] ****************************
2025-07-07 10:48:24,454 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:48:24,476 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:48:24,477 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:48:24,491 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:48:24,996 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:48:25,011 p=164479 u=gpadmin n=ansible | TASK [monitoring : Start Prometheus container] *********************************
2025-07-07 10:48:25,074 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:48:25,092 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:48:25,093 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:48:25,102 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:48:25,675 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:48:25,696 p=164479 u=gpadmin n=ansible | TASK [monitoring : Stop and remove existing Grafana container (idempotency)] ***
2025-07-07 10:48:25,757 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:48:25,774 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:48:25,774 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:48:25,783 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:48:26,175 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:48:26,203 p=164479 u=gpadmin n=ansible | TASK [monitoring : Create Grafana provisioning directories] ********************
2025-07-07 10:48:26,256 p=164479 u=gpadmin n=ansible | skipping: [G-241] => (item=/home/gpadmin/grafana_data/provisioning/datasources) 
2025-07-07 10:48:26,261 p=164479 u=gpadmin n=ansible | skipping: [G-241] => (item=/home/gpadmin/grafana_data/provisioning/dashboards) 
2025-07-07 10:48:26,263 p=164479 u=gpadmin n=ansible | skipping: [G-241] => (item=/home/gpadmin/grafana_data/dashboards) 
2025-07-07 10:48:26,270 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:48:26,273 p=164479 u=gpadmin n=ansible | skipping: [G-242] => (item=/home/gpadmin/grafana_data/provisioning/datasources) 
2025-07-07 10:48:26,278 p=164479 u=gpadmin n=ansible | skipping: [G-242] => (item=/home/gpadmin/grafana_data/provisioning/dashboards) 
2025-07-07 10:48:26,279 p=164479 u=gpadmin n=ansible | skipping: [G-242] => (item=/home/gpadmin/grafana_data/dashboards) 
2025-07-07 10:48:26,285 p=164479 u=gpadmin n=ansible | skipping: [G-243] => (item=/home/gpadmin/grafana_data/provisioning/datasources) 
2025-07-07 10:48:26,286 p=164479 u=gpadmin n=ansible | skipping: [G-243] => (item=/home/gpadmin/grafana_data/provisioning/dashboards) 
2025-07-07 10:48:26,291 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:48:26,294 p=164479 u=gpadmin n=ansible | skipping: [G-243] => (item=/home/gpadmin/grafana_data/dashboards) 
2025-07-07 10:48:26,295 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:48:26,308 p=164479 u=gpadmin n=ansible | skipping: [G-244] => (item=/home/gpadmin/grafana_data/provisioning/datasources) 
2025-07-07 10:48:26,310 p=164479 u=gpadmin n=ansible | skipping: [G-244] => (item=/home/gpadmin/grafana_data/provisioning/dashboards) 
2025-07-07 10:48:26,313 p=164479 u=gpadmin n=ansible | skipping: [G-244] => (item=/home/gpadmin/grafana_data/dashboards) 
2025-07-07 10:48:26,313 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:48:26,521 p=164479 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/datasources)
2025-07-07 10:48:26,796 p=164479 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/provisioning/dashboards)
2025-07-07 10:48:27,075 p=164479 u=gpadmin n=ansible | ok: [MASTER] => (item=/home/gpadmin/grafana_data/dashboards)
2025-07-07 10:48:27,092 p=164479 u=gpadmin n=ansible | TASK [monitoring : Create Grafana datasource configuration] ********************
2025-07-07 10:48:27,140 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:48:27,175 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:48:27,175 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:48:27,184 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:48:27,662 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:48:27,677 p=164479 u=gpadmin n=ansible | TASK [monitoring : Create Grafana dashboard provider configuration] ************
2025-07-07 10:48:27,730 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:48:27,799 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:48:27,816 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:48:27,826 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:48:28,233 p=164479 u=gpadmin n=ansible | ok: [MASTER]
2025-07-07 10:48:28,247 p=164479 u=gpadmin n=ansible | TASK [monitoring : Copy Grafana dashboards] ************************************
2025-07-07 10:48:28,300 p=164479 u=gpadmin n=ansible | skipping: [G-241] => (item=node-exporter-dashboard.json) 
2025-07-07 10:48:28,304 p=164479 u=gpadmin n=ansible | skipping: [G-241] => (item=docker-container-dashboard.json) 
2025-07-07 10:48:28,305 p=164479 u=gpadmin n=ansible | skipping: [G-241] => (item=ray-cluster-dashboard.json) 
2025-07-07 10:48:28,313 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:48:28,315 p=164479 u=gpadmin n=ansible | skipping: [G-242] => (item=node-exporter-dashboard.json) 
2025-07-07 10:48:28,322 p=164479 u=gpadmin n=ansible | skipping: [G-242] => (item=docker-container-dashboard.json) 
2025-07-07 10:48:28,322 p=164479 u=gpadmin n=ansible | skipping: [G-242] => (item=ray-cluster-dashboard.json) 
2025-07-07 10:48:28,333 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:48:28,335 p=164479 u=gpadmin n=ansible | skipping: [G-243] => (item=node-exporter-dashboard.json) 
2025-07-07 10:48:28,336 p=164479 u=gpadmin n=ansible | skipping: [G-243] => (item=docker-container-dashboard.json) 
2025-07-07 10:48:28,337 p=164479 u=gpadmin n=ansible | skipping: [G-243] => (item=ray-cluster-dashboard.json) 
2025-07-07 10:48:28,338 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:48:28,344 p=164479 u=gpadmin n=ansible | skipping: [G-244] => (item=node-exporter-dashboard.json) 
2025-07-07 10:48:28,346 p=164479 u=gpadmin n=ansible | skipping: [G-244] => (item=docker-container-dashboard.json) 
2025-07-07 10:48:28,348 p=164479 u=gpadmin n=ansible | skipping: [G-244] => (item=ray-cluster-dashboard.json) 
2025-07-07 10:48:28,349 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:48:28,790 p=164479 u=gpadmin n=ansible | ok: [MASTER] => (item=node-exporter-dashboard.json)
2025-07-07 10:48:29,318 p=164479 u=gpadmin n=ansible | ok: [MASTER] => (item=docker-container-dashboard.json)
2025-07-07 10:48:29,840 p=164479 u=gpadmin n=ansible | ok: [MASTER] => (item=ray-cluster-dashboard.json)
2025-07-07 10:48:29,857 p=164479 u=gpadmin n=ansible | TASK [monitoring : Start Grafana container] ************************************
2025-07-07 10:48:29,917 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:48:29,937 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:48:29,938 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:48:29,952 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:48:30,559 p=164479 u=gpadmin n=ansible | changed: [MASTER]
2025-07-07 10:48:30,573 p=164479 u=gpadmin n=ansible | TASK [monitoring : Display monitoring access information] **********************
2025-07-07 10:48:30,604 p=164479 u=gpadmin n=ansible | ok: [MASTER] => {
    "msg": [
        "Prometheus is available at http://192.168.40.240:9090",
        "Grafana is available at http://192.168.40.240:3000 (username: admin, password: admin)"
    ]
}
2025-07-07 10:48:30,621 p=164479 u=gpadmin n=ansible | skipping: [G-241]
2025-07-07 10:48:30,637 p=164479 u=gpadmin n=ansible | skipping: [G-242]
2025-07-07 10:48:30,655 p=164479 u=gpadmin n=ansible | skipping: [G-243]
2025-07-07 10:48:30,664 p=164479 u=gpadmin n=ansible | skipping: [G-244]
2025-07-07 10:48:30,805 p=164479 u=gpadmin n=ansible | PLAY RECAP *********************************************************************
2025-07-07 10:48:30,806 p=164479 u=gpadmin n=ansible | G-241                      : ok=47   changed=11   unreachable=0    failed=0    skipped=21   rescued=0    ignored=0   
2025-07-07 10:48:30,806 p=164479 u=gpadmin n=ansible | G-242                      : ok=46   changed=11   unreachable=0    failed=0    skipped=21   rescued=0    ignored=0   
2025-07-07 10:48:30,806 p=164479 u=gpadmin n=ansible | G-243                      : ok=46   changed=11   unreachable=0    failed=0    skipped=21   rescued=0    ignored=0   
2025-07-07 10:48:30,806 p=164479 u=gpadmin n=ansible | G-244                      : ok=46   changed=11   unreachable=0    failed=0    skipped=21   rescued=0    ignored=0   
2025-07-07 10:48:30,806 p=164479 u=gpadmin n=ansible | MASTER                     : ok=59   changed=13   unreachable=0    failed=0    skipped=13   rescued=0    ignored=0   
2025-07-07 10:57:50,188 p=179878 u=gpadmin n=ansible | G-242 | FAILED | rc=1 >>
docker: 'docker ps' accepts no arguments

Usage:  docker ps [OPTIONS]

Run 'docker ps --help' for more informationnon-zero return code

2025-07-07 10:57:50,220 p=179878 u=gpadmin n=ansible | G-243 | FAILED | rc=1 >>
docker: 'docker ps' accepts no arguments

Usage:  docker ps [OPTIONS]

Run 'docker ps --help' for more informationnon-zero return code

2025-07-07 10:57:50,225 p=179878 u=gpadmin n=ansible | G-244 | FAILED | rc=1 >>
docker: 'docker ps' accepts no arguments

Usage:  docker ps [OPTIONS]

Run 'docker ps --help' for more informationnon-zero return code

2025-07-07 10:57:50,241 p=179878 u=gpadmin n=ansible | G-241 | FAILED | rc=1 >>
docker: 'docker ps' accepts no arguments

Usage:  docker ps [OPTIONS]

Run 'docker ps --help' for more informationnon-zero return code

2025-07-07 10:57:55,596 p=180032 u=gpadmin n=ansible | G-244 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                              COMMAND                  CREATED         STATUS                     PORTS     NAMES
630d316749fe   gcr.io/cadvisor/cadvisor:v0.47.2   "/usr/bin/cadvisor -…"   9 minutes ago   Up 9 minutes (unhealthy)             cadvisor
32d4616fd79e   prom/node-exporter:v1.6.1          "/bin/node_exporter …"   9 minutes ago   Up 9 minutes                         node-exporter
071212ca026b   rayproject/ray:2.47.1              "/home/gpadmin/ray_t…"   9 minutes ago   Up 9 minutes                         ray_worker

2025-07-07 10:57:55,596 p=180032 u=gpadmin n=ansible | G-243 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                              COMMAND                  CREATED         STATUS                     PORTS     NAMES
a10fce5901fb   gcr.io/cadvisor/cadvisor:v0.47.2   "/usr/bin/cadvisor -…"   9 minutes ago   Up 9 minutes (unhealthy)             cadvisor
7d7c21b32714   prom/node-exporter:v1.6.1          "/bin/node_exporter …"   9 minutes ago   Up 9 minutes                         node-exporter
089b862d6831   rayproject/ray:2.47.1              "/home/gpadmin/ray_t…"   9 minutes ago   Up 9 minutes                         ray_worker

2025-07-07 10:57:55,609 p=180032 u=gpadmin n=ansible | G-242 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                              COMMAND                  CREATED         STATUS                     PORTS     NAMES
05b9f581ee56   gcr.io/cadvisor/cadvisor:v0.47.2   "/usr/bin/cadvisor -…"   9 minutes ago   Up 9 minutes (unhealthy)             cadvisor
24019e9c233e   prom/node-exporter:v1.6.1          "/bin/node_exporter …"   9 minutes ago   Up 9 minutes                         node-exporter
020bffbbd7f4   rayproject/ray:2.47.1              "/home/gpadmin/ray_t…"   9 minutes ago   Up 9 minutes                         ray_worker

2025-07-07 10:57:55,609 p=180032 u=gpadmin n=ansible | G-241 | CHANGED | rc=0 >>
CONTAINER ID   IMAGE                              COMMAND                  CREATED         STATUS                     PORTS     NAMES
a343574ed1d7   gcr.io/cadvisor/cadvisor:v0.47.2   "/usr/bin/cadvisor -…"   9 minutes ago   Up 9 minutes (unhealthy)             cadvisor
a425be42a14a   prom/node-exporter:v1.6.1          "/bin/node_exporter …"   9 minutes ago   Up 9 minutes                         node-exporter
1e12e6ffad48   rayproject/ray:2.47.1              "/home/gpadmin/ray_t…"   9 minutes ago   Up 9 minutes                         ray_worker

2025-07-07 10:58:04,451 p=180184 u=gpadmin n=ansible | G-241 | CHANGED | rc=0 >>
2025-07-07 07:48:06,132	WARN scripts.py:826 -- [33m`--temp-dir=/home/gpadmin/ray_temp` option will be ignored. `--head` is a required flag to use `--temp-dir`. temp_dir is only configurable from a head node. All the worker nodes will use the same temp_dir as a head node. [39m
2025-07-07 07:48:06,132	INFO scripts.py:1152 -- [37mLocal node IP[39m: [1m192.168.40.241[22m
2025-07-07 07:48:08,194	SUCC scripts.py:1168 -- [32m--------------------[39m
2025-07-07 07:48:08,194	SUCC scripts.py:1169 -- [32mRay runtime started.[39m
2025-07-07 07:48:08,195	SUCC scripts.py:1170 -- [32m--------------------[39m
2025-07-07 07:48:08,195	INFO scripts.py:1172 -- To terminate the Ray runtime, run
2025-07-07 07:48:08,195	INFO scripts.py:1173 -- [1m  ray stop[22m2025-07-07 07:48:06,152	WARNING services.py:2152 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 2147483648 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.
[2025-07-07 07:48:06,186 W 7 7] global_state_accessor.cc:435: Retrying to get node with node ID f9d373fc821be167feef9fd9d2a1b215e9c834d11e2f73e0ac99b211
[2025-07-07 07:48:07,188 W 7 7] global_state_accessor.cc:435: Retrying to get node with node ID f9d373fc821be167feef9fd9d2a1b215e9c834d11e2f73e0ac99b211

